# backend/core/tools/eval_engine.py
# åŠŸèƒ½: Eval V2 è¯„ä¼°æ‰§è¡Œå¼•æ“Ž
# ä¸»è¦å‡½æ•°:
#   - run_task_trial(): æ‰§è¡Œå•ä¸ª EvalTask çš„ä¸€æ¬¡ Trialï¼ˆæ ¸å¿ƒï¼‰
#   - run_grader(): å¯¹ Trial ç»“æžœè¿›è¡Œè¯„åˆ†ï¼ˆå†…å®¹/è¿‡ç¨‹/ç»¼åˆï¼‰
#   - run_diagnoser(): è·¨ Trial è¯Šæ–­
#   - run_eval_run(): æ‰§è¡Œæ•´ä¸ª EvalRunï¼ˆå¹¶è¡Œæ‰§è¡Œæ‰€æœ‰ Taskï¼‰
#   - format_*(): æ ¼å¼åŒ–è¾“å‡º
# æ•°æ®ç»“æž„:
#   - LLMCall: ä¸€æ¬¡ LLM è°ƒç”¨çš„å®Œæ•´è®°å½•ï¼ˆè¾“å…¥/è¾“å‡º/token/è€—æ—¶ï¼‰
#   - TrialResult: Trial æ‰§è¡Œç»“æžœï¼ˆå« llm_calls åˆ—è¡¨ï¼‰

"""
Eval V2 å¼•æ“Ž
Task-based è¯„ä¼°ä½“ç³»ï¼š
  EvalRun â†’ EvalTask[] â†’ EvalTrial[]
  æ¯ä¸ª Trial è®°å½•å®Œæ•´çš„ LLM è°ƒç”¨æ—¥å¿—
  Grader åˆ†ç¦»ï¼šå†…å®¹è¯„åˆ† + è¿‡ç¨‹è¯„åˆ†
"""

import json
import time
import asyncio
from typing import Optional, Dict, List, Tuple
from dataclasses import dataclass, field
from datetime import datetime

from core.ai_client import ai_client, ChatMessage
from core.models.eval_task import SIMULATOR_TYPES


# ============== æ•°æ®ç»“æž„ ==============

@dataclass
class LLMCall:
    """ä¸€æ¬¡ LLM è°ƒç”¨çš„å®Œæ•´è®°å½•"""
    step: str           # "simulator_review" / "consumer_turn_1" / "content_rep_turn_1" / "grader_content" / "grader_process" / "diagnoser"
    input_system: str   # ç³»ç»Ÿæç¤ºè¯
    input_user: str     # ç”¨æˆ·æ¶ˆæ¯
    output: str         # AI å“åº”
    tokens_in: int = 0
    tokens_out: int = 0
    cost: float = 0.0
    duration_ms: int = 0
    timestamp: str = ""

    def to_dict(self) -> dict:
        return {
            "step": self.step,
            "input": {"system_prompt": self.input_system, "user_message": self.input_user},
            "output": self.output,
            "tokens_in": self.tokens_in,
            "tokens_out": self.tokens_out,
            "cost": self.cost,
            "duration_ms": self.duration_ms,
            "timestamp": self.timestamp or datetime.now().isoformat(),
        }


@dataclass
class TrialResult:
    """Trial æ‰§è¡Œç»“æžœ"""
    role: str
    interaction_mode: str
    role_display_name: str = ""  # ç”¨æˆ·é…ç½®çš„æ¨¡æ‹Ÿå™¨æ˜¾ç¤ºåç§°
    nodes: list = field(default_factory=list)          # äº¤äº’èŠ‚ç‚¹
    result: dict = field(default_factory=dict)          # è¯„åˆ†ç»“æžœ
    grader_outputs: list = field(default_factory=list)  # Grader è¾“å‡º
    llm_calls: list = field(default_factory=list)       # å®Œæ•´ LLM è°ƒç”¨æ—¥å¿—
    overall_score: float = 0.0
    success: bool = True
    error: str = ""
    tokens_in: int = 0
    tokens_out: int = 0
    cost: float = 0.0


# ============== LLM è°ƒç”¨å°è£…ï¼ˆå¸¦æ—¥å¿—ï¼‰ ==============

async def _call_llm(
    system_prompt: str,
    user_message: str,
    step: str,
    temperature: float = 0.6,
) -> Tuple[str, LLMCall]:
    """
    å°è£… LLM è°ƒç”¨ï¼Œè¿”å›ž (å“åº”æ–‡æœ¬, LLMCall æ—¥å¿—)
    æ‰€æœ‰ eval ç›¸å…³çš„ LLM è°ƒç”¨éƒ½èµ°è¿™ä¸ªå‡½æ•°ï¼Œç¡®ä¿æ¯æ¬¡è°ƒç”¨éƒ½è¢«è®°å½•
    """
    messages = [
        ChatMessage(role="system", content=system_prompt),
        ChatMessage(role="user", content=user_message),
    ]
    
    start_time = time.time()
    response = await ai_client.async_chat(messages, temperature=temperature)
    duration_ms = int((time.time() - start_time) * 1000)
    
    call = LLMCall(
        step=step,
        input_system=system_prompt,
        input_user=user_message,
        output=response.content,
        tokens_in=response.tokens_in,
        tokens_out=response.tokens_out,
        cost=response.cost,
        duration_ms=duration_ms,
        timestamp=datetime.now().isoformat(),
    )
    
    return response.content, call


async def _call_llm_multi(
    messages: List[ChatMessage],
    step: str,
    temperature: float = 0.6,
) -> Tuple[str, LLMCall]:
    """å¤šæ¶ˆæ¯ç‰ˆæœ¬çš„ LLM è°ƒç”¨ï¼ˆç”¨äºŽå¤šè½®å¯¹è¯ï¼‰"""
    start_time = time.time()
    response = await ai_client.async_chat(messages, temperature=temperature)
    duration_ms = int((time.time() - start_time) * 1000)
    
    # æå–å®Œæ•´å¯¹è¯åŽ†å²ç”¨äºŽæ—¥å¿—ï¼ˆä¸èƒ½åªè®°æœ€åŽ2æ¡ï¼Œå¦åˆ™ç”¨æˆ·çœ‹ä¸åˆ°å¯¹è¯å…¨è²Œï¼‰
    system_prompt = ""
    conversation_parts = []
    for m in messages:
        if m.role == "system":
            system_prompt = m.content
        elif m.role == "assistant":
            conversation_parts.append(f"[æˆ‘æ–¹(assistant)]: {m.content}")
        elif m.role == "user":
            conversation_parts.append(f"[å¯¹æ–¹(user)]: {m.content}")
    
    # å°†å®Œæ•´å¯¹è¯åŽ†å²ä½œä¸º input_userï¼Œè®©ç”¨æˆ·èƒ½çœ‹åˆ°æ¯ä¸€è½®è°è¯´äº†ä»€ä¹ˆ
    full_history = "\n---\n".join(conversation_parts) if conversation_parts else ""
    
    call = LLMCall(
        step=step,
        input_system=system_prompt,
        input_user=full_history,
        output=response.content,
        tokens_in=response.tokens_in,
        tokens_out=response.tokens_out,
        cost=response.cost,
        duration_ms=duration_ms,
        timestamp=datetime.now().isoformat(),
    )
    
    return response.content, call


# ============== æ ¸å¿ƒæ‰§è¡Œå‡½æ•° ==============

async def run_task_trial(
    simulator_type: str,
    interaction_mode: str,
    content: str,
    creator_profile: str = "",
    intent: str = "",
    persona: dict = None,
    simulator_config: dict = None,
    grader_config: dict = None,
    content_field_names: list = None,
) -> TrialResult:
    """
    æ‰§è¡Œå•ä¸ª EvalTask çš„ä¸€æ¬¡ Trial
    
    è¿™æ˜¯ eval å¼•æ“Žçš„æ ¸å¿ƒå‡½æ•°ã€‚æ ¹æ® simulator_type å’Œ interaction_mode 
    é€‰æ‹©æ‰§è¡Œç­–ç•¥ï¼Œæ‰€æœ‰ LLM è°ƒç”¨éƒ½è¢«å®Œæ•´è®°å½•ã€‚
    
    Args:
        simulator_type: æ¨¡æ‹Ÿå™¨è§’è‰² (coach/editor/expert/consumer/seller/custom)
        interaction_mode: äº¤äº’æ¨¡å¼ (review/dialogue/scenario)
        content: è¦è¯„ä¼°çš„å†…å®¹
        creator_profile: åˆ›ä½œè€…ç‰¹è´¨
        intent: é¡¹ç›®æ„å›¾
        persona: æ¶ˆè´¹è€…ç”»åƒ
        simulator_config: æ¨¡æ‹Ÿå™¨è‡ªå®šä¹‰é…ç½®
        grader_config: è¯„åˆ†å™¨é…ç½®
        content_field_names: å†…å®¹å­—æ®µååˆ—è¡¨
    
    Returns:
        TrialResult
    """
    config = simulator_config or {}
    grader_cfg = grader_config or {}
    
    # èŽ·å–æ¨¡æ‹Ÿå™¨æ˜¾ç¤ºåç§°ï¼ˆä¼˜å…ˆç”¨åŽå°é…ç½®çš„åç§°ï¼Œå…¶æ¬¡ç”¨ç¡¬ç¼–ç åç§°ï¼‰
    display_name = config.get("simulator_name", "") or SIMULATOR_TYPES.get(simulator_type, {}).get("name", simulator_type)
    
    # ===== å…¼å®¹æ—§ç‰ˆ interaction_type â†’ æ–°ç‰ˆ interaction_mode æ˜ å°„ =====
    # reading â†’ reviewï¼ˆé˜…è¯»å¼ = ä¸€æ¬¡æ€§å®¡æŸ¥ï¼‰
    # decision â†’ scenarioï¼ˆå†³ç­–å¼ = åœºæ™¯å¯¹è¯ï¼‰
    # exploration â†’ explorationï¼ˆæŽ¢ç´¢å¼ = è‡ªä¸»æŽ¢ç´¢è·¯å¾„ï¼‰
    MODE_COMPAT = {
        "reading": "review",
        "decision": "scenario",
        "exploration": "exploration",
    }
    effective_mode = MODE_COMPAT.get(interaction_mode, interaction_mode)
    
    if effective_mode == "review":
        result = await _run_review(
            simulator_type, content, creator_profile, intent, persona, config, grader_cfg
        )
    elif effective_mode == "exploration":
        result = await _run_exploration(
            simulator_type, content, creator_profile, intent, persona, config, grader_cfg, content_field_names
        )
    elif effective_mode in ("dialogue", "scenario"):
        result = await _run_dialogue(
            simulator_type, content, creator_profile, intent, persona, config, grader_cfg, content_field_names
        )
    else:
        return TrialResult(
            role=simulator_type, interaction_mode=interaction_mode,
            success=False, error=f"ä¸æ”¯æŒçš„äº¤äº’æ¨¡å¼: {interaction_mode}"
        )
    
    result.role_display_name = display_name
    return result


async def _run_review(
    simulator_type: str,
    content: str,
    creator_profile: str,
    intent: str,
    persona: dict,
    config: dict,
    grader_cfg: dict,
) -> TrialResult:
    """å®¡æŸ¥æ¨¡å¼ï¼šAI ä¸€æ¬¡æ€§é˜…è¯»å…¨éƒ¨å†…å®¹ï¼Œç»™å‡ºç»“æž„åŒ–åé¦ˆ"""
    llm_calls = []
    
    # èŽ·å–ç³»ç»Ÿæç¤ºè¯ï¼ˆä¼˜å…ˆåŽå°é…ç½® > ç¡¬ç¼–ç  SIMULATOR_TYPESï¼‰
    type_info = SIMULATOR_TYPES.get(simulator_type, {})
    custom_prompt = config.get("system_prompt", "")
    persona_text = json.dumps(persona, ensure_ascii=False) if persona else ""
    
    if custom_prompt:
        # è‡ªå®šä¹‰æ¨¡æ¿ï¼š{content} å ä½ç¬¦æŒ‡å‘ user_messageï¼Œä¸åœ¨ system é‡Œå±•å¼€
        base_prompt = custom_prompt.replace("{content}", "ï¼ˆè§ä¸‹æ–¹å¾…è¯„ä¼°å†…å®¹ï¼‰").replace("{persona}", persona_text)
    else:
        base_prompt = type_info.get("system_prompt", "è¯·è¯„ä¼°ä»¥ä¸‹å†…å®¹ã€‚")
    
    # æ³¨å…¥ä¸Šä¸‹æ–‡ï¼ˆè§’è‰²èƒŒæ™¯æ”¾ systemï¼Œå†…å®¹æ”¾ userï¼‰
    system_prompt = base_prompt
    if creator_profile:
        system_prompt += f"\n\nã€åˆ›ä½œè€…ç‰¹è´¨ã€‘\n{creator_profile}"
    if intent:
        system_prompt += f"\n\nã€é¡¹ç›®æ„å›¾ã€‘\n{intent}"
    if persona:
        system_prompt += f"\n\nã€ç›®æ ‡æ¶ˆè´¹è€…ã€‘\n{persona_text}"
    
    # èŽ·å–è¯„åˆ†ç»´åº¦
    dimensions = grader_cfg.get("dimensions", []) or type_info.get("default_dimensions", ["ç»¼åˆè¯„ä»·"])
    dim_str = ", ".join([f'"{d}": åˆ†æ•°(1-10)' for d in dimensions])
    dim_comment_str = ", ".join([f'"{d}": "å…·ä½“è¯„è¯­ï¼ˆè‡³å°‘2å¥è¯ï¼‰"' for d in dimensions])
    
    # user_message: åªåœ¨è¿™é‡Œä¼ å†…å®¹ï¼ˆå”¯ä¸€ä¸€æ¬¡ï¼‰
    user_message = f"""ã€å¾…è¯„ä¼°å†…å®¹ã€‘
{content}

è¯·ä»¥ä½ çš„ä¸“ä¸šèº«ä»½è¿›è¡Œè¯„ä¼°ã€‚

**è¾“å‡ºJSONæ ¼å¼**ï¼ˆä¸¥æ ¼éµå¾ªï¼Œä¸è¦è¾“å‡ºå…¶ä»–å†…å®¹ï¼‰ï¼š
{{
    "scores": {{{dim_str}}},
    "comments": {{{dim_comment_str}}},
    "strengths": ["ä¼˜ç‚¹1", "ä¼˜ç‚¹2", "ä¼˜ç‚¹3"],
    "weaknesses": ["é—®é¢˜1", "é—®é¢˜2", "é—®é¢˜3"],
    "suggestions": ["å…·ä½“æ”¹è¿›å»ºè®®1", "å…·ä½“æ”¹è¿›å»ºè®®2", "å…·ä½“æ”¹è¿›å»ºè®®3"],
    "summary": "æ€»ä½“è¯„ä»·ï¼ˆ100-200å­—ï¼‰"
}}"""

    try:
        response_text, call = await _call_llm(
            system_prompt, user_message,
            step=f"simulator_{simulator_type}_review",
            temperature=0.6,
        )
        llm_calls.append(call)
        
        result_data = _parse_json_response(response_text)
        scores = result_data.get("scores", {})
        avg_score = sum(v for v in scores.values() if isinstance(v, (int, float))) / len(scores) if scores else 0
        
        # è¿è¡Œ Grader
        grader_outputs = []
        grader_result, grader_call = await _run_content_grader(
            content, result_data, dimensions, grader_cfg
        )
        if grader_call:
            llm_calls.append(grader_call)
            grader_outputs.append(grader_result)
        
        total_tokens_in = sum(c.tokens_in for c in llm_calls)
        total_tokens_out = sum(c.tokens_out for c in llm_calls)
        total_cost = sum(c.cost for c in llm_calls)
        
        return TrialResult(
            role=simulator_type,
            interaction_mode="review",
            nodes=[
                {"role": "system", "content": system_prompt[:500] + "..."},
                {"role": "user", "content": user_message[:500] + "..."},
                {"role": "assistant", "content": response_text},
            ],
            result={
                "scores": scores,
                "comments": result_data.get("comments", {}),
                "strengths": result_data.get("strengths", []),
                "weaknesses": result_data.get("weaknesses", []),
                "suggestions": result_data.get("suggestions", []),
                "outcome": "reviewed",
                "summary": result_data.get("summary", ""),
            },
            grader_outputs=grader_outputs,
            llm_calls=[c.to_dict() for c in llm_calls],
            overall_score=round(avg_score, 2),
            success=True,
            tokens_in=total_tokens_in,
            tokens_out=total_tokens_out,
            cost=total_cost,
        )
    except Exception as e:
        return TrialResult(
            role=simulator_type, interaction_mode="review",
            llm_calls=[c.to_dict() for c in llm_calls],
            success=False, error=str(e),
        )


async def _run_exploration(
    simulator_type: str,
    content: str,
    creator_profile: str,
    intent: str,
    persona: dict,
    config: dict,
    grader_cfg: dict,
    content_field_names: list = None,
) -> TrialResult:
    """
    æŽ¢ç´¢æ¨¡å¼ï¼šæ¨¡æ‹Ÿæ¶ˆè´¹è€…è‡ªä¸»åˆ¶å®šæŽ¢ç´¢æµç¨‹ï¼Œåœ¨å†…å®¹ä¸­å¯»æ‰¾ç­”æ¡ˆã€‚
    
    æµç¨‹ï¼š
    1. æ¶ˆè´¹è€…è§„åˆ’æŽ¢ç´¢è·¯å¾„ï¼ˆæ ¹æ®è‡ªèº«ç—›ç‚¹å†³å®šå…ˆçœ‹ä»€ä¹ˆï¼‰
    2. é€æ­¥æ‰§è¡ŒæŽ¢ç´¢ï¼Œè®°å½•æ¯ä¸€æ­¥çš„å‘çŽ°å’Œæ„Ÿå—
    3. æœ€ç»ˆç»™å‡ºç»“æž„åŒ–è¯„ä»·
    """
    llm_calls = []
    
    persona = persona or {"name": "å…¸åž‹ç”¨æˆ·", "background": "å¯¹è¯¥é¢†åŸŸæ„Ÿå…´è¶£çš„è¯»è€…"}
    user_name = persona.get("name", "æ¶ˆè´¹è€…")
    persona_text = json.dumps(persona, ensure_ascii=False, indent=2)
    
    content_name = "å†…å®¹"
    if content_field_names:
        content_name = f"ã€Š{content_field_names[0]}ã€‹" if len(content_field_names) == 1 else f"ã€Š{content_field_names[0]}ã€‹ç­‰{len(content_field_names)}ç¯‡"
    
    # æ ¹æ® persona çš„ç—›ç‚¹æŽ¨å¯¼æŽ¢ç´¢ä»»åŠ¡
    pain_points = persona.get("pain_points", [])
    task_hint = ""
    if pain_points:
        task_hint = f"ä½ æœ€æƒ³è§£å†³çš„é—®é¢˜ï¼š{'; '.join(pain_points[:3])}"
    else:
        task_hint = f"ä½ æƒ³äº†è§£ {content_name} æ˜¯å¦å¯¹ä½ æœ‰ç”¨"
    
    custom_prompt = config.get("system_prompt", "")
    
    # ===== ç¬¬ä¸€æ­¥ï¼šæ¶ˆè´¹è€…åˆ¶å®šæŽ¢ç´¢è®¡åˆ’ =====
    if custom_prompt:
        plan_system = custom_prompt.replace("{persona}", persona_text).replace("{content}", "ï¼ˆè§ä¸‹æ–¹ï¼‰").replace("{task}", task_hint)
        if persona_text not in plan_system:
            plan_system += f"\n\nã€ä½ æ‰®æ¼”çš„è§’è‰²ã€‘\n{persona_text}"
    else:
        plan_system = f"""ä½ æ­£åœ¨æ‰®æ¼”ä¸€ä½çœŸå®žç”¨æˆ·ã€‚

ã€ä½ çš„è§’è‰²ã€‘
{persona_text}

ã€èƒŒæ™¯ã€‘
ä½ é¢å‰æœ‰ä¸€ä»½å†…å®¹ï¼ˆ{content_name}ï¼‰ï¼Œä½ éœ€è¦æ ¹æ®è‡ªå·±çš„èƒŒæ™¯å’Œéœ€æ±‚æ¥æŽ¢ç´¢å®ƒã€‚
{task_hint}

ã€è¡Œä¸ºè¦æ±‚ã€‘
1. åƒçœŸå®žç”¨æˆ·ä¸€æ ·æ€è€ƒï¼šä½ ä¼šå…ˆçœ‹å“ªä¸ªéƒ¨åˆ†ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ
2. æ¯ä¸€æ­¥æŽ¢ç´¢éƒ½è¦è®°å½•ä½ çš„çœŸå®žæ„Ÿå—
3. å¦‚æžœå‘çŽ°å†…å®¹æœ‰ç¼ºå¤±æˆ–ä¸æ¸…æ¥šçš„åœ°æ–¹ï¼Œè¦æŒ‡å‡ºæ¥
4. æœ€ç»ˆåˆ¤æ–­è¿™ä¸ªå†…å®¹æ˜¯å¦å¯¹ä½ æœ‰å¸®åŠ©"""

    if creator_profile:
        plan_system += f"\n\nã€åˆ›ä½œè€…ç‰¹è´¨ã€‘\n{creator_profile}"
    if intent:
        plan_system += f"\n\nã€é¡¹ç›®æ„å›¾ã€‘\n{intent}"
    
    type_info = SIMULATOR_TYPES.get(simulator_type, {})
    dimensions = grader_cfg.get("dimensions", []) or type_info.get("default_dimensions", ["æ‰¾åˆ°ç­”æ¡ˆæ•ˆçŽ‡", "ä¿¡æ¯å®Œæ•´æ€§", "æ»¡æ„åº¦"])
    dim_str = ", ".join([f'"{d}": åˆ†æ•°(1-10)' for d in dimensions])
    dim_comment_str = ", ".join([f'"{d}": "å…·ä½“è¯„è¯­ï¼ˆè‡³å°‘2å¥è¯ï¼‰"' for d in dimensions])
    
    plan_user = f"""ä»¥ä¸‹æ˜¯ä½ è¦æŽ¢ç´¢çš„å†…å®¹ï¼š

=== å†…å®¹å¼€å§‹ ===
{content}
=== å†…å®¹ç»“æŸ ===

è¯·ä»¥ä½ çš„è§’è‰²èº«ä»½ï¼Œæ¨¡æ‹Ÿä½ çš„å®Œæ•´æŽ¢ç´¢è¿‡ç¨‹ã€‚

**è¾“å‡ºJSONæ ¼å¼**ï¼ˆä¸¥æ ¼éµå¾ªï¼Œä¸è¦è¾“å‡ºå…¶ä»–å†…å®¹ï¼‰ï¼š
{{
    "exploration_plan": "ä½ æ‰“ç®—æ€Žä¹ˆæµè§ˆè¿™ä¸ªå†…å®¹ï¼ˆ1-2å¥è¯ï¼‰",
    "exploration_steps": [
        {{
            "step": 1,
            "action": "å…ˆçœ‹äº†ä»€ä¹ˆéƒ¨åˆ†",
            "reason": "ä¸ºä»€ä¹ˆå…ˆçœ‹è¿™ä¸ª",
            "finding": "å‘çŽ°äº†ä»€ä¹ˆ",
            "feeling": "æ„Ÿå—å¦‚ä½•ï¼ˆæœ‰ç”¨/å›°æƒ‘/æƒŠå–œ/å¤±æœ›ç­‰ï¼‰"
        }},
        {{
            "step": 2,
            "action": "æŽ¥ç€çœ‹äº†ä»€ä¹ˆ",
            "reason": "ä¸ºä»€ä¹ˆ",
            "finding": "å‘çŽ°äº†ä»€ä¹ˆ",
            "feeling": "æ„Ÿå—"
        }}
    ],
    "attention_points": ["ç‰¹åˆ«å¸å¼•æ³¨æ„åŠ›çš„å†…å®¹1", "å†…å®¹2"],
    "found_answer": true,
    "answer_quality": "æ‰¾åˆ°çš„ç­”æ¡ˆè´¨é‡å¦‚ä½•ï¼ˆè¯¦ç»†/éƒ¨åˆ†/ä¸å¤Ÿç”¨ï¼‰",
    "difficulties": ["é‡åˆ°çš„å›°éš¾1", "å›°éš¾2"],
    "missing_info": ["å¸Œæœ›å†…å®¹ä¸­åŒ…å«ä½†æ²¡æœ‰çš„ä¿¡æ¯1", "ä¿¡æ¯2"],
    "scores": {{{dim_str}}},
    "comments": {{{dim_comment_str}}},
    "would_recommend": true,
    "summary": "ä½œä¸º{user_name}ï¼Œæ€»ä½“è¯„ä»·è¿™ä¸ªå†…å®¹å¯¹æˆ‘çš„å¸®åŠ©ç¨‹åº¦ï¼ˆ100-200å­—ï¼‰"
}}"""

    try:
        response_text, call = await _call_llm(
            plan_system, plan_user,
            step=f"explorer_{simulator_type}_exploration",
            temperature=0.7,
        )
        llm_calls.append(call)
        
        result_data = _parse_json_response(response_text)
        
        # æž„å»º exploration nodesï¼ˆå¯è§†åŒ–æŽ¢ç´¢è¿‡ç¨‹ï¼‰
        exploration_nodes = []
        
        # æŽ¢ç´¢è®¡åˆ’
        plan = result_data.get("exploration_plan", "")
        if plan:
            exploration_nodes.append({
                "role": "consumer", "content": f"ðŸ“‹ æŽ¢ç´¢è®¡åˆ’ï¼š{plan}", "turn": 0,
            })
        
        # æ¯ä¸€æ­¥æŽ¢ç´¢
        for step_data in result_data.get("exploration_steps", []):
            step_num = step_data.get("step", "?")
            action = step_data.get("action", "")
            reason = step_data.get("reason", "")
            finding = step_data.get("finding", "")
            feeling = step_data.get("feeling", "")
            
            step_content = f"ðŸ” æ­¥éª¤ {step_num}ï¼š{action}"
            if reason:
                step_content += f"\nðŸ’­ åŽŸå› ï¼š{reason}"
            if finding:
                step_content += f"\nðŸ“ å‘çŽ°ï¼š{finding}"
            if feeling:
                step_content += f"\nðŸ˜Š æ„Ÿå—ï¼š{feeling}"
            
            exploration_nodes.append({
                "role": "consumer", "content": step_content, "turn": step_num,
            })
        
        # æ³¨æ„åŠ›ç„¦ç‚¹
        attention = result_data.get("attention_points", [])
        if attention:
            exploration_nodes.append({
                "role": "system", "content": "â­ ç‰¹åˆ«å…³æ³¨çš„å†…å®¹ï¼š\n" + "\n".join(f"â€¢ {a}" for a in attention),
            })
        
        # å›°éš¾ä¸Žç¼ºå¤±
        difficulties = result_data.get("difficulties", [])
        missing = result_data.get("missing_info", [])
        if difficulties or missing:
            gap_text = ""
            if difficulties:
                gap_text += "âŒ é‡åˆ°çš„å›°éš¾ï¼š\n" + "\n".join(f"â€¢ {d}" for d in difficulties)
            if missing:
                gap_text += "\nâš ï¸ ç¼ºå¤±çš„ä¿¡æ¯ï¼š\n" + "\n".join(f"â€¢ {m}" for m in missing)
            exploration_nodes.append({
                "role": "system", "content": gap_text.strip(),
            })
        
        # æœ€ç»ˆè¯„ä»·
        found = result_data.get("found_answer", False)
        quality = result_data.get("answer_quality", "")
        summary = result_data.get("summary", "")
        exploration_nodes.append({
            "role": "consumer",
            "content": f"{'âœ…' if found else 'âŒ'} æ˜¯å¦æ‰¾åˆ°ç­”æ¡ˆï¼š{'æ˜¯' if found else 'å¦'}"
                       + (f"\nðŸ“Š ç­”æ¡ˆè´¨é‡ï¼š{quality}" if quality else "")
                       + (f"\n\n{summary}" if summary else ""),
        })
        
        scores = result_data.get("scores", {})
        avg_score = sum(v for v in scores.values() if isinstance(v, (int, float))) / len(scores) if scores else 0
        
        # è¿è¡Œ Grader
        grader_outputs = []
        grader_result, grader_call = await _run_content_grader(
            content, result_data, dimensions, grader_cfg
        )
        if grader_call:
            llm_calls.append(grader_call)
            grader_outputs.append(grader_result)
        
        total_tokens_in = sum(c.tokens_in for c in llm_calls)
        total_tokens_out = sum(c.tokens_out for c in llm_calls)
        total_cost = sum(c.cost for c in llm_calls)
        
        return TrialResult(
            role=simulator_type,
            interaction_mode="exploration",
            nodes=exploration_nodes,
            result={
                "scores": scores,
                "comments": result_data.get("comments", {}),
                "exploration_plan": plan,
                "exploration_steps": result_data.get("exploration_steps", []),
                "attention_points": attention,
                "found_answer": found,
                "answer_quality": quality,
                "difficulties": difficulties,
                "missing_info": missing,
                "strengths": attention,
                "weaknesses": difficulties + missing,
                "suggestions": missing,
                "outcome": "found_answer" if found else "not_found",
                "would_recommend": result_data.get("would_recommend", False),
                "summary": summary,
            },
            grader_outputs=grader_outputs,
            llm_calls=[c.to_dict() for c in llm_calls],
            overall_score=round(avg_score, 2),
            success=True,
            tokens_in=total_tokens_in,
            tokens_out=total_tokens_out,
            cost=total_cost,
        )
    except Exception as e:
        return TrialResult(
            role=simulator_type, interaction_mode="exploration",
            llm_calls=[c.to_dict() for c in llm_calls],
            success=False, error=str(e),
        )


async def _run_dialogue(
    simulator_type: str,
    content: str,
    creator_profile: str,
    intent: str,
    persona: dict,
    config: dict,
    grader_cfg: dict,
    content_field_names: list = None,
) -> TrialResult:
    """å¯¹è¯æ¨¡å¼ï¼šå¤šè½®äº¤äº’ï¼Œæ¨¡æ‹ŸçœŸå®žç”¨æˆ·ä¸Žå†…å®¹çš„äº’åŠ¨"""
    llm_calls = []
    interaction_log = []
    max_turns = config.get("max_turns", 5)
    
    persona = persona or {"name": "å…¸åž‹ç”¨æˆ·", "background": "å¯¹è¯¥é¢†åŸŸæ„Ÿå…´è¶£çš„è¯»è€…"}
    user_name = persona.get("name", "æ¶ˆè´¹è€…")
    persona_text = json.dumps(persona, ensure_ascii=False, indent=2)
    
    content_name = "å†…å®¹"
    if content_field_names:
        content_name = f"ã€Š{content_field_names[0]}ã€‹" if len(content_field_names) == 1 else f"ã€Š{content_field_names[0]}ã€‹ç­‰{len(content_field_names)}ç¯‡"
    
    # æ ¹æ® simulator_type æˆ– interaction_type ç¡®å®šå¯¹è¯æ–¹å‘
    # decision ç±»åž‹æ¨¡æ‹Ÿå™¨ = é”€å”®æ–¹ä¸»åŠ¨ â†’ prompt_template æ˜¯å–æ–¹æç¤ºè¯
    # å¿…é¡»è·¯ç”±åˆ° _run_seller_dialogueï¼Œå¦åˆ™è§’è‰²ä¼šå€’ç½®
    interaction_type = config.get("interaction_type", "")
    if simulator_type == "seller" or interaction_type == "decision":
        # é”€å”®/å†³ç­–æ¨¡å¼ï¼šå†…å®¹ä»£è¡¨ä¸»åŠ¨ï¼Œæ¶ˆè´¹è€…å›žåº”
        return await _run_seller_dialogue(
            content, persona, config, grader_cfg, llm_calls, content_field_names
        )
    
    # ç”¨æˆ·è‡ªå®šä¹‰çš„ simulator æç¤ºè¯ï¼ˆæ¥è‡ªåŽå°é…ç½®ï¼‰
    custom_sim_prompt = config.get("system_prompt", "")
    sim_name = config.get("simulator_name", simulator_type)
    
    # ===== æ¶ˆè´¹è€…/ç¬¬ä¸€æ–¹æç¤ºè¯ =====
    if custom_sim_prompt:
        # ç”¨åŽå°é…ç½®çš„æç¤ºè¯ï¼Œæ›¿æ¢å ä½ç¬¦
        consumer_system = custom_sim_prompt.replace("{persona}", persona_text).replace("{content}", content)
        if persona_text not in consumer_system and "{persona}" not in custom_sim_prompt:
            consumer_system += f"\n\nã€ä½ æ‰®æ¼”çš„æ¶ˆè´¹è€…è§’è‰²ã€‘\n{persona_text}"
    else:
        consumer_system = f"""ä½ æ­£åœ¨æ‰®æ¼”ä¸€ä½çœŸå®žç”¨æˆ·è¿›è¡Œæ¨¡æ‹Ÿå¯¹è¯ã€‚

ã€ä½ çš„è§’è‰²ã€‘
{persona_text}

ã€ä½ çš„ç›®æ ‡ã€‘
ä½ æœ‰ä¸€äº›å›°æƒ‘å’Œé—®é¢˜æƒ³è¦è§£å†³ã€‚ä½ æ­£åœ¨é€šè¿‡é˜…è¯»/å’¨è¯¢{content_name}æ¥å¯»æ‰¾ç­”æ¡ˆã€‚

ã€è¡Œä¸ºè¦æ±‚ã€‘
1. æ¯æ¬¡åªé—®ä¸€ä¸ªé—®é¢˜ï¼Œè¡¨è¾¾ç®€çŸ­è‡ªç„¶
2. é—®é¢˜è¦åŸºäºŽä½ çš„çœŸå®žèƒŒæ™¯å’Œç—›ç‚¹
3. å¦‚æžœå¯¹æ–¹çš„å›žç­”è®©ä½ æ»¡æ„ï¼Œå¯ä»¥è¡¨ç¤ºæ„Ÿè°¢
4. å¦‚æžœå¯¹æ–¹çš„å›žç­”ä¸å¤Ÿå¥½ï¼Œç»§ç»­è¿½é—®
5. å¦‚æžœè§‰å¾—å·²ç»äº†è§£è¶³å¤Ÿäº†ï¼Œè¯´"å¥½çš„ï¼Œæˆ‘äº†è§£äº†"ç»“æŸå¯¹è¯"""

    # ===== å†…å®¹ä»£è¡¨/ç¬¬äºŒæ–¹æç¤ºè¯ =====
    custom_secondary = config.get("secondary_prompt", "")
    if custom_secondary:
        # ç”¨åŽå°é…ç½®çš„æç¤ºè¯ï¼Œæ›¿æ¢å ä½ç¬¦
        content_system = custom_secondary.replace("{content}", content).replace("{persona}", persona_text)
        # ===== é˜²å¾¡æ€§æ£€æŸ¥ï¼šå¦‚æžœæ¨¡æ¿ä¸­æ²¡æœ‰ {content} å ä½ç¬¦ï¼Œå†…å®¹æœªè¢«æ³¨å…¥ =====
        # è¿™æ˜¯æœ€å…³é”®çš„ä¿®å¤ï¼šç¡®ä¿å†…å®¹ä»£è¡¨ä¸€å®šèƒ½çœ‹åˆ°é¡¹ç›®å†…å®¹
        if "{content}" not in custom_secondary and content and content.strip():
            content_system += f"\n\n=== ä½ å¿…é¡»ä¸¥æ ¼åŸºäºŽä»¥ä¸‹å†…å®¹å›žç­” ===\n{content}\n=== å†…å®¹ç»“æŸ ==="
    else:
        content_system = f"""ä½ æ˜¯{content_name}çš„å†…å®¹ä»£è¡¨ï¼Œä¸¥æ ¼åŸºäºŽä»¥ä¸‹å†…å®¹å›žç­”é—®é¢˜ã€‚

=== å†…å®¹å¼€å§‹ ===
{content}
=== å†…å®¹ç»“æŸ ===

ã€å›žç­”è§„åˆ™ã€‘
1. ä¸¥æ ¼åŸºäºŽå†…å®¹å›žç­”ï¼Œä¸è¦ç¼–é€ 
2. å¦‚æžœå†…å®¹ä¸­æ²¡æœ‰æ¶‰åŠï¼Œè¯šå®žè¯´æ˜Ž
3. å°½é‡å¼•ç”¨å†…å®¹ä¸­çš„åŽŸè¯æˆ–æ ¸å¿ƒè§‚ç‚¹"""

    try:
        for turn in range(max_turns):
            # æ¶ˆè´¹è€…æé—®
            user_messages = [ChatMessage(role="system", content=consumer_system)]
            for log in interaction_log:
                if log["role"] == "consumer":
                    user_messages.append(ChatMessage(role="assistant", content=log["content"]))
                else:
                    user_messages.append(ChatMessage(role="user", content=log["content"]))
            
            prompt = "è¯·åŸºäºŽä½ çš„èƒŒæ™¯ï¼Œæå‡ºä½ æœ€æƒ³è§£å†³çš„ç¬¬ä¸€ä¸ªé—®é¢˜ã€‚" if turn == 0 else "è¯·åŸºäºŽä¹‹å‰çš„å¯¹è¯ï¼Œç»§ç»­ä½ çš„å’¨è¯¢ã€‚"
            user_messages.append(ChatMessage(role="user", content=prompt))
            
            user_response_text, user_call = await _call_llm_multi(
                user_messages, step=f"consumer_turn_{turn+1}", temperature=0.8
            )
            llm_calls.append(user_call)
            
            interaction_log.append({
                "role": "consumer", "name": user_name,
                "content": user_response_text, "turn": turn + 1,
            })
            
            # æ£€æŸ¥æ˜¯å¦ç»“æŸ
            end_signals = ["äº†è§£äº†", "æ˜Žç™½äº†", "å¥½çš„è°¢è°¢", "è°¢è°¢", "å†è§", "ä¸éœ€è¦äº†", "è¶³å¤Ÿäº†", "æ¸…æ¥šäº†"]
            if any(s in user_response_text for s in end_signals):
                break
            
            # å†…å®¹ä»£è¡¨å›žå¤
            content_messages = [ChatMessage(role="system", content=content_system)]
            for log in interaction_log:
                if log["role"] == "consumer":
                    content_messages.append(ChatMessage(role="user", content=log["content"]))
                else:
                    content_messages.append(ChatMessage(role="assistant", content=log["content"]))
            
            content_response_text, content_call = await _call_llm_multi(
                content_messages, step=f"content_rep_turn_{turn+1}", temperature=0.5
            )
            llm_calls.append(content_call)
            
            interaction_log.append({
                "role": "content_rep", "name": content_name,
                "content": content_response_text, "turn": turn + 1,
            })
        
        # è¯„ä¼°é˜¶æ®µ - å†…å®¹è¯„åˆ† (Grader)
        dialogue_transcript = "\n".join([
            f"[{log.get('name', log['role'])}]: {log['content']}"
            for log in interaction_log
        ])
        
        type_info = SIMULATOR_TYPES.get(simulator_type, {})
        dimensions = grader_cfg.get("dimensions", []) or type_info.get("default_dimensions", ["ç»¼åˆè¯„ä»·"])
        dim_str = ", ".join([f'"{d}": åˆ†æ•°(1-10)' for d in dimensions])
        
        eval_system = f"""ä½ æ˜¯{user_name}ï¼Œåˆšåˆšå®Œæˆäº†ä¸€æ¬¡å’¨è¯¢å¯¹è¯ã€‚
ä½ çš„èƒŒæ™¯ï¼š{persona_text}
è¯·è¯„ä¼°å†…å®¹å¯¹ä½ çš„å¸®åŠ©ç¨‹åº¦ã€‚"""
        
        eval_user = f"""å¯¹è¯è®°å½•ï¼š
{dialogue_transcript}

è¯·ä»¥JSONæ ¼å¼è¾“å‡ºï¼š
{{
    "scores": {{{dim_str}}},
    "comments": {{{", ".join([f'"{d}": "è¯„è¯­"' for d in dimensions])}}},
    "problems_solved": ["è¢«è§£å†³çš„é—®é¢˜"],
    "problems_unsolved": ["æœªè¢«è§£å†³çš„é—®é¢˜"],
    "content_gaps": ["å†…å®¹ç¼ºå¤±çš„éƒ¨åˆ†"],
    "would_recommend": true,
    "summary": "æ€»ä½“è¯„ä»·ï¼ˆ100å­—ä»¥å†…ï¼‰"
}}"""
        
        eval_text, eval_call = await _call_llm(
            eval_system, eval_user,
            step=f"grader_content_{simulator_type}",
            temperature=0.5,
        )
        llm_calls.append(eval_call)
        
        result_data = _parse_json_response(eval_text)
        scores = result_data.get("scores", {})
        avg_score = sum(v for v in scores.values() if isinstance(v, (int, float))) / len(scores) if scores else 0
        
        # è¿‡ç¨‹è¯„åˆ† (Process Grader)
        grader_outputs = []
        process_grader, process_call = await _run_process_grader(
            dialogue_transcript, dimensions, grader_cfg
        )
        if process_call:
            llm_calls.append(process_call)
            grader_outputs.append(process_grader)
        
        total_tokens_in = sum(c.tokens_in for c in llm_calls)
        total_tokens_out = sum(c.tokens_out for c in llm_calls)
        total_cost = sum(c.cost for c in llm_calls)
        
        return TrialResult(
            role=simulator_type,
            interaction_mode="dialogue",
            nodes=[{"role": log["role"], "content": log["content"], "turn": log.get("turn")} for log in interaction_log],
            result={
                "scores": scores,
                "comments": result_data.get("comments", {}),
                "strengths": result_data.get("problems_solved", []),
                "weaknesses": result_data.get("problems_unsolved", []),
                "suggestions": result_data.get("content_gaps", []),
                "outcome": "recommended" if result_data.get("would_recommend") else "not_recommended",
                "summary": result_data.get("summary", ""),
            },
            grader_outputs=grader_outputs,
            llm_calls=[c.to_dict() for c in llm_calls],
            overall_score=round(avg_score, 2),
            success=True,
            tokens_in=total_tokens_in,
            tokens_out=total_tokens_out,
            cost=total_cost,
        )
    except Exception as e:
        return TrialResult(
            role=simulator_type, interaction_mode="dialogue",
            nodes=[{"role": log["role"], "content": log["content"]} for log in interaction_log],
            llm_calls=[c.to_dict() for c in llm_calls],
            success=False, error=str(e),
        )


async def _run_seller_dialogue(
    content: str,
    persona: dict,
    config: dict,
    grader_cfg: dict,
    llm_calls: list,
    content_field_names: list = None,
) -> TrialResult:
    """é”€å”®å¯¹è¯ï¼šé”€å”®é¡¾é—®ä¸»åŠ¨æŽ¨ä»‹ï¼Œæ¶ˆè´¹è€…å›žåº”"""
    max_turns = config.get("max_turns", 8)
    consumer_name = persona.get("name", "æ¶ˆè´¹è€…")
    persona_text = json.dumps(persona, ensure_ascii=False, indent=2)
    interaction_log = []
    
    # é”€å”®æ–¹æç¤ºè¯ï¼šä¼˜å…ˆç”¨åŽå°é…ç½®
    custom_primary = config.get("system_prompt", "")
    if custom_primary:
        seller_system = custom_primary.replace("{content}", content).replace("{persona}", persona_text)
        # é˜²å¾¡æ€§æ£€æŸ¥ï¼šé”€å”®æ–¹å¿…é¡»çŸ¥é“å†…å®¹
        if "{content}" not in custom_primary and content and content.strip():
            seller_system += f"\n\n=== ä½ æŽŒæ¡çš„å†…å®¹ï¼ˆå¿…é¡»åŸºäºŽæ­¤é”€å”®ï¼‰===\n{content}\n=== å†…å®¹ç»“æŸ ==="
    else:
        seller_system = f"""ä½ æ˜¯è¿™ä¸ªå†…å®¹çš„é”€å”®é¡¾é—®ã€‚ä½ æ·±å…¥äº†è§£å†…å®¹çš„æ¯ä¸ªç»†èŠ‚ã€‚

=== ä½ æŽŒæ¡çš„å†…å®¹ ===
{content}
=== å†…å®¹ç»“æŸ ===

ã€ä½ çš„ç›®æ ‡æ¶ˆè´¹è€…ã€‘
{persona_text}

ã€é”€å”®ç­–ç•¥ã€‘
Phase 1 (ç¬¬1è½®): æœ‰å¸å¼•åŠ›çš„å¼€åœºç™½ï¼ŒåŒæ—¶æå‡ºä¸€ä¸ªäº†è§£éœ€æ±‚çš„é—®é¢˜
Phase 2 (ç¬¬2-3è½®): æ·±å…¥äº†è§£æ¶ˆè´¹è€…çš„å…·ä½“éœ€æ±‚å’Œç—›ç‚¹
Phase 3 (ç¬¬4-5è½®): åŒ¹é…å†…å®¹ä¸­çš„ä»·å€¼ç‚¹åˆ°æ¶ˆè´¹è€…éœ€æ±‚
Phase 4 (ç¬¬6-7è½®): å¤„ç†å¼‚è®®
Phase 5 (æœ€åŽ): æ€»ç»“ä»·å€¼ï¼Œè¯¢é—®å†³å®š

ã€è¡Œä¸ºè¦æ±‚ã€‘- ä¸»åŠ¨å¼•å¯¼å¯¹è¯ - å¼•ç”¨å…·ä½“ä¿¡æ¯ - è¯šå®žä½†æœ‰è¯´æœåŠ› - æ¯æ¬¡200å­—ä»¥å†…"""

    # æ¶ˆè´¹è€…æ–¹æç¤ºè¯ï¼šä¼˜å…ˆç”¨åŽå°é…ç½®çš„ secondary_prompt
    custom_secondary = config.get("secondary_prompt", "")
    if custom_secondary:
        consumer_system = custom_secondary.replace("{persona}", persona_text).replace("{content}", content)
    else:
        consumer_system = f"""ä½ æ˜¯ä¸€ä½çœŸå®žçš„æ½œåœ¨ç”¨æˆ·ã€‚æœ‰äººæ­£åœ¨å‘ä½ æŽ¨ä»‹å†…å®¹/äº§å“ã€‚

ã€ä½ çš„èº«ä»½ã€‘
{persona_text}

ã€ä½ çš„æ€åº¦ã€‘- æœ‰çœŸå®žéœ€æ±‚ï¼Œä½†ä¸è½»æ˜“è¢«è¯´æœ - ä¼šæå‡ºçœŸå®žè´¨ç–‘ - å¦‚æžœç¡®å®žæœ‰ä»·å€¼ï¼Œæ„¿æ„æŽ¥å— - ä¸é€‚åˆå°±æ˜Žç¡®æ‹’ç»

ã€è¡Œä¸ºè¦æ±‚ã€‘åŸºäºŽçœŸå®žèƒŒæ™¯å›žåº”ï¼Œé€‚å½“è´¨ç–‘ï¼Œæœ€åŽåšå‡ºæ˜Žç¡®å†³å®šã€‚"""

    try:
        for turn in range(max_turns):
            # é”€å”®å‘è¨€
            seller_messages = [ChatMessage(role="system", content=seller_system)]
            for log in interaction_log:
                if log["role"] == "seller":
                    seller_messages.append(ChatMessage(role="assistant", content=log["content"]))
                else:
                    seller_messages.append(ChatMessage(role="user", content=log["content"]))
            seller_messages.append(ChatMessage(role="user", content="è¯·å¼€å§‹ä½ çš„é”€å”®å¼€åœºç™½ã€‚" if turn == 0 else "è¯·ç»§ç»­ã€‚"))
            
            seller_text, seller_call = await _call_llm_multi(
                seller_messages, step=f"seller_turn_{turn+1}", temperature=0.7
            )
            llm_calls.append(seller_call)
            interaction_log.append({"role": "seller", "name": "é”€å”®é¡¾é—®", "content": seller_text, "turn": turn + 1, "phase": _get_sales_phase(turn)})
            
            # æ¶ˆè´¹è€…å›žåº”
            consumer_messages = [ChatMessage(role="system", content=consumer_system)]
            for log in interaction_log:
                if log["role"] == "consumer":
                    consumer_messages.append(ChatMessage(role="assistant", content=log["content"]))
                else:
                    consumer_messages.append(ChatMessage(role="user", content=log["content"]))
            
            consumer_text, consumer_call = await _call_llm_multi(
                consumer_messages, step=f"consumer_turn_{turn+1}", temperature=0.8
            )
            llm_calls.append(consumer_call)
            interaction_log.append({"role": "consumer", "name": consumer_name, "content": consumer_text, "turn": turn + 1})
            
            # æ£€æŸ¥å†³å®š
            decision_signals = ["æˆ‘å†³å®š", "æˆ‘æŽ¥å—", "æˆ‘ä¸éœ€è¦", "æˆ‘æ‹’ç»", "å¯ä»¥", "å¥½çš„"]
            if turn >= 3 and any(s in consumer_text for s in decision_signals):
                break
        
        # è¯„ä¼°
        dialogue_transcript = "\n".join([f"[{log.get('name', log['role'])}]: {log['content']}" for log in interaction_log])
        
        dimensions = grader_cfg.get("dimensions", []) or SIMULATOR_TYPES.get("seller", {}).get("default_dimensions", ["ç»¼åˆè¯„ä»·"])
        dim_str = ", ".join([f'"{d}": åˆ†æ•°(1-10)' for d in dimensions])
        
        eval_text, eval_call = await _call_llm(
            "ä½ æ˜¯ä¸€ä½é”€å”®æ•ˆæžœè¯„ä¼°ä¸“å®¶ã€‚è¯·åˆ†æžä»¥ä¸‹é”€å”®å¯¹è¯çš„æ•ˆæžœã€‚",
            f"""é”€å”®å¯¹è¯è®°å½•ï¼š
{dialogue_transcript}

è¯·ä»¥JSONæ ¼å¼è¾“å‡ºï¼š
{{
    "scores": {{{dim_str}}},
    "comments": {{{", ".join([f'"{d}": "è¯„è¯­"' for d in dimensions])}}},
    "conversion": true,
    "conversion_factors": ["å› ç´ "],
    "rejection_factors": ["å› ç´ "],
    "content_strengths": ["ä¼˜åŠ¿"],
    "content_gaps": ["ç¼ºå¤±"],
    "summary": "é”€å”®æ•ˆæžœæ€»ä½“è¯„ä»·ï¼ˆ100-200å­—ï¼‰"
}}""",
            step="grader_content_seller",
            temperature=0.5,
        )
        llm_calls.append(eval_call)
        
        result_data = _parse_json_response(eval_text)
        scores = result_data.get("scores", {})
        avg_score = sum(v for v in scores.values() if isinstance(v, (int, float))) / len(scores) if scores else 0
        
        # è¿‡ç¨‹è¯„åˆ†
        grader_outputs = []
        process_grader, process_call = await _run_process_grader(dialogue_transcript, dimensions, grader_cfg)
        if process_call:
            llm_calls.append(process_call)
            grader_outputs.append(process_grader)
        
        total_tokens_in = sum(c.tokens_in for c in llm_calls)
        total_tokens_out = sum(c.tokens_out for c in llm_calls)
        total_cost = sum(c.cost for c in llm_calls)
        
        return TrialResult(
            role="seller", interaction_mode="dialogue",
            nodes=[{"role": log["role"], "content": log["content"], "turn": log.get("turn"), "phase": log.get("phase")} for log in interaction_log],
            result={
                "scores": scores, "comments": result_data.get("comments", {}),
                "strengths": result_data.get("content_strengths", []),
                "weaknesses": result_data.get("content_gaps", []),
                "suggestions": result_data.get("rejection_factors", []),
                "conversion_factors": result_data.get("conversion_factors", []),
                "outcome": "converted" if result_data.get("conversion") else "not_converted",
                "summary": result_data.get("summary", ""),
            },
            grader_outputs=grader_outputs,
            llm_calls=[c.to_dict() for c in llm_calls],
            overall_score=round(avg_score, 2),
            success=True,
            tokens_in=total_tokens_in, tokens_out=total_tokens_out, cost=total_cost,
        )
    except Exception as e:
        return TrialResult(
            role="seller", interaction_mode="dialogue",
            nodes=[{"role": log["role"], "content": log["content"]} for log in interaction_log],
            llm_calls=[c.to_dict() for c in llm_calls],
            success=False, error=str(e),
        )


# ============== Grader ç³»ç»Ÿ ==============

async def run_individual_grader(
    grader_name: str,
    grader_type: str,
    prompt_template: str,
    dimensions: list,
    content: str,
    trial_result_data: dict,
    process_transcript: str = "",
) -> Tuple[dict, Optional[LLMCall]]:
    """
    è¿è¡Œå•ä¸ª Graderã€‚
    
    æ ¸å¿ƒåŽŸåˆ™ï¼šprompt_template å°±æ˜¯å‘é€ç»™ LLM çš„å®Œæ•´ system_promptã€‚
    å¼•æ“Žåªè´Ÿè´£æ›¿æ¢å ä½ç¬¦ {content} / {process}ï¼Œä¸é¢å¤–æ‹¼æŽ¥ä»»ä½•å†…å®¹ã€‚
    ç”¨æˆ·åœ¨åŽå°çœ‹åˆ°çš„æç¤ºè¯ = LLM å®žé™…æ”¶åˆ°çš„æç¤ºè¯ã€‚
    
    Args:
        grader_name: è¯„åˆ†å™¨åç§°
        grader_type: "content_only" / "content_and_process"
        prompt_template: å®Œæ•´çš„è¯„åˆ†æç¤ºè¯ï¼Œæ”¯æŒ {content} å’Œ {process} å ä½ç¬¦
        dimensions: è¯„åˆ†ç»´åº¦åˆ—è¡¨ï¼ˆç”¨äºŽè§£æžç»“æžœï¼‰
        content: è¢«è¯„ä¼°çš„å†…å®¹
        trial_result_data: è¯•éªŒç»“æžœï¼ˆå¤‡ç”¨ï¼Œæ¨¡æ¿ä¸­æ— å¼•ç”¨åˆ™ä¸ä½¿ç”¨ï¼‰
        process_transcript: äº’åŠ¨è¿‡ç¨‹è®°å½•
    
    Returns:
        (grader_output_dict, LLMCall_or_None)
    """
    dims = dimensions or ["ç»¼åˆè¯„ä»·"]
    
    # ===== æ ¸å¿ƒï¼šæ¨¡æ¿å°±æ˜¯æœ€ç»ˆæç¤ºè¯ï¼Œåªåšå ä½ç¬¦æ›¿æ¢ =====
    raw_template = prompt_template or ""
    
    if raw_template:
        # ç›´æŽ¥æ›¿æ¢å ä½ç¬¦
        system_prompt = raw_template
        system_prompt = system_prompt.replace("{content}", content[:6000] if content else "ï¼ˆæ— å†…å®¹ï¼‰")
        
        # {process}: content_and_process ç±»åž‹æ‰å¡«å……ï¼Œå¦åˆ™æ ‡æ³¨æ— 
        if grader_type == "content_and_process" and process_transcript:
            system_prompt = system_prompt.replace("{process}", process_transcript[:4000])
        else:
            system_prompt = system_prompt.replace("{process}", "ï¼ˆæ— äº’åŠ¨è¿‡ç¨‹ï¼‰")
    else:
        # æ— æ¨¡æ¿æ—¶çš„å…œåº•ï¼ˆä¸åº”å‘ç”Ÿï¼Œé¢„ç½®è¯„åˆ†å™¨éƒ½æœ‰æ¨¡æ¿ï¼‰
        dim_score_str = ", ".join([f'"{d}": åˆ†æ•°(1-10)' for d in dims])
        dim_comment_str = ", ".join([f'"{d}": "è¯„è¯­"' for d in dims])
        
        process_section = ""
        if grader_type == "content_and_process" and process_transcript:
            process_section = f"\n\nã€äº’åŠ¨è¿‡ç¨‹è®°å½•ã€‘\n{process_transcript[:4000]}"
        
        system_prompt = f"""ä½ æ˜¯ã€Œ{grader_name}ã€ï¼Œè¯·å¯¹ä»¥ä¸‹å†…å®¹è¿›è¡Œå®¢è§‚ã€ä¸¥è°¨çš„è¯„åˆ†ã€‚

ã€è¢«è¯„ä¼°å†…å®¹ã€‘
{content[:6000] if content else 'ï¼ˆæ— å†…å®¹ï¼‰'}{process_section}

ã€è¯„ä¼°ç»´åº¦ã€‘
{chr(10).join([f'{i+1}. {d} (1-10)' for i, d in enumerate(dims)])}

è¯·ä¸¥æ ¼è¾“å‡ºä»¥ä¸‹ JSON æ ¼å¼ï¼Œä¸è¦è¾“å‡ºå…¶ä»–å†…å®¹ï¼š
{{"scores": {{{dim_score_str}}}, "comments": {{{dim_comment_str}}}, "feedback": "æ•´ä½“è¯„ä»·å’Œæ”¹è¿›å»ºè®®ï¼ˆ100-200å­—ï¼‰"}}"""

    # user_message: ç®€å•æŒ‡ä»¤å³å¯ï¼Œæ‰€æœ‰ä¿¡æ¯å·²åœ¨ system_prompt ä¸­
    user_message = "è¯·æ ¹æ®ä¸Šè¿°è¦æ±‚è¿›è¡Œè¯„åˆ†ï¼Œä¸¥æ ¼æŒ‰ç…§æŒ‡å®šçš„ JSON æ ¼å¼è¾“å‡ºã€‚"

    try:
        text, call = await _call_llm(
            system_prompt, user_message,
            step=f"grader_{grader_name}",
            temperature=0.4,
        )
        result = _parse_json_response(text)
        
        scores = result.get("scores", {})
        valid_scores = [v for v in scores.values() if isinstance(v, (int, float))]
        overall = round(sum(valid_scores) / len(valid_scores), 2) if valid_scores else 0
        
        return {
            "grader_name": grader_name,
            "grader_type": grader_type,
            "overall": overall,
            "scores": scores,
            "comments": result.get("comments", {}),
            "feedback": result.get("feedback", result.get("analysis", "")),
        }, call
    except Exception as e:
        return {
            "grader_name": grader_name,
            "grader_type": grader_type,
            "overall": None,
            "scores": {},
            "feedback": f"è¯„åˆ†å¤±è´¥: {str(e)}",
            "error": str(e),
        }, None


async def _run_content_grader(
    content: str,
    trial_result_data: dict,
    dimensions: list,
    grader_cfg: dict,
) -> Tuple[dict, Optional[LLMCall]]:
    """
    å†…å®¹è¯„åˆ†å™¨ - ç›´æŽ¥è¯„ä»·å†…å®¹æœ¬èº«çš„è´¨é‡ï¼ˆå…¼å®¹æ—§é€»è¾‘ï¼‰
    """
    custom_prompt = grader_cfg.get("custom_prompt", "")
    grader_type = grader_cfg.get("type", "content")
    
    if grader_type not in ("content", "combined"):
        return {}, None
    
    # è½¬ä¸ºæ–°çš„ run_individual_grader
    return await run_individual_grader(
        grader_name="å†…å®¹è´¨é‡è¯„åˆ†",
        grader_type="content_only",
        prompt_template=custom_prompt,
        dimensions=dimensions or ["ç»¼åˆè¯„ä»·"],
        content=content,
        trial_result_data=trial_result_data,
    )


async def _run_process_grader(
    dialogue_transcript: str,
    dimensions: list,
    grader_cfg: dict,
) -> Tuple[dict, Optional[LLMCall]]:
    """
    è¿‡ç¨‹è¯„åˆ†å™¨ - è¯„ä»·äº’åŠ¨è¿‡ç¨‹çš„è´¨é‡ï¼ˆå…¼å®¹æ—§é€»è¾‘ï¼‰
    """
    grader_type = grader_cfg.get("type", "content")
    
    if grader_type not in ("process", "combined"):
        return {}, None
    
    return await run_individual_grader(
        grader_name="äº’åŠ¨è¿‡ç¨‹è¯„åˆ†",
        grader_type="content_and_process",
        prompt_template="",
        dimensions=["å¯¹è¯æµç•…æ€§", "é—®é¢˜è§£å†³æ•ˆçŽ‡", "ä¿¡æ¯ä¼ é€’æœ‰æ•ˆæ€§", "ç”¨æˆ·ä½“éªŒ"],
        content="",
        trial_result_data={},
        process_transcript=dialogue_transcript,
    )


# ============== Diagnoser ==============

async def run_diagnoser(
    trial_results: List[TrialResult],
    content_summary: str = "",
    intent: str = "",
) -> Tuple[dict, Optional[LLMCall]]:
    """
    è·¨ Trial ç»¼åˆè¯Šæ–­ - ä¸ç»™åˆ†æ•°ï¼Œåªåšå®šæ€§åˆ†æžï¼š
    è¯„ä¼°äº†å“ªäº›å†…å®¹å—ã€ç”¨äº†ä»€ä¹ˆæ–¹æ³•ã€å¥½åœ¨å“ªã€å“ªé‡Œéœ€è¦æå‡ã€‚
    
    Returns:
        (diagnosis_dict, LLMCall_or_None)
    """
    if not trial_results:
        return {"summary": "æ— å¯åˆ†æžçš„Trialç»“æžœ", "content_blocks_evaluated": [], "improvements": []}, None
    
    trials_summary = []
    for tr in trial_results:
        if not tr.success:
            continue
        display_name = tr.role_display_name or SIMULATOR_TYPES.get(tr.role, {}).get("name", tr.role)
        mode_label = {"review": "å®¡æŸ¥æ¨¡å¼", "dialogue": "å¯¹è¯æ¨¡å¼", "scenario": "æƒ…æ™¯æ¨¡å¼"}.get(tr.interaction_mode, tr.interaction_mode)
        summary_text = f"""### {display_name}ï¼ˆ{mode_label}ï¼‰
- ç»“æžœ: {tr.result.get('outcome', 'N/A')}
- æ€»ç»“: {tr.result.get('summary', 'N/A')}
- ä¼˜ç‚¹: {', '.join(tr.result.get('strengths', []))}
- é—®é¢˜: {', '.join(tr.result.get('weaknesses', []))}
- å»ºè®®: {', '.join(tr.result.get('suggestions', []))}"""
        trials_summary.append(summary_text)
    
    trials_text = "\n\n---\n\n".join(trials_summary)
    
    system_prompt = """ä½ æ˜¯ä¸€ä½å†…å®¹è¯„ä¼°è¯Šæ–­ä¸“å®¶ã€‚è¯·åŸºäºŽå¤šä¸ªè¯•éªŒçš„å®šæ€§åé¦ˆï¼Œå†™ä¸€ä»½ç®€æ´æ‰¼è¦çš„ç»¼åˆè¯Šæ–­æŠ¥å‘Šã€‚

**é‡è¦ï¼šä¸è¦ç»™å‡ºä»»ä½•åˆ†æ•°ã€‚** åªåšå®šæ€§åˆ†æžã€‚

æŠ¥å‘Šç»“æž„ï¼š
1. æ€»è§ˆï¼šä¸€å…±è¯„ä¼°äº†å¤šå°‘ä¸ªå†…å®¹å—ï¼Œæ¯ä¸ªç”¨çš„ä»€ä¹ˆè¯„ä¼°æ–¹æ³•ï¼ˆå®¡æŸ¥/å¯¹è¯/æƒ…æ™¯ï¼‰
2. äº®ç‚¹ï¼šå†…å®¹åšå¾—å¥½çš„åœ°æ–¹ï¼ˆè·¨è¯•éªŒå…±è¯†ï¼‰
3. å¾…æå‡ï¼šéœ€è¦æ”¹è¿›çš„å…³é”®é—®é¢˜ï¼ˆæŒ‰ä¼˜å…ˆçº§æŽ’åºï¼‰
4. å»ºè®®ï¼šæœ€å€¼å¾—å…ˆè¡ŒåŠ¨çš„ 2-3 æ¡æ”¹è¿›å»ºè®®

è¯·è¾“å‡ºä¸¥æ ¼çš„JSONæ ¼å¼ï¼Œè¯­è¨€ç®€æ´ç›´æŽ¥ã€‚"""

    user_message = f"""# é¡¹ç›®æ„å›¾
{intent or 'æœªæä¾›'}

# å„è¯•éªŒè¯„ä¼°åé¦ˆï¼ˆå…± {len(trial_results)} ä¸ªè¯•éªŒï¼‰

{trials_text}

è¯·è¾“å‡ºJSONæ ¼å¼ï¼š
{{
    "overview": "æ€»è§ˆï¼šè¯„ä¼°äº†Xä¸ªå†…å®¹å—ï¼Œåˆ†åˆ«ä½¿ç”¨äº†...",
    "strengths": ["äº®ç‚¹1", "äº®ç‚¹2", "äº®ç‚¹3"],
    "improvements": [
        {{"issue": "é—®é¢˜", "priority": "high/medium/low", "suggested_action": "å»ºè®®"}}
    ],
    "action_items": ["æœ€ä¼˜å…ˆçš„è¡ŒåŠ¨å»ºè®®1", "è¡ŒåŠ¨å»ºè®®2"],
    "summary": "ç»¼åˆè¯Šæ–­æ€»ç»“ï¼ˆ100-200å­—ï¼Œä¸å«åˆ†æ•°ï¼‰"
}}"""

    try:
        text, call = await _call_llm(system_prompt, user_message, step="diagnoser", temperature=0.5)
        result = _parse_json_response(text)
        return result, call
    except Exception as e:
        return {
            "summary": f"è¯Šæ–­å¤±è´¥: {str(e)}",
            "strengths": [], "improvements": [], "action_items": [], "error": str(e),
        }, None


# ============== ä¸»å…¥å£ ==============

async def run_eval(
    content: str,
    roles: List[str] = None,
    creator_profile: str = "",
    intent: str = "",
    personas: List[dict] = None,
    max_turns: int = 5,
    content_field_names: list = None,
) -> Tuple[List[TrialResult], dict]:
    """
    å…¼å®¹æ—§æŽ¥å£ï¼šè¿è¡Œå®Œæ•´è¯„ä¼°ï¼ˆæ‰€æœ‰è§’è‰²å¹¶è¡Œæ‰§è¡Œï¼‰
    
    æ–°ä»£ç åº”ä½¿ç”¨ run_task_trial() é€ä¸ª Task æ‰§è¡Œ
    """
    if roles is None:
        roles = ["coach", "editor", "expert", "consumer", "seller"]
    
    if personas is None:
        personas = [{"name": "å…¸åž‹ç”¨æˆ·", "background": "å¯¹è¯¥é¢†åŸŸæ„Ÿå…´è¶£çš„æ™®é€šè¯»è€…"}]
    
    tasks = []
    
    for role in roles:
        if role in ("coach", "editor", "expert"):
            tasks.append(run_task_trial(
                simulator_type=role, interaction_mode="review",
                content=content, creator_profile=creator_profile, intent=intent,
                grader_config={"type": "content", "dimensions": []},
            ))
        elif role == "consumer":
            for persona in personas[:2]:
                tasks.append(run_task_trial(
                    simulator_type=role, interaction_mode="dialogue",
                    content=content, creator_profile=creator_profile, intent=intent,
                    persona=persona,
                    simulator_config={"max_turns": max_turns},
                    grader_config={"type": "combined", "dimensions": []},
                    content_field_names=content_field_names,
                ))
        elif role == "seller":
            for persona in personas[:2]:
                tasks.append(run_task_trial(
                    simulator_type=role, interaction_mode="dialogue",
                    content=content, creator_profile=creator_profile, intent=intent,
                    persona=persona,
                    simulator_config={"max_turns": max_turns},
                    grader_config={"type": "combined", "dimensions": []},
                    content_field_names=content_field_names,
                ))
    
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    trial_results = []
    for result in results:
        if isinstance(result, Exception):
            trial_results.append(TrialResult(
                role="unknown", interaction_mode="unknown",
                success=False, error=str(result),
            ))
        else:
            trial_results.append(result)
    
    diagnosis, _ = await run_diagnoser(trial_results, content[:500] if content else "", intent)
    
    return trial_results, diagnosis


# ============== å·¥å…·å‡½æ•° ==============

def _parse_json_response(text: str) -> dict:
    """å®‰å…¨è§£æž AI è¿”å›žçš„ JSONï¼ˆå®¹é”™ï¼šå¤„ç† LLM è¾“å‡ºå¤šä½™çš„æ‹¬å·ã€å‰åŽç¼€æ–‡æœ¬ç­‰ï¼‰"""
    text = text.strip()
    
    # 1. ç›´æŽ¥è§£æž
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        pass
    
    # 2. raw_decodeï¼šå¤„ç† JSON åŽé¢æœ‰å¤šä½™å­—ç¬¦ï¼ˆå¦‚å¤šä½™çš„ } æˆ–è§£é‡Šæ–‡æœ¬ï¼‰
    try:
        decoder = json.JSONDecoder()
        obj, _ = decoder.raw_decode(text)
        if isinstance(obj, dict):
            return obj
    except (json.JSONDecodeError, ValueError):
        pass
    
    # 3. æå– Markdown ä»£ç å—ä¸­çš„ JSON
    import re
    json_match = re.search(r'```(?:json)?\s*\n?(.*?)\n?```', text, re.DOTALL)
    if json_match:
        inner = json_match.group(1).strip()
        try:
            return json.loads(inner)
        except json.JSONDecodeError:
            pass
        # ä»£ç å—å†…ä¹Ÿå¯èƒ½æœ‰å¤šä½™å­—ç¬¦
        try:
            obj, _ = json.JSONDecoder().raw_decode(inner)
            if isinstance(obj, dict):
                return obj
        except (json.JSONDecodeError, ValueError):
            pass
    
    # 4. æå–ç¬¬ä¸€ä¸ª { åˆ°æœ€åŽä¸€ä¸ª } ä¹‹é—´çš„å†…å®¹
    start = text.find('{')
    end = text.rfind('}')
    if start >= 0 and end > start:
        snippet = text[start:end+1]
        try:
            return json.loads(snippet)
        except json.JSONDecodeError:
            pass
        # snippet å†…éƒ¨å¯èƒ½ä¹Ÿæœ‰å¤šä½™å°¾éƒ¨
        try:
            obj, _ = json.JSONDecoder().raw_decode(snippet)
            if isinstance(obj, dict):
                return obj
        except (json.JSONDecodeError, ValueError):
            pass
    
    return {"raw_output": text, "parse_error": True}


def _get_sales_phase(turn: int) -> str:
    """èŽ·å–é”€å”®é˜¶æ®µåç§°"""
    if turn == 0:
        return "opening"
    elif turn <= 2:
        return "need_discovery"
    elif turn <= 4:
        return "value_matching"
    elif turn <= 6:
        return "objection_handling"
    else:
        return "closing"


def format_trial_result_markdown(trial: TrialResult) -> str:
    """å°† Trial ç»“æžœæ ¼å¼åŒ–ä¸º Markdown"""
    type_info = SIMULATOR_TYPES.get(trial.role, {"name": trial.role, "icon": "ðŸ“‹"})
    
    md = f"## {type_info.get('icon', 'ðŸ“‹')} {type_info.get('name', trial.role)}è¯„ä¼°\n\n"
    
    if not trial.success:
        md += f"âŒ è¯„ä¼°å¤±è´¥: {trial.error}\n"
        return md
    
    md += f"**ç»¼åˆè¯„åˆ†: {trial.overall_score}/10** | æ¨¡å¼: {trial.interaction_mode}\n\n"
    
    scores = trial.result.get("scores", {})
    if scores:
        md += "### å„ç»´åº¦è¯„åˆ†\n"
        for dim, score in scores.items():
            if isinstance(score, (int, float)):
                bar = "â–ˆ" * int(score) + "â–‘" * (10 - int(score))
                md += f"- {dim}: **{score}/10** {bar}\n"
                comment = trial.result.get("comments", {}).get(dim, "")
                if comment:
                    md += f"  - {comment}\n"
        md += "\n"
    
    if trial.interaction_mode == "dialogue" and trial.nodes:
        md += "### å¯¹è¯è®°å½•\n"
        for node in trial.nodes:
            role_label = {"consumer": "ðŸ—£ æ¶ˆè´¹è€…", "seller": "ðŸ’¼ é”€å”®", "content_rep": "ðŸ“„ å†…å®¹"}.get(node.get("role"), node.get("role", ""))
            md += f"**{role_label}** (ç¬¬{node.get('turn', '?')}è½®): {node.get('content', '')}\n\n"
    
    for label, key, icon in [("ä¼˜ç‚¹", "strengths", "âœ…"), ("é—®é¢˜", "weaknesses", "âš ï¸"), ("æ”¹è¿›å»ºè®®", "suggestions", "ðŸ’¡")]:
        items = trial.result.get(key, [])
        if items:
            md += f"### {icon} {label}\n"
            for s in items:
                md += f"- {s}\n"
            md += "\n"
    
    summary = trial.result.get("summary", "")
    if summary:
        md += f"### æ€»ç»“\n{summary}\n\n"
    
    outcome = trial.result.get("outcome", "")
    if outcome:
        outcome_map = {
            "converted": "âœ… è½¬åŒ–æˆåŠŸ", "not_converted": "âŒ æœªè½¬åŒ–",
            "recommended": "ðŸ‘ æŽ¨è", "not_recommended": "ðŸ‘Ž ä¸æŽ¨è", "reviewed": "ðŸ“ å·²å®¡æŸ¥",
        }
        md += f"**ç»“æžœ: {outcome_map.get(outcome, outcome)}**\n\n"
    
    # LLM è°ƒç”¨ç»Ÿè®¡
    if trial.llm_calls:
        md += f"---\nðŸ“Š å…± {len(trial.llm_calls)} æ¬¡ LLM è°ƒç”¨ | "
        md += f"Tokens: {trial.tokens_in}â†‘ {trial.tokens_out}â†“ | "
        md += f"è´¹ç”¨: Â¥{trial.cost:.4f}\n"
    
    return md


def format_diagnosis_markdown(diagnosis: dict) -> str:
    """å°†è¯Šæ–­ç»“æžœæ ¼å¼åŒ–ä¸º Markdownï¼ˆå®šæ€§åˆ†æžï¼Œæ— åˆ†æ•°ï¼‰"""
    md = "## ðŸ” ç»¼åˆè¯Šæ–­\n\n"
    
    overview = diagnosis.get("overview", "")
    if overview:
        md += f"{overview}\n\n"
    
    strengths = diagnosis.get("strengths", [])
    if strengths:
        md += "### âœ… äº®ç‚¹\n"
        for s in strengths:
            md += f"- {s}\n"
        md += "\n"
    
    improvements = diagnosis.get("improvements", [])
    if improvements:
        md += "### âš ï¸ å¾…æå‡\n"
        for imp in improvements:
            if isinstance(imp, dict):
                severity_icon = {"high": "ðŸ”´", "medium": "ðŸŸ¡", "low": "ðŸŸ¢"}.get(imp.get("priority", ""), "âšª")
                md += f"- {severity_icon} **{imp.get('issue', '')}**\n"
                if imp.get("suggested_action"):
                    md += f"  â†’ {imp['suggested_action']}\n"
            else:
                md += f"- {imp}\n"
        md += "\n"
    
    action_items = diagnosis.get("action_items", [])
    if action_items:
        md += "### ðŸŽ¯ ä¼˜å…ˆè¡ŒåŠ¨\n"
        for i, a in enumerate(action_items, 1):
            md += f"{i}. {a}\n"
        md += "\n"
    
    summary = diagnosis.get("summary", "")
    if summary:
        md += f"### æ€»ç»“\n{summary}\n"
    
    return md
