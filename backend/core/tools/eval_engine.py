# backend/core/tools/eval_engine.py
# åŠŸèƒ½: Eval V2 è¯„ä¼°æ‰§è¡Œå¼•æ“
# ä¸»è¦å‡½æ•°:
#   - run_task_trial(): æ‰§è¡Œå•ä¸ª EvalTask çš„ä¸€æ¬¡ Trialï¼ˆæ ¸å¿ƒï¼‰
#   - run_grader(): å¯¹ Trial ç»“æœè¿›è¡Œè¯„åˆ†ï¼ˆå†…å®¹/è¿‡ç¨‹/ç»¼åˆï¼‰
#   - run_diagnoser(): è·¨ Trial è¯Šæ–­
#   - run_eval_run(): æ‰§è¡Œæ•´ä¸ª EvalRunï¼ˆå¹¶è¡Œæ‰§è¡Œæ‰€æœ‰ Taskï¼‰
#   - format_*(): æ ¼å¼åŒ–è¾“å‡º
# æ•°æ®ç»“æ„:
#   - LLMCall: ä¸€æ¬¡ LLM è°ƒç”¨çš„å®Œæ•´è®°å½•ï¼ˆè¾“å…¥/è¾“å‡º/token/è€—æ—¶ï¼‰
#   - TrialResult: Trial æ‰§è¡Œç»“æœï¼ˆå« llm_calls åˆ—è¡¨ï¼‰

"""
Eval V2 å¼•æ“
Task-based è¯„ä¼°ä½“ç³»ï¼š
  EvalRun â†’ EvalTask[] â†’ EvalTrial[]
  æ¯ä¸ª Trial è®°å½•å®Œæ•´çš„ LLM è°ƒç”¨æ—¥å¿—
  Grader åˆ†ç¦»ï¼šå†…å®¹è¯„åˆ† + è¿‡ç¨‹è¯„åˆ†
"""

import json
import time
import asyncio
from typing import Optional, Dict, List, Tuple
from dataclasses import dataclass, field
from datetime import datetime

from core.ai_client import ai_client, ChatMessage
from core.models.eval_task import SIMULATOR_TYPES


# ============== æ•°æ®ç»“æ„ ==============

@dataclass
class LLMCall:
    """ä¸€æ¬¡ LLM è°ƒç”¨çš„å®Œæ•´è®°å½•"""
    step: str           # "simulator_review" / "consumer_turn_1" / "content_rep_turn_1" / "grader_content" / "grader_process" / "diagnoser"
    input_system: str   # ç³»ç»Ÿæç¤ºè¯
    input_user: str     # ç”¨æˆ·æ¶ˆæ¯
    output: str         # AI å“åº”
    tokens_in: int = 0
    tokens_out: int = 0
    cost: float = 0.0
    duration_ms: int = 0
    timestamp: str = ""

    def to_dict(self) -> dict:
        return {
            "step": self.step,
            "input": {"system_prompt": self.input_system, "user_message": self.input_user},
            "output": self.output,
            "tokens_in": self.tokens_in,
            "tokens_out": self.tokens_out,
            "cost": self.cost,
            "duration_ms": self.duration_ms,
            "timestamp": self.timestamp or datetime.now().isoformat(),
        }


@dataclass
class TrialResult:
    """Trial æ‰§è¡Œç»“æœ"""
    role: str
    interaction_mode: str
    role_display_name: str = ""  # ç”¨æˆ·é…ç½®çš„æ¨¡æ‹Ÿå™¨æ˜¾ç¤ºåç§°
    nodes: list = field(default_factory=list)          # äº¤äº’èŠ‚ç‚¹
    result: dict = field(default_factory=dict)          # è¯„åˆ†ç»“æœ
    grader_outputs: list = field(default_factory=list)  # Grader è¾“å‡º
    llm_calls: list = field(default_factory=list)       # å®Œæ•´ LLM è°ƒç”¨æ—¥å¿—
    overall_score: float = 0.0
    success: bool = True
    error: str = ""
    tokens_in: int = 0
    tokens_out: int = 0
    cost: float = 0.0


# ============== LLM è°ƒç”¨å°è£…ï¼ˆå¸¦æ—¥å¿—ï¼‰ ==============

async def _call_llm(
    system_prompt: str,
    user_message: str,
    step: str,
    temperature: float = 0.6,
) -> Tuple[str, LLMCall]:
    """
    å°è£… LLM è°ƒç”¨ï¼Œè¿”å› (å“åº”æ–‡æœ¬, LLMCall æ—¥å¿—)
    æ‰€æœ‰ eval ç›¸å…³çš„ LLM è°ƒç”¨éƒ½èµ°è¿™ä¸ªå‡½æ•°ï¼Œç¡®ä¿æ¯æ¬¡è°ƒç”¨éƒ½è¢«è®°å½•
    """
    messages = [
        ChatMessage(role="system", content=system_prompt),
        ChatMessage(role="user", content=user_message),
    ]
    
    start_time = time.time()
    response = await ai_client.async_chat(messages, temperature=temperature)
    duration_ms = int((time.time() - start_time) * 1000)
    
    call = LLMCall(
        step=step,
        input_system=system_prompt,
        input_user=user_message,
        output=response.content,
        tokens_in=response.tokens_in,
        tokens_out=response.tokens_out,
        cost=response.cost,
        duration_ms=duration_ms,
        timestamp=datetime.now().isoformat(),
    )
    
    return response.content, call


async def _call_llm_multi(
    messages: List[ChatMessage],
    step: str,
    temperature: float = 0.6,
) -> Tuple[str, LLMCall]:
    """å¤šæ¶ˆæ¯ç‰ˆæœ¬çš„ LLM è°ƒç”¨ï¼ˆç”¨äºå¤šè½®å¯¹è¯ï¼‰"""
    start_time = time.time()
    response = await ai_client.async_chat(messages, temperature=temperature)
    duration_ms = int((time.time() - start_time) * 1000)
    
    # æå–ç³»ç»Ÿå’Œç”¨æˆ·æ¶ˆæ¯ç”¨äºæ—¥å¿—
    system_prompt = ""
    user_messages = []
    for m in messages:
        if m.role == "system":
            system_prompt = m.content
        elif m.role == "user":
            user_messages.append(m.content)
    
    call = LLMCall(
        step=step,
        input_system=system_prompt,
        input_user="\n---\n".join(user_messages[-2:]) if user_messages else "",  # æœ€è¿‘2æ¡
        output=response.content,
        tokens_in=response.tokens_in,
        tokens_out=response.tokens_out,
        cost=response.cost,
        duration_ms=duration_ms,
        timestamp=datetime.now().isoformat(),
    )
    
    return response.content, call


# ============== æ ¸å¿ƒæ‰§è¡Œå‡½æ•° ==============

async def run_task_trial(
    simulator_type: str,
    interaction_mode: str,
    content: str,
    creator_profile: str = "",
    intent: str = "",
    persona: dict = None,
    simulator_config: dict = None,
    grader_config: dict = None,
    content_field_names: list = None,
) -> TrialResult:
    """
    æ‰§è¡Œå•ä¸ª EvalTask çš„ä¸€æ¬¡ Trial
    
    è¿™æ˜¯ eval å¼•æ“çš„æ ¸å¿ƒå‡½æ•°ã€‚æ ¹æ® simulator_type å’Œ interaction_mode 
    é€‰æ‹©æ‰§è¡Œç­–ç•¥ï¼Œæ‰€æœ‰ LLM è°ƒç”¨éƒ½è¢«å®Œæ•´è®°å½•ã€‚
    
    Args:
        simulator_type: æ¨¡æ‹Ÿå™¨è§’è‰² (coach/editor/expert/consumer/seller/custom)
        interaction_mode: äº¤äº’æ¨¡å¼ (review/dialogue/scenario)
        content: è¦è¯„ä¼°çš„å†…å®¹
        creator_profile: åˆ›ä½œè€…ç‰¹è´¨
        intent: é¡¹ç›®æ„å›¾
        persona: æ¶ˆè´¹è€…ç”»åƒ
        simulator_config: æ¨¡æ‹Ÿå™¨è‡ªå®šä¹‰é…ç½®
        grader_config: è¯„åˆ†å™¨é…ç½®
        content_field_names: å†…å®¹å­—æ®µååˆ—è¡¨
    
    Returns:
        TrialResult
    """
    config = simulator_config or {}
    grader_cfg = grader_config or {}
    
    # è·å–æ¨¡æ‹Ÿå™¨æ˜¾ç¤ºåç§°ï¼ˆä¼˜å…ˆç”¨åå°é…ç½®çš„åç§°ï¼Œå…¶æ¬¡ç”¨ç¡¬ç¼–ç åç§°ï¼‰
    display_name = config.get("simulator_name", "") or SIMULATOR_TYPES.get(simulator_type, {}).get("name", simulator_type)
    
    if interaction_mode == "review":
        result = await _run_review(
            simulator_type, content, creator_profile, intent, persona, config, grader_cfg
        )
    elif interaction_mode == "dialogue":
        result = await _run_dialogue(
            simulator_type, content, creator_profile, intent, persona, config, grader_cfg, content_field_names
        )
    elif interaction_mode == "scenario":
        result = await _run_dialogue(
            simulator_type, content, creator_profile, intent, persona, config, grader_cfg, content_field_names
        )
    else:
        return TrialResult(
            role=simulator_type, interaction_mode=interaction_mode,
            success=False, error=f"ä¸æ”¯æŒçš„äº¤äº’æ¨¡å¼: {interaction_mode}"
        )
    
    result.role_display_name = display_name
    return result


async def _run_review(
    simulator_type: str,
    content: str,
    creator_profile: str,
    intent: str,
    persona: dict,
    config: dict,
    grader_cfg: dict,
) -> TrialResult:
    """å®¡æŸ¥æ¨¡å¼ï¼šAI ä¸€æ¬¡æ€§é˜…è¯»å…¨éƒ¨å†…å®¹ï¼Œç»™å‡ºç»“æ„åŒ–åé¦ˆ"""
    llm_calls = []
    
    # è·å–ç³»ç»Ÿæç¤ºè¯ï¼ˆä¼˜å…ˆåå°é…ç½® > ç¡¬ç¼–ç  SIMULATOR_TYPESï¼‰
    type_info = SIMULATOR_TYPES.get(simulator_type, {})
    custom_prompt = config.get("system_prompt", "")
    if custom_prompt:
        # æ›¿æ¢å ä½ç¬¦
        persona_text_for_sub = json.dumps(persona, ensure_ascii=False) if persona else ""
        base_prompt = custom_prompt.replace("{content}", content).replace("{persona}", persona_text_for_sub)
    else:
        base_prompt = type_info.get("system_prompt", "è¯·è¯„ä¼°ä»¥ä¸‹å†…å®¹ã€‚")
    
    # æ³¨å…¥ä¸Šä¸‹æ–‡
    system_prompt = base_prompt
    if creator_profile:
        system_prompt += f"\n\nã€åˆ›ä½œè€…ç‰¹è´¨ã€‘\n{creator_profile}"
    if intent:
        system_prompt += f"\n\nã€é¡¹ç›®æ„å›¾ã€‘\n{intent}"
    if persona:
        persona_text = json.dumps(persona, ensure_ascii=False)
        system_prompt += f"\n\nã€ç›®æ ‡æ¶ˆè´¹è€…ã€‘\n{persona_text}"
    
    # è·å–è¯„åˆ†ç»´åº¦
    dimensions = grader_cfg.get("dimensions", []) or type_info.get("default_dimensions", ["ç»¼åˆè¯„ä»·"])
    dim_str = ", ".join([f'"{d}": åˆ†æ•°(1-10)' for d in dimensions])
    dim_comment_str = ", ".join([f'"{d}": "å…·ä½“è¯„è¯­ï¼ˆè‡³å°‘2å¥è¯ï¼‰"' for d in dimensions])
    
    user_message = f"""ä»¥ä¸‹æ˜¯è¦è¯„ä¼°çš„å†…å®¹ï¼š

{content}

è¯·ä»¥ä½ çš„ä¸“ä¸šèº«ä»½è¿›è¡Œè¯„ä¼°ã€‚

**è¾“å‡ºJSONæ ¼å¼**ï¼ˆä¸¥æ ¼éµå¾ªï¼Œä¸è¦è¾“å‡ºå…¶ä»–å†…å®¹ï¼‰ï¼š
{{
    "scores": {{{dim_str}}},
    "comments": {{{dim_comment_str}}},
    "strengths": ["ä¼˜ç‚¹1", "ä¼˜ç‚¹2", "ä¼˜ç‚¹3"],
    "weaknesses": ["é—®é¢˜1", "é—®é¢˜2", "é—®é¢˜3"],
    "suggestions": ["å…·ä½“æ”¹è¿›å»ºè®®1", "å…·ä½“æ”¹è¿›å»ºè®®2", "å…·ä½“æ”¹è¿›å»ºè®®3"],
    "summary": "æ€»ä½“è¯„ä»·ï¼ˆ100-200å­—ï¼‰"
}}"""

    try:
        response_text, call = await _call_llm(
            system_prompt, user_message,
            step=f"simulator_{simulator_type}_review",
            temperature=0.6,
        )
        llm_calls.append(call)
        
        result_data = _parse_json_response(response_text)
        scores = result_data.get("scores", {})
        avg_score = sum(v for v in scores.values() if isinstance(v, (int, float))) / len(scores) if scores else 0
        
        # è¿è¡Œ Grader
        grader_outputs = []
        grader_result, grader_call = await _run_content_grader(
            content, result_data, dimensions, grader_cfg
        )
        if grader_call:
            llm_calls.append(grader_call)
            grader_outputs.append(grader_result)
        
        total_tokens_in = sum(c.tokens_in for c in llm_calls)
        total_tokens_out = sum(c.tokens_out for c in llm_calls)
        total_cost = sum(c.cost for c in llm_calls)
        
        return TrialResult(
            role=simulator_type,
            interaction_mode="review",
            nodes=[
                {"role": "system", "content": system_prompt[:500] + "..."},
                {"role": "user", "content": user_message[:500] + "..."},
                {"role": "assistant", "content": response_text},
            ],
            result={
                "scores": scores,
                "comments": result_data.get("comments", {}),
                "strengths": result_data.get("strengths", []),
                "weaknesses": result_data.get("weaknesses", []),
                "suggestions": result_data.get("suggestions", []),
                "outcome": "reviewed",
                "summary": result_data.get("summary", ""),
            },
            grader_outputs=grader_outputs,
            llm_calls=[c.to_dict() for c in llm_calls],
            overall_score=round(avg_score, 2),
            success=True,
            tokens_in=total_tokens_in,
            tokens_out=total_tokens_out,
            cost=total_cost,
        )
    except Exception as e:
        return TrialResult(
            role=simulator_type, interaction_mode="review",
            llm_calls=[c.to_dict() for c in llm_calls],
            success=False, error=str(e),
        )


async def _run_dialogue(
    simulator_type: str,
    content: str,
    creator_profile: str,
    intent: str,
    persona: dict,
    config: dict,
    grader_cfg: dict,
    content_field_names: list = None,
) -> TrialResult:
    """å¯¹è¯æ¨¡å¼ï¼šå¤šè½®äº¤äº’ï¼Œæ¨¡æ‹ŸçœŸå®ç”¨æˆ·ä¸å†…å®¹çš„äº’åŠ¨"""
    llm_calls = []
    interaction_log = []
    max_turns = config.get("max_turns", 5)
    
    persona = persona or {"name": "å…¸å‹ç”¨æˆ·", "background": "å¯¹è¯¥é¢†åŸŸæ„Ÿå…´è¶£çš„è¯»è€…"}
    user_name = persona.get("name", "æ¶ˆè´¹è€…")
    persona_text = json.dumps(persona, ensure_ascii=False, indent=2)
    
    content_name = "å†…å®¹"
    if content_field_names:
        content_name = f"ã€Š{content_field_names[0]}ã€‹" if len(content_field_names) == 1 else f"ã€Š{content_field_names[0]}ã€‹ç­‰{len(content_field_names)}ç¯‡"
    
    # æ ¹æ® simulator_type ç¡®å®šå¯¹è¯æ–¹å‘
    if simulator_type == "seller":
        # é”€å”®æ¨¡å¼ï¼šé”€å”®æ–¹ä¸»åŠ¨ï¼Œæ¶ˆè´¹è€…å›åº”
        return await _run_seller_dialogue(
            content, persona, config, grader_cfg, llm_calls, content_field_names
        )
    
    # ç”¨æˆ·è‡ªå®šä¹‰çš„ simulator æç¤ºè¯ï¼ˆæ¥è‡ªåå°é…ç½®ï¼‰
    custom_sim_prompt = config.get("system_prompt", "")
    sim_name = config.get("simulator_name", simulator_type)
    
    # é€šç”¨å¯¹è¯æ¨¡å¼ï¼šæ¶ˆè´¹è€…æé—®ï¼Œå†…å®¹ä»£è¡¨å›ç­”
    if custom_sim_prompt:
        # ç”¨åå°é…ç½®çš„æç¤ºè¯ï¼Œæ›¿æ¢å ä½ç¬¦
        consumer_system = custom_sim_prompt.replace("{persona}", persona_text).replace("{content}", content)
        if persona_text not in consumer_system and "{persona}" not in custom_sim_prompt:
            consumer_system += f"\n\nã€ä½ æ‰®æ¼”çš„æ¶ˆè´¹è€…è§’è‰²ã€‘\n{persona_text}"
    else:
        consumer_system = f"""ä½ æ­£åœ¨æ‰®æ¼”ä¸€ä½çœŸå®ç”¨æˆ·è¿›è¡Œæ¨¡æ‹Ÿå¯¹è¯ã€‚

ã€ä½ çš„è§’è‰²ã€‘
{persona_text}

ã€ä½ çš„ç›®æ ‡ã€‘
ä½ æœ‰ä¸€äº›å›°æƒ‘å’Œé—®é¢˜æƒ³è¦è§£å†³ã€‚ä½ æ­£åœ¨é€šè¿‡é˜…è¯»/å’¨è¯¢{content_name}æ¥å¯»æ‰¾ç­”æ¡ˆã€‚

ã€è¡Œä¸ºè¦æ±‚ã€‘
1. æ¯æ¬¡åªé—®ä¸€ä¸ªé—®é¢˜ï¼Œè¡¨è¾¾ç®€çŸ­è‡ªç„¶
2. é—®é¢˜è¦åŸºäºä½ çš„çœŸå®èƒŒæ™¯å’Œç—›ç‚¹
3. å¦‚æœå¯¹æ–¹çš„å›ç­”è®©ä½ æ»¡æ„ï¼Œå¯ä»¥è¡¨ç¤ºæ„Ÿè°¢
4. å¦‚æœå¯¹æ–¹çš„å›ç­”ä¸å¤Ÿå¥½ï¼Œç»§ç»­è¿½é—®
5. å¦‚æœè§‰å¾—å·²ç»äº†è§£è¶³å¤Ÿäº†ï¼Œè¯´"å¥½çš„ï¼Œæˆ‘äº†è§£äº†"ç»“æŸå¯¹è¯"""

    # å†…å®¹ä»£è¡¨/ç¬¬äºŒæ–¹çš„æç¤ºè¯ï¼šä¼˜å…ˆç”¨åå°é…ç½®çš„ secondary_prompt
    custom_secondary = config.get("secondary_prompt", "")
    if custom_secondary:
        # ç”¨åå°é…ç½®çš„æç¤ºè¯ï¼Œæ›¿æ¢ {content} å ä½ç¬¦
        content_system = custom_secondary.replace("{content}", content).replace("{persona}", persona_text)
    else:
        content_system = f"""ä½ æ˜¯{content_name}çš„å†…å®¹ä»£è¡¨ï¼Œä¸¥æ ¼åŸºäºä»¥ä¸‹å†…å®¹å›ç­”é—®é¢˜ã€‚

=== å†…å®¹å¼€å§‹ ===
{content}
=== å†…å®¹ç»“æŸ ===

ã€å›ç­”è§„åˆ™ã€‘
1. ä¸¥æ ¼åŸºäºå†…å®¹å›ç­”ï¼Œä¸è¦ç¼–é€ 
2. å¦‚æœå†…å®¹ä¸­æ²¡æœ‰æ¶‰åŠï¼Œè¯šå®è¯´æ˜
3. å°½é‡å¼•ç”¨å†…å®¹ä¸­çš„åŸè¯æˆ–æ ¸å¿ƒè§‚ç‚¹"""

    try:
        for turn in range(max_turns):
            # æ¶ˆè´¹è€…æé—®
            user_messages = [ChatMessage(role="system", content=consumer_system)]
            for log in interaction_log:
                if log["role"] == "consumer":
                    user_messages.append(ChatMessage(role="assistant", content=log["content"]))
                else:
                    user_messages.append(ChatMessage(role="user", content=log["content"]))
            
            prompt = "è¯·åŸºäºä½ çš„èƒŒæ™¯ï¼Œæå‡ºä½ æœ€æƒ³è§£å†³çš„ç¬¬ä¸€ä¸ªé—®é¢˜ã€‚" if turn == 0 else "è¯·åŸºäºä¹‹å‰çš„å¯¹è¯ï¼Œç»§ç»­ä½ çš„å’¨è¯¢ã€‚"
            user_messages.append(ChatMessage(role="user", content=prompt))
            
            user_response_text, user_call = await _call_llm_multi(
                user_messages, step=f"consumer_turn_{turn+1}", temperature=0.8
            )
            llm_calls.append(user_call)
            
            interaction_log.append({
                "role": "consumer", "name": user_name,
                "content": user_response_text, "turn": turn + 1,
            })
            
            # æ£€æŸ¥æ˜¯å¦ç»“æŸ
            end_signals = ["äº†è§£äº†", "æ˜ç™½äº†", "å¥½çš„è°¢è°¢", "è°¢è°¢", "å†è§", "ä¸éœ€è¦äº†", "è¶³å¤Ÿäº†", "æ¸…æ¥šäº†"]
            if any(s in user_response_text for s in end_signals):
                break
            
            # å†…å®¹ä»£è¡¨å›å¤
            content_messages = [ChatMessage(role="system", content=content_system)]
            for log in interaction_log:
                if log["role"] == "consumer":
                    content_messages.append(ChatMessage(role="user", content=log["content"]))
                else:
                    content_messages.append(ChatMessage(role="assistant", content=log["content"]))
            
            content_response_text, content_call = await _call_llm_multi(
                content_messages, step=f"content_rep_turn_{turn+1}", temperature=0.5
            )
            llm_calls.append(content_call)
            
            interaction_log.append({
                "role": "content_rep", "name": content_name,
                "content": content_response_text, "turn": turn + 1,
            })
        
        # è¯„ä¼°é˜¶æ®µ - å†…å®¹è¯„åˆ† (Grader)
        dialogue_transcript = "\n".join([
            f"[{log.get('name', log['role'])}]: {log['content']}"
            for log in interaction_log
        ])
        
        type_info = SIMULATOR_TYPES.get(simulator_type, {})
        dimensions = grader_cfg.get("dimensions", []) or type_info.get("default_dimensions", ["ç»¼åˆè¯„ä»·"])
        dim_str = ", ".join([f'"{d}": åˆ†æ•°(1-10)' for d in dimensions])
        
        eval_system = f"""ä½ æ˜¯{user_name}ï¼Œåˆšåˆšå®Œæˆäº†ä¸€æ¬¡å’¨è¯¢å¯¹è¯ã€‚
ä½ çš„èƒŒæ™¯ï¼š{persona_text}
è¯·è¯„ä¼°å†…å®¹å¯¹ä½ çš„å¸®åŠ©ç¨‹åº¦ã€‚"""
        
        eval_user = f"""å¯¹è¯è®°å½•ï¼š
{dialogue_transcript}

è¯·ä»¥JSONæ ¼å¼è¾“å‡ºï¼š
{{
    "scores": {{{dim_str}}},
    "comments": {{{", ".join([f'"{d}": "è¯„è¯­"' for d in dimensions])}}},
    "problems_solved": ["è¢«è§£å†³çš„é—®é¢˜"],
    "problems_unsolved": ["æœªè¢«è§£å†³çš„é—®é¢˜"],
    "content_gaps": ["å†…å®¹ç¼ºå¤±çš„éƒ¨åˆ†"],
    "would_recommend": true,
    "summary": "æ€»ä½“è¯„ä»·ï¼ˆ100å­—ä»¥å†…ï¼‰"
}}"""
        
        eval_text, eval_call = await _call_llm(
            eval_system, eval_user,
            step=f"grader_content_{simulator_type}",
            temperature=0.5,
        )
        llm_calls.append(eval_call)
        
        result_data = _parse_json_response(eval_text)
        scores = result_data.get("scores", {})
        avg_score = sum(v for v in scores.values() if isinstance(v, (int, float))) / len(scores) if scores else 0
        
        # è¿‡ç¨‹è¯„åˆ† (Process Grader)
        grader_outputs = []
        process_grader, process_call = await _run_process_grader(
            dialogue_transcript, dimensions, grader_cfg
        )
        if process_call:
            llm_calls.append(process_call)
            grader_outputs.append(process_grader)
        
        total_tokens_in = sum(c.tokens_in for c in llm_calls)
        total_tokens_out = sum(c.tokens_out for c in llm_calls)
        total_cost = sum(c.cost for c in llm_calls)
        
        return TrialResult(
            role=simulator_type,
            interaction_mode="dialogue",
            nodes=[{"role": log["role"], "content": log["content"], "turn": log.get("turn")} for log in interaction_log],
            result={
                "scores": scores,
                "comments": result_data.get("comments", {}),
                "strengths": result_data.get("problems_solved", []),
                "weaknesses": result_data.get("problems_unsolved", []),
                "suggestions": result_data.get("content_gaps", []),
                "outcome": "recommended" if result_data.get("would_recommend") else "not_recommended",
                "summary": result_data.get("summary", ""),
            },
            grader_outputs=grader_outputs,
            llm_calls=[c.to_dict() for c in llm_calls],
            overall_score=round(avg_score, 2),
            success=True,
            tokens_in=total_tokens_in,
            tokens_out=total_tokens_out,
            cost=total_cost,
        )
    except Exception as e:
        return TrialResult(
            role=simulator_type, interaction_mode="dialogue",
            nodes=[{"role": log["role"], "content": log["content"]} for log in interaction_log],
            llm_calls=[c.to_dict() for c in llm_calls],
            success=False, error=str(e),
        )


async def _run_seller_dialogue(
    content: str,
    persona: dict,
    config: dict,
    grader_cfg: dict,
    llm_calls: list,
    content_field_names: list = None,
) -> TrialResult:
    """é”€å”®å¯¹è¯ï¼šé”€å”®é¡¾é—®ä¸»åŠ¨æ¨ä»‹ï¼Œæ¶ˆè´¹è€…å›åº”"""
    max_turns = config.get("max_turns", 8)
    consumer_name = persona.get("name", "æ¶ˆè´¹è€…")
    persona_text = json.dumps(persona, ensure_ascii=False, indent=2)
    interaction_log = []
    
    # é”€å”®æ–¹æç¤ºè¯ï¼šä¼˜å…ˆç”¨åå°é…ç½®
    custom_primary = config.get("system_prompt", "")
    if custom_primary:
        seller_system = custom_primary.replace("{content}", content).replace("{persona}", persona_text)
    else:
        seller_system = f"""ä½ æ˜¯è¿™ä¸ªå†…å®¹çš„é”€å”®é¡¾é—®ã€‚ä½ æ·±å…¥äº†è§£å†…å®¹çš„æ¯ä¸ªç»†èŠ‚ã€‚

=== ä½ æŒæ¡çš„å†…å®¹ ===
{content}
=== å†…å®¹ç»“æŸ ===

ã€ä½ çš„ç›®æ ‡æ¶ˆè´¹è€…ã€‘
{persona_text}

ã€é”€å”®ç­–ç•¥ã€‘
Phase 1 (ç¬¬1è½®): æœ‰å¸å¼•åŠ›çš„å¼€åœºç™½ï¼ŒåŒæ—¶æå‡ºä¸€ä¸ªäº†è§£éœ€æ±‚çš„é—®é¢˜
Phase 2 (ç¬¬2-3è½®): æ·±å…¥äº†è§£æ¶ˆè´¹è€…çš„å…·ä½“éœ€æ±‚å’Œç—›ç‚¹
Phase 3 (ç¬¬4-5è½®): åŒ¹é…å†…å®¹ä¸­çš„ä»·å€¼ç‚¹åˆ°æ¶ˆè´¹è€…éœ€æ±‚
Phase 4 (ç¬¬6-7è½®): å¤„ç†å¼‚è®®
Phase 5 (æœ€å): æ€»ç»“ä»·å€¼ï¼Œè¯¢é—®å†³å®š

ã€è¡Œä¸ºè¦æ±‚ã€‘- ä¸»åŠ¨å¼•å¯¼å¯¹è¯ - å¼•ç”¨å…·ä½“ä¿¡æ¯ - è¯šå®ä½†æœ‰è¯´æœåŠ› - æ¯æ¬¡200å­—ä»¥å†…"""

    # æ¶ˆè´¹è€…æ–¹æç¤ºè¯ï¼šä¼˜å…ˆç”¨åå°é…ç½®çš„ secondary_prompt
    custom_secondary = config.get("secondary_prompt", "")
    if custom_secondary:
        consumer_system = custom_secondary.replace("{persona}", persona_text).replace("{content}", content)
    else:
        consumer_system = f"""ä½ æ˜¯ä¸€ä½çœŸå®çš„æ½œåœ¨ç”¨æˆ·ã€‚æœ‰äººæ­£åœ¨å‘ä½ æ¨ä»‹å†…å®¹/äº§å“ã€‚

ã€ä½ çš„èº«ä»½ã€‘
{persona_text}

ã€ä½ çš„æ€åº¦ã€‘- æœ‰çœŸå®éœ€æ±‚ï¼Œä½†ä¸è½»æ˜“è¢«è¯´æœ - ä¼šæå‡ºçœŸå®è´¨ç–‘ - å¦‚æœç¡®å®æœ‰ä»·å€¼ï¼Œæ„¿æ„æ¥å— - ä¸é€‚åˆå°±æ˜ç¡®æ‹’ç»

ã€è¡Œä¸ºè¦æ±‚ã€‘åŸºäºçœŸå®èƒŒæ™¯å›åº”ï¼Œé€‚å½“è´¨ç–‘ï¼Œæœ€ååšå‡ºæ˜ç¡®å†³å®šã€‚"""

    try:
        for turn in range(max_turns):
            # é”€å”®å‘è¨€
            seller_messages = [ChatMessage(role="system", content=seller_system)]
            for log in interaction_log:
                if log["role"] == "seller":
                    seller_messages.append(ChatMessage(role="assistant", content=log["content"]))
                else:
                    seller_messages.append(ChatMessage(role="user", content=log["content"]))
            seller_messages.append(ChatMessage(role="user", content="è¯·å¼€å§‹ä½ çš„é”€å”®å¼€åœºç™½ã€‚" if turn == 0 else "è¯·ç»§ç»­ã€‚"))
            
            seller_text, seller_call = await _call_llm_multi(
                seller_messages, step=f"seller_turn_{turn+1}", temperature=0.7
            )
            llm_calls.append(seller_call)
            interaction_log.append({"role": "seller", "name": "é”€å”®é¡¾é—®", "content": seller_text, "turn": turn + 1, "phase": _get_sales_phase(turn)})
            
            # æ¶ˆè´¹è€…å›åº”
            consumer_messages = [ChatMessage(role="system", content=consumer_system)]
            for log in interaction_log:
                if log["role"] == "consumer":
                    consumer_messages.append(ChatMessage(role="assistant", content=log["content"]))
                else:
                    consumer_messages.append(ChatMessage(role="user", content=log["content"]))
            
            consumer_text, consumer_call = await _call_llm_multi(
                consumer_messages, step=f"consumer_turn_{turn+1}", temperature=0.8
            )
            llm_calls.append(consumer_call)
            interaction_log.append({"role": "consumer", "name": consumer_name, "content": consumer_text, "turn": turn + 1})
            
            # æ£€æŸ¥å†³å®š
            decision_signals = ["æˆ‘å†³å®š", "æˆ‘æ¥å—", "æˆ‘ä¸éœ€è¦", "æˆ‘æ‹’ç»", "å¯ä»¥", "å¥½çš„"]
            if turn >= 3 and any(s in consumer_text for s in decision_signals):
                break
        
        # è¯„ä¼°
        dialogue_transcript = "\n".join([f"[{log.get('name', log['role'])}]: {log['content']}" for log in interaction_log])
        
        dimensions = grader_cfg.get("dimensions", []) or SIMULATOR_TYPES.get("seller", {}).get("default_dimensions", ["ç»¼åˆè¯„ä»·"])
        dim_str = ", ".join([f'"{d}": åˆ†æ•°(1-10)' for d in dimensions])
        
        eval_text, eval_call = await _call_llm(
            "ä½ æ˜¯ä¸€ä½é”€å”®æ•ˆæœè¯„ä¼°ä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹é”€å”®å¯¹è¯çš„æ•ˆæœã€‚",
            f"""é”€å”®å¯¹è¯è®°å½•ï¼š
{dialogue_transcript}

è¯·ä»¥JSONæ ¼å¼è¾“å‡ºï¼š
{{
    "scores": {{{dim_str}}},
    "comments": {{{", ".join([f'"{d}": "è¯„è¯­"' for d in dimensions])}}},
    "conversion": true,
    "conversion_factors": ["å› ç´ "],
    "rejection_factors": ["å› ç´ "],
    "content_strengths": ["ä¼˜åŠ¿"],
    "content_gaps": ["ç¼ºå¤±"],
    "summary": "é”€å”®æ•ˆæœæ€»ä½“è¯„ä»·ï¼ˆ100-200å­—ï¼‰"
}}""",
            step="grader_content_seller",
            temperature=0.5,
        )
        llm_calls.append(eval_call)
        
        result_data = _parse_json_response(eval_text)
        scores = result_data.get("scores", {})
        avg_score = sum(v for v in scores.values() if isinstance(v, (int, float))) / len(scores) if scores else 0
        
        # è¿‡ç¨‹è¯„åˆ†
        grader_outputs = []
        process_grader, process_call = await _run_process_grader(dialogue_transcript, dimensions, grader_cfg)
        if process_call:
            llm_calls.append(process_call)
            grader_outputs.append(process_grader)
        
        total_tokens_in = sum(c.tokens_in for c in llm_calls)
        total_tokens_out = sum(c.tokens_out for c in llm_calls)
        total_cost = sum(c.cost for c in llm_calls)
        
        return TrialResult(
            role="seller", interaction_mode="dialogue",
            nodes=[{"role": log["role"], "content": log["content"], "turn": log.get("turn"), "phase": log.get("phase")} for log in interaction_log],
            result={
                "scores": scores, "comments": result_data.get("comments", {}),
                "strengths": result_data.get("content_strengths", []),
                "weaknesses": result_data.get("content_gaps", []),
                "suggestions": result_data.get("rejection_factors", []),
                "conversion_factors": result_data.get("conversion_factors", []),
                "outcome": "converted" if result_data.get("conversion") else "not_converted",
                "summary": result_data.get("summary", ""),
            },
            grader_outputs=grader_outputs,
            llm_calls=[c.to_dict() for c in llm_calls],
            overall_score=round(avg_score, 2),
            success=True,
            tokens_in=total_tokens_in, tokens_out=total_tokens_out, cost=total_cost,
        )
    except Exception as e:
        return TrialResult(
            role="seller", interaction_mode="dialogue",
            nodes=[{"role": log["role"], "content": log["content"]} for log in interaction_log],
            llm_calls=[c.to_dict() for c in llm_calls],
            success=False, error=str(e),
        )


# ============== Grader ç³»ç»Ÿ ==============

async def _run_content_grader(
    content: str,
    trial_result_data: dict,
    dimensions: list,
    grader_cfg: dict,
) -> Tuple[dict, Optional[LLMCall]]:
    """
    å†…å®¹è¯„åˆ†å™¨ - ç›´æ¥è¯„ä»·å†…å®¹æœ¬èº«çš„è´¨é‡
    åœ¨ review æ¨¡å¼ä¸‹ä½¿ç”¨
    """
    custom_prompt = grader_cfg.get("custom_prompt", "")
    grader_type = grader_cfg.get("type", "content")
    
    if grader_type not in ("content", "combined"):
        return {}, None
    
    system_prompt = custom_prompt or """ä½ æ˜¯ä¸€ä¸ªå†…å®¹è´¨é‡è¯„åˆ†ä¸“å®¶ã€‚åŸºäºä¹‹å‰è§’è‰²çš„è¯„ä¼°åé¦ˆï¼Œ
è¯·å¯¹å†…å®¹è¿›è¡Œç‹¬ç«‹çš„è´¨é‡è¯„åˆ†ï¼Œå…³æ³¨å†…å®¹æœ¬èº«çš„å®¢è§‚è´¨é‡ã€‚
è¾“å‡ºJSONæ ¼å¼çš„è¯„åˆ†ã€‚"""
    
    user_message = f"""å†…å®¹æ‘˜è¦ï¼ˆå‰1000å­—ï¼‰ï¼š
{content[:1000]}

è§’è‰²è¯„ä¼°åé¦ˆæ‘˜è¦ï¼š
- ä¼˜ç‚¹: {trial_result_data.get('strengths', [])}
- é—®é¢˜: {trial_result_data.get('weaknesses', [])}
- æ€»ç»“: {trial_result_data.get('summary', '')}

è¯·è¾“å‡ºJSONï¼š
{{
    "grader_type": "content",
    "quality_score": åˆ†æ•°(1-10),
    "analysis": "å†…å®¹è´¨é‡åˆ†æï¼ˆ50-100å­—ï¼‰",
    "key_issues": ["å…³é”®é—®é¢˜1", "å…³é”®é—®é¢˜2"]
}}"""

    try:
        text, call = await _call_llm(system_prompt, user_message, step="grader_content", temperature=0.4)
        result = _parse_json_response(text)
        result["grader_type"] = "content"
        return result, call
    except Exception:
        return {"grader_type": "content", "error": "è¯„åˆ†å¤±è´¥"}, None


async def _run_process_grader(
    dialogue_transcript: str,
    dimensions: list,
    grader_cfg: dict,
) -> Tuple[dict, Optional[LLMCall]]:
    """
    è¿‡ç¨‹è¯„åˆ†å™¨ - è¯„ä»·äº’åŠ¨è¿‡ç¨‹çš„è´¨é‡
    åœ¨ dialogue/scenario æ¨¡å¼ä¸‹ä½¿ç”¨
    """
    grader_type = grader_cfg.get("type", "content")
    
    if grader_type not in ("process", "combined"):
        return {}, None
    
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªäº¤äº’è¿‡ç¨‹è¯„ä¼°ä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹å¯¹è¯è¿‡ç¨‹çš„è´¨é‡ã€‚
å…³æ³¨ï¼šå¯¹è¯æµç•…æ€§ã€é—®é¢˜è§£å†³æ•ˆç‡ã€ä¿¡æ¯ä¼ é€’æœ‰æ•ˆæ€§ã€ç”¨æˆ·ä½“éªŒã€‚"""

    user_message = f"""å¯¹è¯è®°å½•ï¼š
{dialogue_transcript}

è¯·è¾“å‡ºJSONï¼š
{{
    "grader_type": "process",
    "process_score": åˆ†æ•°(1-10),
    "dialogue_quality": "å¯¹è¯è´¨é‡åˆ†æ",
    "information_delivery": "ä¿¡æ¯ä¼ é€’æ•ˆç‡è¯„ä¼°",
    "user_experience": "ç”¨æˆ·ä½“éªŒè¯„ä¼°",
    "key_moments": ["å¯¹è¯ä¸­çš„å…³é”®æ—¶åˆ»/è½¬æŠ˜ç‚¹"]
}}"""

    try:
        text, call = await _call_llm(system_prompt, user_message, step="grader_process", temperature=0.4)
        result = _parse_json_response(text)
        result["grader_type"] = "process"
        return result, call
    except Exception:
        return {"grader_type": "process", "error": "è¯„åˆ†å¤±è´¥"}, None


# ============== Diagnoser ==============

async def run_diagnoser(
    trial_results: List[TrialResult],
    content_summary: str = "",
    intent: str = "",
) -> Tuple[dict, Optional[LLMCall]]:
    """
    è·¨ Trial è¯Šæ–­å™¨ - åˆ†æå¤šä¸ª Trial çš„ç»“æœï¼Œæ‰¾å‡ºç³»ç»Ÿæ€§é—®é¢˜
    
    Returns:
        (diagnosis_dict, LLMCall_or_None)
    """
    if not trial_results:
        return {"summary": "æ— å¯åˆ†æçš„Trialç»“æœ", "patterns": [], "priorities": []}, None
    
    trials_summary = []
    for tr in trial_results:
        if not tr.success:
            continue
        # ä¼˜å…ˆä½¿ç”¨é…ç½®çš„æ˜¾ç¤ºåç§°ï¼Œé¿å…ç”¨ç¡¬ç¼–ç çš„ "æ•™ç»ƒ/ç¼–è¾‘/ä¸“å®¶"
        display_name = tr.role_display_name or SIMULATOR_TYPES.get(tr.role, {}).get("name", tr.role)
        type_icon = SIMULATOR_TYPES.get(tr.role, {}).get("icon", "ğŸ”")
        summary_text = f"""## {display_name} ({type_icon})
- è¯„åˆ†: {tr.overall_score}/10 | æ¨¡å¼: {tr.interaction_mode}
- ç»“æœ: {tr.result.get('outcome', 'N/A')}
- æ€»ç»“: {tr.result.get('summary', 'N/A')}
- ä¼˜ç‚¹: {', '.join(tr.result.get('strengths', []))}
- é—®é¢˜: {', '.join(tr.result.get('weaknesses', []))}
- å»ºè®®: {', '.join(tr.result.get('suggestions', []))}
- è¯„åˆ†: {json.dumps(tr.result.get('scores', {}), ensure_ascii=False)}"""
        trials_summary.append(summary_text)
    
    trials_text = "\n\n---\n\n".join(trials_summary)
    
    system_prompt = """ä½ æ˜¯ä¸€ä½å†…å®¹è¯„ä¼°è¯Šæ–­ä¸“å®¶ã€‚åˆ†æå¤šä¸ªè¯„ä¼°è§’è‰²çš„åé¦ˆï¼Œæ‰¾å‡ºï¼š
1. **è·¨è§’è‰²ä¸€è‡´æ€§**: å¤šä¸ªè§’è‰²æ˜¯å¦æŒ‡å‡ºäº†ç›¸åŒçš„é—®é¢˜ï¼ŸçŸ›ç›¾åœ¨å“ªï¼Ÿ
2. **ç³»ç»Ÿæ€§ç¼ºé™·**: è¢«å¤šä¸ªè§’è‰²åå¤æåˆ°çš„é—®é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ
3. **æ”¹è¿›ä¼˜å…ˆçº§**: å“ªäº›é—®é¢˜æœ€å€¼å¾—å…ˆä¿®å¤ï¼Ÿ
4. **æ ¸å¿ƒå‘ç°**: æœ€é‡è¦çš„3-5ä¸ªå‘ç°

è¯·è¾“å‡ºä¸¥æ ¼çš„JSONæ ¼å¼ã€‚"""

    user_message = f"""# é¡¹ç›®æ„å›¾
{intent or 'æœªæä¾›'}

# å„è§’è‰²è¯„ä¼°ç»“æœ

{trials_text}

è¯·è¿›è¡Œè·¨è§’è‰²è¯Šæ–­åˆ†æï¼Œè¾“å‡ºJSONæ ¼å¼ï¼š
{{
    "overall_score": ç»¼åˆè¯„åˆ†(1-10),
    "consistency_analysis": "è·¨è§’è‰²ä¸€è‡´æ€§åˆ†æ",
    "patterns": [
        {{"pattern": "æ¨¡å¼", "mentioned_by": ["è§’è‰²"], "severity": "high/medium/low", "description": "æè¿°"}}
    ],
    "priorities": [
        {{"priority": 1, "issue": "é—®é¢˜", "suggested_action": "å»ºè®®", "expected_impact": "å½±å“"}}
    ],
    "key_findings": ["å‘ç°1", "å‘ç°2", "å‘ç°3"],
    "summary": "ç»¼åˆè¯Šæ–­æ€»ç»“ï¼ˆ200-300å­—ï¼‰"
}}"""

    try:
        text, call = await _call_llm(system_prompt, user_message, step="diagnoser", temperature=0.5)
        result = _parse_json_response(text)
        return result, call
    except Exception as e:
        return {
            "overall_score": 0, "summary": f"è¯Šæ–­å¤±è´¥: {str(e)}",
            "patterns": [], "priorities": [], "key_findings": [], "error": str(e),
        }, None


# ============== ä¸»å…¥å£ ==============

async def run_eval(
    content: str,
    roles: List[str] = None,
    creator_profile: str = "",
    intent: str = "",
    personas: List[dict] = None,
    max_turns: int = 5,
    content_field_names: list = None,
) -> Tuple[List[TrialResult], dict]:
    """
    å…¼å®¹æ—§æ¥å£ï¼šè¿è¡Œå®Œæ•´è¯„ä¼°ï¼ˆæ‰€æœ‰è§’è‰²å¹¶è¡Œæ‰§è¡Œï¼‰
    
    æ–°ä»£ç åº”ä½¿ç”¨ run_task_trial() é€ä¸ª Task æ‰§è¡Œ
    """
    if roles is None:
        roles = ["coach", "editor", "expert", "consumer", "seller"]
    
    if personas is None:
        personas = [{"name": "å…¸å‹ç”¨æˆ·", "background": "å¯¹è¯¥é¢†åŸŸæ„Ÿå…´è¶£çš„æ™®é€šè¯»è€…"}]
    
    tasks = []
    
    for role in roles:
        if role in ("coach", "editor", "expert"):
            tasks.append(run_task_trial(
                simulator_type=role, interaction_mode="review",
                content=content, creator_profile=creator_profile, intent=intent,
                grader_config={"type": "content", "dimensions": []},
            ))
        elif role == "consumer":
            for persona in personas[:2]:
                tasks.append(run_task_trial(
                    simulator_type=role, interaction_mode="dialogue",
                    content=content, creator_profile=creator_profile, intent=intent,
                    persona=persona,
                    simulator_config={"max_turns": max_turns},
                    grader_config={"type": "combined", "dimensions": []},
                    content_field_names=content_field_names,
                ))
        elif role == "seller":
            for persona in personas[:2]:
                tasks.append(run_task_trial(
                    simulator_type=role, interaction_mode="dialogue",
                    content=content, creator_profile=creator_profile, intent=intent,
                    persona=persona,
                    simulator_config={"max_turns": max_turns},
                    grader_config={"type": "combined", "dimensions": []},
                    content_field_names=content_field_names,
                ))
    
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    trial_results = []
    for result in results:
        if isinstance(result, Exception):
            trial_results.append(TrialResult(
                role="unknown", interaction_mode="unknown",
                success=False, error=str(result),
            ))
        else:
            trial_results.append(result)
    
    diagnosis, _ = await run_diagnoser(trial_results, content[:500] if content else "", intent)
    
    return trial_results, diagnosis


# ============== å·¥å…·å‡½æ•° ==============

def _parse_json_response(text: str) -> dict:
    """å®‰å…¨è§£æ AI è¿”å›çš„ JSON"""
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        pass
    
    import re
    json_match = re.search(r'```(?:json)?\s*\n?(.*?)\n?```', text, re.DOTALL)
    if json_match:
        try:
            return json.loads(json_match.group(1))
        except json.JSONDecodeError:
            pass
    
    start = text.find('{')
    end = text.rfind('}')
    if start >= 0 and end > start:
        try:
            return json.loads(text[start:end+1])
        except json.JSONDecodeError:
            pass
    
    return {"raw_output": text, "parse_error": True}


def _get_sales_phase(turn: int) -> str:
    """è·å–é”€å”®é˜¶æ®µåç§°"""
    if turn == 0:
        return "opening"
    elif turn <= 2:
        return "need_discovery"
    elif turn <= 4:
        return "value_matching"
    elif turn <= 6:
        return "objection_handling"
    else:
        return "closing"


def format_trial_result_markdown(trial: TrialResult) -> str:
    """å°† Trial ç»“æœæ ¼å¼åŒ–ä¸º Markdown"""
    type_info = SIMULATOR_TYPES.get(trial.role, {"name": trial.role, "icon": "ğŸ“‹"})
    
    md = f"## {type_info.get('icon', 'ğŸ“‹')} {type_info.get('name', trial.role)}è¯„ä¼°\n\n"
    
    if not trial.success:
        md += f"âŒ è¯„ä¼°å¤±è´¥: {trial.error}\n"
        return md
    
    md += f"**ç»¼åˆè¯„åˆ†: {trial.overall_score}/10** | æ¨¡å¼: {trial.interaction_mode}\n\n"
    
    scores = trial.result.get("scores", {})
    if scores:
        md += "### å„ç»´åº¦è¯„åˆ†\n"
        for dim, score in scores.items():
            if isinstance(score, (int, float)):
                bar = "â–ˆ" * int(score) + "â–‘" * (10 - int(score))
                md += f"- {dim}: **{score}/10** {bar}\n"
                comment = trial.result.get("comments", {}).get(dim, "")
                if comment:
                    md += f"  - {comment}\n"
        md += "\n"
    
    if trial.interaction_mode == "dialogue" and trial.nodes:
        md += "### å¯¹è¯è®°å½•\n"
        for node in trial.nodes:
            role_label = {"consumer": "ğŸ—£ æ¶ˆè´¹è€…", "seller": "ğŸ’¼ é”€å”®", "content_rep": "ğŸ“„ å†…å®¹"}.get(node.get("role"), node.get("role", ""))
            md += f"**{role_label}** (ç¬¬{node.get('turn', '?')}è½®): {node.get('content', '')}\n\n"
    
    for label, key, icon in [("ä¼˜ç‚¹", "strengths", "âœ…"), ("é—®é¢˜", "weaknesses", "âš ï¸"), ("æ”¹è¿›å»ºè®®", "suggestions", "ğŸ’¡")]:
        items = trial.result.get(key, [])
        if items:
            md += f"### {icon} {label}\n"
            for s in items:
                md += f"- {s}\n"
            md += "\n"
    
    summary = trial.result.get("summary", "")
    if summary:
        md += f"### æ€»ç»“\n{summary}\n\n"
    
    outcome = trial.result.get("outcome", "")
    if outcome:
        outcome_map = {
            "converted": "âœ… è½¬åŒ–æˆåŠŸ", "not_converted": "âŒ æœªè½¬åŒ–",
            "recommended": "ğŸ‘ æ¨è", "not_recommended": "ğŸ‘ ä¸æ¨è", "reviewed": "ğŸ“ å·²å®¡æŸ¥",
        }
        md += f"**ç»“æœ: {outcome_map.get(outcome, outcome)}**\n\n"
    
    # LLM è°ƒç”¨ç»Ÿè®¡
    if trial.llm_calls:
        md += f"---\nğŸ“Š å…± {len(trial.llm_calls)} æ¬¡ LLM è°ƒç”¨ | "
        md += f"Tokens: {trial.tokens_in}â†‘ {trial.tokens_out}â†“ | "
        md += f"è´¹ç”¨: Â¥{trial.cost:.4f}\n"
    
    return md


def format_diagnosis_markdown(diagnosis: dict) -> str:
    """å°†è¯Šæ–­ç»“æœæ ¼å¼åŒ–ä¸º Markdown"""
    md = "## ğŸ” ç»¼åˆè¯Šæ–­\n\n"
    
    overall = diagnosis.get("overall_score")
    if overall:
        md += f"**ç»¼åˆè¯„åˆ†: {overall}/10**\n\n"
    
    consistency = diagnosis.get("consistency_analysis", "")
    if consistency:
        md += f"### è·¨è§’è‰²ä¸€è‡´æ€§\n{consistency}\n\n"
    
    patterns = diagnosis.get("patterns", [])
    if patterns:
        md += "### ç³»ç»Ÿæ€§é—®é¢˜\n"
        for p in patterns:
            severity_icon = {"high": "ğŸ”´", "medium": "ğŸŸ¡", "low": "ğŸŸ¢"}.get(p.get("severity", ""), "âšª")
            md += f"- {severity_icon} **{p.get('pattern', '')}** (æåˆ°: {', '.join(p.get('mentioned_by', []))})\n"
            md += f"  {p.get('description', '')}\n"
        md += "\n"
    
    priorities = diagnosis.get("priorities", [])
    if priorities:
        md += "### æ”¹è¿›ä¼˜å…ˆçº§\n"
        for p in priorities:
            md += f"**{p.get('priority', '?')}. {p.get('issue', '')}**\n"
            md += f"- å»ºè®®æ“ä½œ: {p.get('suggested_action', '')}\n"
            md += f"- é¢„æœŸå½±å“: {p.get('expected_impact', '')}\n\n"
    
    findings = diagnosis.get("key_findings", [])
    if findings:
        md += "### æ ¸å¿ƒå‘ç°\n"
        for f in findings:
            md += f"- {f}\n"
        md += "\n"
    
    summary = diagnosis.get("summary", "")
    if summary:
        md += f"### æ€»ç»“\n{summary}\n"
    
    return md
