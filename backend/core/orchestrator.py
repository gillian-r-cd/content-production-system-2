# backend/core/orchestrator.py
# åŠŸèƒ½: LangGraph Agentæ ¸å¿ƒç¼–æ’å™¨
# ä¸»è¦ç±»: ContentProductionAgent
# ä¸»è¦å‡½æ•°: create_graph(), run(), stream()
# æ•°æ®ç»“æ„: ContentProductionState

"""
LangGraph Agent æ ¸å¿ƒç¼–æ’å™¨

å®ç°çœŸæ­£çš„Agentæ¶æ„ï¼ˆéif/elseï¼‰ï¼Œä½¿ç”¨LangGraphçŠ¶æ€å›¾å®ç°ï¼š
1. æ„å›¾è·¯ç”±å™¨ - åˆ¤æ–­ç”¨æˆ·æ„å›¾å¹¶è·¯ç”±åˆ°ç›¸åº”èŠ‚ç‚¹
2. é˜¶æ®µèŠ‚ç‚¹ - å„ä¸ªå†…å®¹ç”Ÿäº§é˜¶æ®µ
3. å·¥å…·èŠ‚ç‚¹ - è°ƒç ”ã€ç”Ÿæˆã€æ¨¡æ‹Ÿã€è¯„ä¼°
4. æ£€æŸ¥ç‚¹ - Agentè‡ªä¸»æƒæ§åˆ¶
"""

from typing import TypedDict, Literal, Optional, Any, Annotated, List, Dict, Tuple
from dataclasses import dataclass, field
import operator
import re
import json

from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage

from core.ai_client import ai_client, ChatMessage
from core.prompt_engine import prompt_engine, GoldenContext, PromptContext
from core.models import PROJECT_PHASES
from core.models.project_field import ProjectField
from core.tools.architecture_reader import (
    get_project_architecture, 
    format_architecture_for_llm,
    get_field_content,
    get_intent_and_research,
    get_dependency_contents,
)
import json as json_module  # é¿å…ä¸å±€éƒ¨å˜é‡å†²çª


# ============== è¾…åŠ©å‡½æ•° ==============

def normalize_intent(raw_intent) -> str:
    """
    å°†é¡¹ç›®æ„å›¾è§„èŒƒåŒ–ä¸ºå­—ç¬¦ä¸²æ ¼å¼
    
    æ„å›¾å¯èƒ½æ˜¯ï¼š
    - å­—å…¸ {"åšä»€ä¹ˆ": "...", "ç»™è°çœ‹": "...", "æ ¸å¿ƒä»·å€¼": "..."}
    - å­—ç¬¦ä¸²
    - None æˆ–ç©ºå€¼
    
    è¿”å›ï¼šæ ¼å¼åŒ–çš„å­—ç¬¦ä¸²
    """
    if not raw_intent:
        return ""
    
    if isinstance(raw_intent, dict):
        # è½¬æ¢ä¸ºç»“æ„åŒ–çš„ JSON å­—ç¬¦ä¸²
        return json_module.dumps(raw_intent, ensure_ascii=False, indent=2)
    
    return str(raw_intent)


def normalize_consumer_personas(raw_personas) -> str:
    """
    å°†æ¶ˆè´¹è€…ç”»åƒè§„èŒƒåŒ–ä¸ºå­—ç¬¦ä¸²æ ¼å¼
    
    å¯èƒ½æ˜¯ï¼š
    - JSON å­—ç¬¦ä¸²
    - å­—å…¸å¯¹è±¡
    - None
    """
    if not raw_personas:
        return ""
    
    if isinstance(raw_personas, dict):
        return json_module.dumps(raw_personas, ensure_ascii=False, indent=2)
    
    return str(raw_personas)


# ============== çŠ¶æ€å®šä¹‰ ==============

class ContentProductionState(TypedDict):
    """
    å†…å®¹ç”Ÿäº§çŠ¶æ€
    
    åœ¨æ•´ä¸ªAgentæ‰§è¡Œè¿‡ç¨‹ä¸­ä¼ é€’å’Œæ›´æ–°
    """
    # é¡¹ç›®æ ‡è¯†
    project_id: str
    
    # å½“å‰é˜¶æ®µ
    current_phase: str
    
    # é˜¶æ®µé¡ºåºï¼ˆå¯è°ƒæ•´ï¼‰
    phase_order: List[str]
    
    # æ¯é˜¶æ®µçŠ¶æ€
    phase_status: Dict[str, str]
    
    # Agentè‡ªä¸»æƒè®¾ç½®
    autonomy_settings: Dict[str, bool]
    
    # åˆ›ä½œè€…ç‰¹è´¨ï¼ˆå…¨å±€æ³¨å…¥åˆ°æ¯ä¸ª LLM è°ƒç”¨ï¼‰
    creator_profile: str
    
    # å·²ç”Ÿæˆçš„å­—æ®µ {field_id: content}
    fields: Dict[str, str]
    
    # å¯¹è¯å†å²ï¼ˆç”¨äºå³æ Agentå¯¹è¯ï¼‰
    messages: Annotated[List[BaseMessage], operator.add]
    
    # å½“å‰ç”¨æˆ·è¾“å…¥
    user_input: str
    
    # Agentè¾“å‡ºï¼ˆæµå¼è¾“å‡ºç”¨ï¼‰
    agent_output: str
    
    # æ˜¯å¦ç­‰å¾…äººå·¥ç¡®è®¤
    waiting_for_human: bool
    
    # è·¯ç”±ç›®æ ‡
    route_target: str
    
    # æ˜¯å¦ä½¿ç”¨DeepResearch
    use_deep_research: bool
    
    # æ˜¯å¦æ˜¯äº§å‡ºæ¨¡å¼ï¼ˆç”¨äºåˆ¤æ–­æ˜¯å¦ä¿å­˜ä¸ºå­—æ®µï¼‰
    is_producing: bool
    
    # é”™è¯¯ä¿¡æ¯
    error: Optional[str]
    
    # API è°ƒç”¨ç»Ÿè®¡ï¼ˆç”¨äºæ—¥å¿—è®°å½•ï¼‰
    tokens_in: int
    tokens_out: int
    duration_ms: int
    cost: float
    
    # å®Œæ•´çš„ promptï¼ˆç”¨äºæ—¥å¿—è®°å½•ï¼ŒåŒ…å«ç³»ç»Ÿæç¤ºè¯ï¼‰
    full_prompt: str
    
    # ===== @ å¼•ç”¨ç›¸å…³ï¼ˆæ–°å¢ï¼‰=====
    # @ å¼•ç”¨çš„å­—æ®µååˆ—è¡¨
    references: List[str]
    
    # å¼•ç”¨å­—æ®µçš„å®é™…å†…å®¹ {å­—æ®µå: å†…å®¹}
    referenced_contents: Dict[str, str]
    
    # ===== è§£æåçš„æ„å›¾ï¼ˆæ–°å¢ï¼‰=====
    # æ„å›¾ç±»å‹: modify/generate/query/chat/phase_action/tool_call
    parsed_intent_type: str
    
    # ç›®æ ‡å­—æ®µåï¼ˆå¦‚æœæœ‰ï¼‰
    parsed_target_field: Optional[str]
    
    # æ“ä½œæè¿°
    parsed_operation: str
    
    # ä¿®æ”¹æ“ä½œçš„ç›®æ ‡å­—æ®µï¼ˆç”¨äºä¿å­˜ä¿®æ”¹åçš„å†…å®¹ï¼‰
    modify_target_field: Optional[str]
    
    # ===== å¤šæ„å›¾æ”¯æŒï¼ˆæ–°å¢ï¼‰=====
    # å¾…å¤„ç†çš„æ„å›¾é˜Ÿåˆ—
    pending_intents: List[dict]


# ============== æ„å›¾è·¯ç”± ==============

ROUTE_OPTIONS = Literal[
    "advance_phase",  # æ¨è¿›åˆ°ä¸‹ä¸€é˜¶æ®µ
    "generate",       # ç”Ÿæˆå†…å®¹
    "modify",         # ä¿®æ”¹å†…å®¹
    "research",       # è°ƒç ”
    "generic_research",  # é€šç”¨æ·±åº¦è°ƒç ”
    "evaluate",       # è¯„ä¼°
    "query",          # æŸ¥è¯¢
    "chat",           # è‡ªç”±å¯¹è¯
]


def _detect_modify_target(user_input: str, references: list) -> str:
    """
    å½“æœ‰å¤šä¸ª @ å¼•ç”¨æ—¶ï¼Œæ™ºèƒ½è¯†åˆ«å“ªä¸ªæ˜¯ä¿®æ”¹ç›®æ ‡å­—æ®µã€‚
    
    ç­–ç•¥ï¼šæ‰¾åˆ°ä¿®æ”¹ç±»å…³é”®è¯åæœ€è¿‘çš„ @å¼•ç”¨ â†’ é‚£ä¸ªå°±æ˜¯ç›®æ ‡ã€‚
    è‹¥æ— æ³•åˆ¤æ–­ï¼Œé€€å›æœ€åä¸€ä¸ªå¼•ç”¨ï¼ˆè‡ªç„¶è¯­è¨€é€šå¸¸æŠŠç›®æ ‡æ”¾åœ¨æœ€åï¼‰ã€‚
    
    ç¤ºä¾‹:
      "å‚è€ƒ @é€å­—ç¨¿1 ä¿®æ”¹ @é€å­—ç¨¿2" â†’ é€å­—ç¨¿2ï¼ˆ"ä¿®æ”¹"åé¢çš„å¼•ç”¨ï¼‰
      "ä¿®æ”¹ @é€å­—ç¨¿2 å‚è€ƒ @é€å­—ç¨¿1" â†’ é€å­—ç¨¿2ï¼ˆ"ä¿®æ”¹"åé¢çš„å¼•ç”¨ï¼‰
      "å¸®æˆ‘æ”¹ä¸€ä¸‹ @é€å­—ç¨¿1"         â†’ é€å­—ç¨¿1ï¼ˆå•å¼•ç”¨ç›´æ¥è¿”å›ï¼‰
    """
    if len(references) <= 1:
        return references[0] if references else ""
    
    import re as _re
    
    # ä¿®æ”¹ç±»å…³é”®è¯ï¼ˆæŒ‰é•¿åº¦é™åºï¼Œé¿å…"æ”¹"è¯¯åŒ¹é…"ä¿®æ”¹"çš„å°¾éƒ¨ï¼‰
    modify_keywords = ["å¸®æˆ‘ä¿®æ”¹", "å¸®æˆ‘æ”¹å†™", "å¸®æˆ‘é‡å†™", "å¸®æˆ‘è°ƒæ•´", "å¸®æˆ‘æ›´æ–°",
                        "ä¿®æ”¹", "æ”¹å†™", "é‡å†™", "è°ƒæ•´", "æ›´æ–°", "æ”¹ä¸€ä¸‹", "æ”¹æˆ", "æ”¹"]
    
    # æ„å»º å¼•ç”¨â†’ä½ç½® æ˜ å°„
    ref_positions = {}
    for ref in references:
        pos = user_input.find(f"@{ref}")
        if pos >= 0:
            ref_positions[ref] = pos
    
    # æ‰¾ä¿®æ”¹å…³é”®è¯åé¢æœ€è¿‘çš„å¼•ç”¨
    best_target = None
    best_distance = float('inf')
    
    for keyword in modify_keywords:
        # æœç´¢æ‰€æœ‰å‡ºç°ä½ç½®
        start = 0
        while True:
            kw_pos = user_input.find(keyword, start)
            if kw_pos < 0:
                break
            kw_end = kw_pos + len(keyword)
            
            # æ‰¾è¯¥å…³é”®è¯åé¢æœ€è¿‘çš„å¼•ç”¨
            for ref, ref_pos in ref_positions.items():
                if ref_pos >= kw_pos:  # å¼•ç”¨åœ¨å…³é”®è¯ä¹‹åï¼ˆæˆ–é‡å ä½ç½®ï¼‰
                    distance = ref_pos - kw_end
                    if 0 <= distance < best_distance:
                        best_distance = distance
                        best_target = ref
            
            start = kw_pos + 1  # ç»§ç»­æœç´¢ä¸‹ä¸€ä¸ªä½ç½®
    
    if best_target:
        return best_target
    
    # å…œåº•ï¼šè¿”å›æœ€åä¸€ä¸ªå¼•ç”¨ï¼ˆè‡ªç„¶è¯­è¨€é€šå¸¸æŠŠåŠ¨ä½œå¯¹è±¡æ”¾åœ¨æœ€åï¼‰
    return references[-1]


async def route_intent(state: ContentProductionState) -> ContentProductionState:
    """
    æ„å›¾è·¯ç”±å™¨ï¼ˆé‡æ„ç‰ˆï¼‰
    
    æ ¸å¿ƒåŸåˆ™ï¼šæ„å›¾ä¼˜å…ˆï¼Œä¸è¢«é˜¶æ®µé”å®š
    
    è·¯ç”±é€»è¾‘ï¼š
    1. å¦‚æœæœ‰ @ å¼•ç”¨ â†’ åˆ†ææ˜¯ä¿®æ”¹è¿˜æ˜¯æŸ¥è¯¢
    2. å¦‚æœæ˜¯é˜¶æ®µæ¨è¿›è§¦å‘è¯ â†’ advance_phase
    3. å¦‚æœæ˜¯é˜¶æ®µå¼€å§‹è§¦å‘è¯ + é˜¶æ®µæœªå®Œæˆ â†’ phase_current
    4. å…¶ä»–æƒ…å†µ â†’ LLM æ„å›¾åˆ†ç±»
    """
    user_input = state.get("user_input", "")
    current_phase = state.get("current_phase", "intent")
    phase_status = state.get("phase_status", {})
    references = state.get("references", [])
    referenced_contents = state.get("referenced_contents", {})
    
    current_phase_status = phase_status.get(current_phase, "pending")
    
    # ===== è§„åˆ™ 1: æœ‰ @ å¼•ç”¨æ—¶ï¼Œä¼˜å…ˆåˆ†æå¼•ç”¨æ„å›¾ =====
    # é€šç”¨åŸåˆ™: ç”¨æˆ·å¼•ç”¨äº†å…·ä½“å†…å®¹å¹¶ç»™å‡ºæŒ‡ä»¤ â†’ ä¿®æ”¹; åªæƒ³äº†è§£ â†’ æŸ¥è¯¢
    if references and referenced_contents:
        # æ£€æŸ¥æ˜¯å¦æ˜¯æŸ¥è¯¢æ„å›¾ï¼ˆç”¨æˆ·æƒ³çœ‹/äº†è§£å¼•ç”¨çš„å†…å®¹ï¼‰
        query_keywords = ["æ˜¯ä»€ä¹ˆ", "ä»€ä¹ˆæ„æ€", "è§£é‡Š", "æ€»ç»“", "çœ‹çœ‹", "åˆ†æ", "æ€ä¹ˆæ ·"]
        if any(kw in user_input for kw in query_keywords):
            target_field = references[0]
            return {
                **state, 
                "route_target": "query",
                "parsed_intent_type": "query",
                "parsed_target_field": target_field,
                "parsed_operation": user_input,
            }
        
        # é»˜è®¤: @ å¼•ç”¨ + ä»»ä½•éæŸ¥è¯¢æŒ‡ä»¤ = ä¿®æ”¹æ„å›¾
        # å…³é”®ä¿®å¤ï¼šå¤šä¸ª @ å¼•ç”¨æ—¶ï¼Œæ™ºèƒ½è¯†åˆ«å“ªä¸ªæ˜¯ä¿®æ”¹ç›®æ ‡
        # ä¾‹å¦‚ "å‚è€ƒ @é€å­—ç¨¿1 ä¿®æ”¹ @é€å­—ç¨¿2" â†’ ç›®æ ‡æ˜¯é€å­—ç¨¿2
        target_field = _detect_modify_target(user_input, references)
        print(f"[route_intent] è§„åˆ™1: @å¼•ç”¨ + æŒ‡ä»¤ â†’ modify, target={target_field}")
        return {
            **state, 
            "route_target": "modify",
            "parsed_intent_type": "modify",
            "parsed_target_field": target_field,
            "parsed_operation": user_input,
        }
    
    # ===== è§„åˆ™ 2 & 3: å·²ç§»é™¤ç¡¬ç¼–ç è§¦å‘è¯ =====
    # ç°åœ¨å®Œå…¨ä¾é  LLM ç†è§£ç”¨æˆ·æ„å›¾ï¼ˆè§„åˆ™5ï¼‰
    # è¿™æ ·å¯ä»¥æ­£ç¡®å¤„ç†ï¼š
    # - "è¿›å…¥å¤–å»¶è®¾è®¡é˜¶æ®µ" â†’ advance_phase (è€Œä¸æ˜¯ tool_architecture)
    # - "åœ¨å†…æ¶µç”Ÿäº§éƒ¨åˆ†è¡¥å……å­—æ®µ" â†’ tool_architecture
    # ç¡¬ç¼–ç çš„é—®é¢˜æ˜¯æ— æ³•åŒºåˆ†è¿™ä¸¤ç§æƒ…å†µ
    
    # ===== è§„åˆ™ 4: æ„å›¾åˆ†æé˜¶æ®µçš„é—®ç­”æµç¨‹ï¼ˆé‡æ„ï¼šæ™ºèƒ½åˆ¤æ–­ï¼‰=====
    # å…³é”®æ”¹è¿›ï¼šä¸å†å¼ºåˆ¶é”å®šï¼Œè€Œæ˜¯æ™ºèƒ½åˆ¤æ–­ç”¨æˆ·æ˜¯å¦åœ¨å›ç­”æ„å›¾é—®é¢˜
    phase_order = state.get("phase_order", PROJECT_PHASES)
    if "intent" in phase_order and current_phase == "intent" and current_phase_status != "completed":
        chat_history = state.get("messages", [])
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¾…å›ç­”çš„æ„å›¾åˆ†æé—®é¢˜
        has_pending_question = False
        for m in reversed(chat_history[-10:]):  # åªæ£€æŸ¥æœ€è¿‘10æ¡
            if isinstance(m, AIMessage):
                content = m.content if hasattr(m, 'content') else str(m)
                if "ã€é—®é¢˜" in content and "/3ã€‘" in content:
                    has_pending_question = True
                    break
                # å¦‚æœé‡åˆ°å·²å®Œæˆçš„åˆ†æï¼Œåœæ­¢æ£€æŸ¥
                if "âœ… å·²ç”Ÿæˆ" in content or ('"åšä»€ä¹ˆ"' in content and '"ç»™è°çœ‹"' in content):
                    break
        
        # åˆ¤æ–­ç”¨æˆ·è¾“å…¥æ˜¯å¦æ˜¯"é—®é—®é¢˜"ï¼ˆè€Œé"å›ç­”é—®é¢˜"ï¼‰
        # é—®é—®é¢˜çš„ç‰¹å¾ï¼šä»¥"ï¼Ÿ"ç»“å°¾ã€åŒ…å«ç–‘é—®è¯ã€è¯¢é—® agent èƒ½åŠ›ç­‰
        question_indicators = ["ï¼Ÿ", "?", "ä»€ä¹ˆ", "æ€ä¹ˆ", "å¦‚ä½•", "èƒ½ä¸èƒ½", "å¯ä»¥å—", "æ˜¯ä¸æ˜¯", "å—ï¼Ÿ", "å‘¢ï¼Ÿ", "èƒ½åš", "æœ‰ä»€ä¹ˆ", "ä½ æ˜¯"]
        is_asking_question = any(qi in user_input for qi in question_indicators)
        
        # åªæœ‰åœ¨æœ‰å¾…å›ç­”çš„é—®é¢˜ ä¸” ç”¨æˆ·ä¸æ˜¯åœ¨é—®å…¶ä»–é—®é¢˜æ—¶ï¼Œæ‰è¿›å…¥æ„å›¾åˆ†ææµç¨‹
        if has_pending_question and not is_asking_question:
            print(f"[route_intent] è§„åˆ™4å‘½ä¸­: ç”¨æˆ·å›ç­”æ„å›¾é—®é¢˜")
            return {**state, "route_target": "phase_current", "parsed_intent_type": "phase_action"}
        
        # å¦‚æœç”¨æˆ·æ˜¯åœ¨é—®é—®é¢˜ï¼ˆæ¯”å¦‚é—® agent èƒ½åšä»€ä¹ˆï¼‰ï¼Œåˆ™è¿›å…¥ LLM åˆ¤æ–­
        if is_asking_question:
            print(f"[route_intent] è§„åˆ™4è·³è¿‡: ç”¨æˆ·åœ¨é—®é—®é¢˜ï¼Œäº¤ç»™ LLM åˆ¤æ–­")
    
    # ===== è§„åˆ™ 5: LLM æ™ºèƒ½æ„å›¾åˆ†ç±»ï¼ˆå¸¦æ¶æ„æ„ŸçŸ¥å’Œå·¥å…·é€‰æ‹©ï¼‰=====
    # è·å–é¡¹ç›®æ¶æ„ä¿¡æ¯
    project_id = state.get("project_id", "")
    arch_info = ""
    if project_id:
        try:
            arch = get_project_architecture(project_id)
            if arch:
                arch_info = f"\n\né¡¹ç›®æ¶æ„:\n{format_architecture_for_llm(arch)}"
        except Exception as e:
            arch_info = f"\n\n(æ¶æ„ä¿¡æ¯è·å–å¤±è´¥: {str(e)})"
    
    # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
    ref_info = ""
    if references:
        ref_info = f"\nç”¨æˆ·å¼•ç”¨çš„å­—æ®µ: {', '.join(references)}"
        ref_contents = "\n".join([f"- {name}: {content[:100]}..." for name, content in referenced_contents.items()])
        ref_info += f"\nå¼•ç”¨å†…å®¹é¢„è§ˆ:\n{ref_contents}"
    
    messages = [
        ChatMessage(
            role="system",
            content=f"""ä½ æ˜¯ä¸€ä¸ªç²¾å‡†çš„æ„å›¾åˆ†ç±»å™¨ã€‚æ ¹æ®ç”¨æˆ·è¾“å…¥ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·æ¥æ‰§è¡Œå®é™…æ“ä½œã€‚

## æ ¸å¿ƒåˆ¤æ–­åŸåˆ™
**å¦‚æœç”¨æˆ·æƒ³è¦"åš"æŸäº‹ï¼ˆåˆ›å»º/æ·»åŠ /åˆ é™¤/ä¿®æ”¹/ç”Ÿæˆï¼‰ï¼Œä¼˜å…ˆé€‰æ‹©å·¥å…·æ‰§è¡Œï¼**
**å¦‚æœç”¨æˆ·åªæ˜¯"é—®"æˆ–"èŠ"ï¼Œæ‰é€‰æ‹© chatã€‚**

## å½“å‰ä¸Šä¸‹æ–‡
- å½“å‰é˜¶æ®µ: {current_phase}
- é˜¶æ®µçŠ¶æ€: {current_phase_status}{ref_info}{arch_info}

## å·¥å…·æ¸…å•ï¼ˆä¼˜å…ˆè€ƒè™‘ï¼‰

### tool_architecture - é¡¹ç›®ç»“æ„æ“ä½œ
å½“ç”¨æˆ·æåˆ°è¿™äº›æ¦‚å¿µæ—¶é€‰æ‹©ï¼šåŠ å­—æ®µã€æ·»åŠ å­—æ®µã€è¡¥å……å­—æ®µã€æ–°å¢å­—æ®µã€åˆ å­—æ®µã€åŠ é˜¶æ®µã€åˆ é˜¶æ®µã€ç§»åŠ¨ã€æ‹†åˆ†ã€åˆå¹¶ã€è°ƒæ•´ç»“æ„ã€æ–°å¢æ¨¡å—
ä¾‹å¦‚ï¼š
- "å¸®æˆ‘åŠ ä¸€ä¸ªæ–°å­—æ®µ" â†’ tool_architecture
- "æŠŠè¿™ä¸ªé˜¶æ®µåˆ æ‰" â†’ tool_architecture  
- "æˆ‘æƒ³æ‹†åˆ†ä¸€ä¸‹å†…æ¶µè®¾è®¡" â†’ tool_architecture
- "åœ¨å†…å®¹é‡ŒåŠ ä¸€ä¸ªå…³é”®æ´å¯Ÿ" â†’ tool_architecture
- "åœ¨å†…æ¶µç”Ÿäº§éƒ¨åˆ†è¡¥å……å­—æ®µ" â†’ tool_architecture
- "ç»™å†…æ¶µè®¾è®¡æ·»åŠ ä¸€ä¸ªå­—æ®µ" â†’ tool_architecture

**é‡è¦ï¼šåŒºåˆ†"æ·»åŠ å­—æ®µ"å’Œ"ä¿®æ”¹å­—æ®µå†…å®¹"**
- "åœ¨XXé˜¶æ®µåŠ /æ·»åŠ /è¡¥å……ä¸€ä¸ªå­—æ®µ" â†’ tool_architectureï¼ˆåˆ›å»ºæ–°å­—æ®µï¼‰
- "ä¿®æ”¹XXå­—æ®µçš„å†…å®¹" â†’ modifyï¼ˆä¿®æ”¹å·²æœ‰å­—æ®µçš„å†…å®¹ï¼Œéœ€è¦@å¼•ç”¨ï¼‰

### tool_outline - å†…å®¹è§„åˆ’
å½“ç”¨æˆ·æƒ³è§„åˆ’å†…å®¹ç»“æ„æ—¶é€‰æ‹©ï¼šè®¾è®¡å¤§çº²ã€å†…å®¹æ¡†æ¶ã€æ€ä¹ˆç»„ç»‡ã€è¯¾ç¨‹ç»“æ„ã€æ–‡ç« æ¶æ„
ä¾‹å¦‚ï¼š
- "å¸®æˆ‘è®¾è®¡ä¸€ä¸‹å¤§çº²" â†’ tool_outline
- "è¿™ä¸ªå†…å®¹æ€ä¹ˆç»„ç»‡æ¯”è¾ƒå¥½" â†’ tool_outline

### tool_persona - äººç‰©ç®¡ç†
å½“ç”¨æˆ·æƒ³ç®¡ç†ç”¨æˆ·ç”»åƒ/è§’è‰²æ—¶é€‰æ‹©ï¼šç”Ÿæˆäººç‰©ã€åˆ›å»ºç”¨æˆ·ã€æŸ¥çœ‹äººç‰©ã€é€‰ä¸­ã€å–æ¶ˆé€‰ä¸­ã€è¡¥å……è§’è‰²ã€æ·»åŠ ç”¨æˆ·ã€æ–°å¢äººç‰©
ä¾‹å¦‚ï¼š
- "å†ç”Ÿæˆä¸€ä¸ªç¨‹åºå‘˜ç”¨æˆ·" â†’ tool_persona
- "çœ‹çœ‹æœ‰å“ªäº›äººç‰©" â†’ tool_persona
- "è¡¥å……ä¸€ä¸ªè§’è‰²ï¼Œ22å²åº”å±Šæ¯•ä¸šç”Ÿ" â†’ tool_persona
- "æ·»åŠ ä¸€ä¸ªæ–°ç”¨æˆ·ç”»åƒ" â†’ tool_persona

### tool_skill - æŠ€èƒ½ä½¿ç”¨
å½“ç”¨æˆ·æƒ³ç”¨ç‰¹å®šé£æ ¼/æŠ€èƒ½æ—¶é€‰æ‹©ï¼šç”¨ä¸“ä¸šæ–¹å¼ã€ç”¨æ•…äº‹åŒ–æ–¹å¼ã€ç®€åŒ–å†…å®¹ã€æ‰¹åˆ¤åˆ†æã€æœ‰ä»€ä¹ˆæŠ€èƒ½
ä¾‹å¦‚ï¼š
- "ç”¨ä¸“ä¸šçš„æ–¹å¼å¸®æˆ‘å†™" â†’ tool_skill
- "æœ‰ä»€ä¹ˆæŠ€èƒ½å¯ä»¥ç”¨" â†’ tool_skill

## é˜¶æ®µæµè½¬æ„å›¾ï¼ˆéå¸¸é‡è¦ï¼ä¼˜å…ˆåˆ¤æ–­ï¼‰

### advance_phase - è¿›å…¥ä¸‹ä¸€é˜¶æ®µæˆ–æŒ‡å®šé˜¶æ®µ
å½“ç”¨æˆ·æƒ³æ¨è¿›é¡¹ç›®è¿›åº¦ã€è¿›å…¥æŸä¸ªé˜¶æ®µæ—¶é€‰æ‹©ã€‚
ä¾‹å¦‚ï¼š
- "è¿›å…¥ä¸‹ä¸€é˜¶æ®µ" â†’ advance_phase
- "ç»§ç»­" / "ä¸‹ä¸€æ­¥" â†’ advance_phase
- "å¼€å§‹å¤–å»¶è®¾è®¡" / "è¿›å…¥å¤–å»¶è®¾è®¡é˜¶æ®µ" â†’ advance_phase (target: "design_outer")
- "å¼€å§‹æ¶ˆè´¹è€…è°ƒç ”" â†’ advance_phase (target: "research")
- "å¯ä»¥äº†ï¼Œè¿›å…¥ä¸‹ä¸€æ­¥" â†’ advance_phase

**å…³é”®åŒºåˆ†**ï¼š
- "è¿›å…¥XXé˜¶æ®µ" â†’ advance_phaseï¼ˆæ¨è¿›æµç¨‹ï¼‰
- "åœ¨XXé˜¶æ®µæ·»åŠ å­—æ®µ" â†’ tool_architectureï¼ˆä¿®æ”¹ç»“æ„ï¼‰

### phase_action - æ‰§è¡Œå½“å‰é˜¶æ®µ
å½“ç”¨æˆ·æƒ³åœ¨å½“å‰é˜¶æ®µæ‰§è¡Œæ“ä½œæ—¶é€‰æ‹©ã€‚
ä¾‹å¦‚ï¼š
- "å¼€å§‹" / "å¼€å§‹å§" / "æ‰§è¡Œ" â†’ phase_action
- "å¼€å§‹ç”Ÿæˆ" â†’ phase_action

## å…¶ä»–æ„å›¾ç±»å‹
- modify: ä¿®æ”¹å·²æœ‰å­—æ®µå†…å®¹ï¼ˆé…åˆ@å¼•ç”¨ï¼‰
- query: æŸ¥è¯¢ä¿¡æ¯ï¼ˆæœ‰å“ªäº›é˜¶æ®µã€å½“å‰è¿›åº¦ç­‰ï¼‰
- generate: ç”Ÿæˆå­—æ®µå†…å®¹
- research: æ‰§è¡Œæ¶ˆè´¹è€…è°ƒç ”é˜¶æ®µï¼ˆä»…å½“ç”¨æˆ·æ˜ç¡®è¦åšæ¶ˆè´¹è€…è°ƒç ”/ç”¨æˆ·è°ƒç ”æ—¶ï¼‰
- generic_research: ç”¨æˆ·è¦æ±‚å¯¹æŸä¸ªä¸»é¢˜åšæ·±åº¦è°ƒç ”/èµ„æ–™æœé›†ï¼ˆéæ¶ˆè´¹è€…è°ƒç ”ï¼Œæ¯”å¦‚"å¸®æˆ‘è°ƒç ”ä¸€ä¸‹Xå¸‚åœº"ã€"å¸®æˆ‘æŸ¥ä¸€ä¸‹Yçš„èµ„æ–™"ã€"å¯¹Zåšä¸ªè°ƒç ”"ï¼‰
- evaluate: æ‰§è¡Œè¯„ä¼°é˜¶æ®µ
- **chat**: å½“ç”¨æˆ·åœ¨é—²èŠã€è¯¢é—® agent èƒ½åŠ›ã€é—®é€šç”¨é—®é¢˜æ—¶é€‰æ‹©

**å…³é”®åŒºåˆ† research vs generic_research**:
- "å¼€å§‹æ¶ˆè´¹è€…è°ƒç ”" / "åšç”¨æˆ·è°ƒç ”" â†’ researchï¼ˆæ¶ˆè´¹è€…è°ƒç ”é˜¶æ®µï¼‰
- "å¸®æˆ‘è°ƒç ”ä¸€ä¸‹X" / "æœç´¢ä¸€ä¸‹Yçš„èµ„æ–™" / "å¸®æˆ‘æŸ¥ä¸€ä¸‹Z" â†’ generic_researchï¼ˆé€šç”¨æ·±åº¦è°ƒç ”ï¼Œç»“æœä»¥æ ‡å‡†å­—æ®µå‘ˆç°ï¼‰

## chat æ„å›¾ç¤ºä¾‹ï¼ˆé‡è¦ï¼è¿™äº›å¿…é¡»è·¯ç”±åˆ° chatï¼‰
- "ä½ èƒ½åšä»€ä¹ˆï¼Ÿ" â†’ chat
- "ä½ æ˜¯è°ï¼Ÿ" â†’ chat
- "å¸®æˆ‘è§£é‡Šä¸€ä¸‹è¿™ä¸ªç³»ç»Ÿ" â†’ chat
- "ä½œä¸º agent ä½ æœ‰ä»€ä¹ˆèƒ½åŠ›ï¼Ÿ" â†’ chat
- "hello" / "ä½ å¥½" â†’ chat
- ä»»ä½•è¯¢é—®æ€§çš„é—®é¢˜ï¼ˆä¸æ¶‰åŠå…·ä½“æ“ä½œï¼‰â†’ chat

## å¤åˆæ„å›¾å¤„ç†ï¼ˆéå¸¸é‡è¦ï¼ï¼‰
ç”¨æˆ·çš„ä¸€å¥è¯å¯èƒ½åŒ…å«å¤šä¸ªæ„å›¾ï¼Œä½ å¿…é¡»**å…¨éƒ¨è¯†åˆ«**å¹¶æŒ‰**æ­£ç¡®çš„æ‰§è¡Œé¡ºåº**æ’åˆ—ã€‚

ä¾‹å¦‚ï¼š
- "æ¸…ç©ºå¤–å»¶è®¾è®¡çš„æ‰€æœ‰å­—æ®µï¼Œé‡æ–°å¼€å±•å¤–å»¶è®¾è®¡" â†’ ä¸¤ä¸ªæ„å›¾ï¼š
  1. tool_architectureï¼ˆåˆ é™¤ design_outer çš„å­—æ®µï¼‰
  2. advance_phaseï¼ˆé‡æ–°å¼€å§‹ design_outerï¼‰
- "åˆ æ‰è¿™ä¸ªå­—æ®µï¼Œç„¶åå¸®æˆ‘ç”Ÿæˆä¸€ä¸ªæ–°çš„" â†’ ä¸¤ä¸ªæ„å›¾ï¼š
  1. tool_architectureï¼ˆåˆ é™¤å­—æ®µï¼‰
  2. generateï¼ˆç”Ÿæˆå†…å®¹ï¼‰
- "å…ˆè°ƒç ”ä¸€ä¸‹ï¼Œå†ç”Ÿæˆå†…å®¹" â†’ ä¸¤ä¸ªæ„å›¾ï¼š
  1. research
  2. generate

## è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰
å¦‚æœåªæœ‰ä¸€ä¸ªæ„å›¾ï¼š
{{"intents": [{{"type": "æ„å›¾ç±»å‹", "target": "ç›®æ ‡å¯¹è±¡", "operation": "æ“ä½œæè¿°"}}]}}

å¦‚æœæœ‰å¤šä¸ªæ„å›¾ï¼ŒæŒ‰æ‰§è¡Œé¡ºåºæ’åˆ—ï¼š
{{"intents": [
  {{"type": "ç¬¬ä¸€ä¸ªæ„å›¾", "target": "ç›®æ ‡1", "operation": "æè¿°1"}},
  {{"type": "ç¬¬äºŒä¸ªæ„å›¾", "target": "ç›®æ ‡2", "operation": "æè¿°2"}}
]}}

åªè¾“å‡ºJSONã€‚"""
        ),
        ChatMessage(role="user", content=user_input),
    ]
    
    response = await ai_client.async_chat(messages, temperature=0.3)
    
    # è§£æ JSON å“åº” - æ”¯æŒå¤šæ„å›¾åˆ—è¡¨
    import json
    intents_list = []
    try:
        content = response.content.strip()
        if content.startswith("```"):
            content = content.split("```")[1]
            if content.startswith("json"):
                content = content[4:]
        result = json.loads(content)
        
        # æ–°æ ¼å¼ï¼šintents åˆ—è¡¨
        if "intents" in result and isinstance(result["intents"], list):
            intents_list = result["intents"]
            print(f"[route_intent] LLMè¿”å› {len(intents_list)} ä¸ªæ„å›¾")
        # å…¼å®¹æ—§æ ¼å¼ï¼šå•ä¸ªæ„å›¾
        elif "intent" in result:
            intents_list = [{
                "type": result.get("intent", "chat"),
                "target": result.get("target", ""),
                "operation": result.get("operation", user_input)
            }]
        else:
            intents_list = [{"type": "chat", "target": "", "operation": user_input}]
            
    except Exception as e:
        print(f"[route_intent] JSONè§£æå¤±è´¥: {e}, åŸå§‹å“åº”: {response.content[:200]}")
        intents_list = [{"type": "chat", "target": "", "operation": user_input}]
    
    # å–ç¬¬ä¸€ä¸ªæ„å›¾ä½œä¸ºå½“å‰æ„å›¾ï¼Œå…¶ä½™æ”¾å…¥ pending_intents
    current_intent = intents_list[0] if intents_list else {"type": "chat", "target": "", "operation": user_input}
    pending_intents = intents_list[1:] if len(intents_list) > 1 else []
    
    intent = current_intent.get("type", "chat")
    target = current_intent.get("target", "")
    operation = current_intent.get("operation", user_input)
    print(f"[route_intent] å½“å‰æ„å›¾: {intent}, target={target}, å¾…å¤„ç†æ„å›¾: {len(pending_intents)} ä¸ª")
    
    # æ ‡å‡†åŒ–æ„å›¾ï¼ˆæ”¯æŒç®€å†™æ˜ å°„ï¼‰
    intent_mapping = {
        # ç®€å†™ â†’ å®Œæ•´å½¢å¼
        "architecture": "tool_architecture",
        "outline": "tool_outline", 
        "persona": "tool_persona",
        "skill": "tool_skill",
        # å®Œæ•´å½¢å¼ä¿æŒä¸å˜
        "tool_architecture": "tool_architecture",
        "tool_outline": "tool_outline",
        "tool_persona": "tool_persona",
        "tool_skill": "tool_skill",
        # å…¶ä»–æ„å›¾
        "advance_phase": "advance_phase",
        "generate": "generate",
        "modify": "modify",
        "research": "research",
        "generic_research": "generic_research",
        "evaluate": "evaluate",
        "query": "query",
        "phase_action": "phase_action",
        "chat": "chat",
    }
    intent = intent_mapping.get(intent, "chat")
    print(f"[route_intent] æ ‡å‡†åŒ–å: intent={intent}")
    
    # å·¥å…·æ„å›¾ç›´æ¥ä½¿ç”¨å…·ä½“çš„å·¥å…·ç±»å‹ï¼Œä»¥ä¾¿æ­£ç¡®è·¯ç”±
    if intent.startswith("tool_"):
        route_target = intent  # ä¿ç•™å…·ä½“ç±»å‹: tool_architecture, tool_persona ç­‰
    elif intent == "phase_action":
        route_target = "phase_current"
    else:
        route_target = intent
    
    # å¦‚æœæ˜¯ modify/query ä½†æ²¡æœ‰ç›®æ ‡å­—æ®µï¼Œå°è¯•ä»å¼•ç”¨æˆ–è§£æç»“æœä¸­è·å–
    target_field = target if target else None
    if intent in ["modify", "query"] and references and not target_field:
        target_field = references[0]
    
    # æ ‡å‡†åŒ– pending_intents ä¸­çš„æ„å›¾ç±»å‹
    normalized_pending = []
    for pi in pending_intents:
        pi_type = intent_mapping.get(pi.get("type", "chat"), "chat")
        normalized_pending.append({
            "type": pi_type,
            "target": pi.get("target", ""),
            "operation": pi.get("operation", "")
        })
    
    return {
        **state, 
        "route_target": route_target,
        "parsed_intent_type": intent,
        "parsed_target_field": target_field,
        "parsed_operation": operation,
        "pending_intents": normalized_pending,  # æ–°å¢ï¼šå¾…å¤„ç†çš„æ„å›¾åˆ—è¡¨
    }


# ============== é˜¶æ®µèŠ‚ç‚¹ ==============

async def intent_analysis_node(state: ContentProductionState) -> ContentProductionState:
    """
    æ„å›¾åˆ†æèŠ‚ç‚¹
    
    ä¸¥æ ¼æµç¨‹ï¼š
    1. é—®é¢˜1ï¼ˆå›ºå®šï¼‰ï¼šåšä»€ä¹ˆé¡¹ç›®
    2. é—®é¢˜2ï¼ˆAIç”Ÿæˆï¼‰ï¼šæ ¹æ®ç”¨æˆ·å›ç­”çš„ä¸ªæ€§åŒ–è·Ÿè¿›
    3. é—®é¢˜3ï¼ˆAIç”Ÿæˆï¼‰ï¼šæ ¹æ®ç”¨æˆ·å›ç­”çš„ä¸ªæ€§åŒ–è·Ÿè¿›
    4. é—®å®Œä¸¥æ ¼3ä¸ªé—®é¢˜åï¼Œæ‰ç”Ÿæˆæ„å›¾åˆ†æ
    
    å…³é”®ï¼šåªç»Ÿè®¡ã€æœ€è¿‘ä¸€è½®ã€‘çš„é—®é¢˜ï¼Œé‡åˆ°æ„å›¾åˆ†æç»“æœå°±é‡ç½®
    """
    creator_profile = state.get("creator_profile", "")
    user_input = state.get("user_input", "")
    chat_history = state.get("messages", [])
    
    MAX_QUESTIONS = 3
    
    # ===== å…³é”®ï¼šåªç»Ÿè®¡ã€å½“å‰è½®æ¬¡ã€‘çš„é—®é¢˜ =====
    # åœæ­¢æ¡ä»¶ï¼šé‡åˆ°æ„å›¾åˆ†æç»“æœã€ç¡®è®¤æ¶ˆæ¯ã€æˆ–éé—®é¢˜çš„AIæ¶ˆæ¯
    question_count = 0
    found_questions = []  # è®°å½•æ‰¾åˆ°çš„é—®é¢˜ç¼–å·
    
    for m in reversed(chat_history):
        if isinstance(m, AIMessage):
            content = m.content if hasattr(m, 'content') else str(m)
            
            # åœæ­¢æ¡ä»¶1ï¼šç¡®è®¤æ¶ˆæ¯ï¼ˆä¸Šä¸€è½®å·²å®Œæˆï¼‰
            if "âœ… å·²ç”Ÿæˆ" in content or "å·²ç”Ÿæˆã€æ„å›¾åˆ†æã€‘" in content:
                break
            
            # åœæ­¢æ¡ä»¶2ï¼šæ„å›¾åˆ†æJSONç»“æœ
            if ('"åšä»€ä¹ˆ"' in content and '"ç»™è°çœ‹"' in content) or \
               ("åšä»€ä¹ˆ" in content and "ç»™è°çœ‹" in content and "æœŸæœ›è¡ŒåŠ¨" in content and "ã€é—®é¢˜" not in content):
                break
            
            # ç»Ÿè®¡é—®é¢˜æ¶ˆæ¯
            if "ã€é—®é¢˜" in content and "/3ã€‘" in content:
                # æå–é—®é¢˜ç¼–å·
                import re
                match = re.search(r'ã€é—®é¢˜\s*(\d)/3ã€‘', content)
                if match:
                    q_num = int(match.group(1))
                    if q_num not in found_questions:
                        found_questions.append(q_num)
                        question_count += 1
    
    # æœ¬æ¬¡åº”è¯¥é—®ç¬¬å‡ ä¸ªé—®é¢˜
    next_question_num = question_count + 1
    
    # ===== åˆ¤æ–­æ˜¯å¦è¿›å…¥äº§å‡ºæ¨¡å¼ =====
    # æ¡ä»¶ï¼šå·²ç»é—®å®Œ3ä¸ªé—®é¢˜
    all_questions_done = question_count >= MAX_QUESTIONS
    is_producing = all_questions_done
    
    # æ„å»ºå¯¹è¯å†å²ä¸Šä¸‹æ–‡ï¼ˆåªä¿ç•™æœ€åä¸€è½®æœ‰æ•ˆå¯¹è¯ï¼‰
    # æ‰¾åˆ°æœ€åä¸€ä¸ª"å¼€å§‹"æˆ–"é—®é¢˜ 1/3"çš„ä½ç½®ï¼Œåªä½¿ç”¨é‚£ä¹‹åçš„å†å²
    last_start_idx = 0
    for idx, msg in enumerate(chat_history):
        content = msg.content if hasattr(msg, 'content') else str(msg)
        if isinstance(msg, HumanMessage) and content.strip() in ["å¼€å§‹", "å¼€å§‹å§", "start", "Start"]:
            last_start_idx = idx
        elif isinstance(msg, AIMessage) and "ã€é—®é¢˜ 1/3ã€‘" in content:
            last_start_idx = idx
    
    # åªä½¿ç”¨æœ€åä¸€è½®å¯¹è¯ï¼ˆä»æœ€åä¸€ä¸ªèµ·ç‚¹å¼€å§‹ï¼‰
    relevant_history = chat_history[last_start_idx:]
    
    # å»é‡ï¼šè¿‡æ»¤æ‰é‡å¤çš„é—®ç­”
    seen_contents = set()
    deduped_history = []
    for msg in relevant_history:
        content = msg.content if hasattr(msg, 'content') else str(msg)
        # è·³è¿‡å•çº¯çš„"å¼€å§‹"
        if isinstance(msg, HumanMessage) and content.strip() in ["å¼€å§‹", "å¼€å§‹å§", "start", "Start"]:
            continue
        # å»é‡
        if content not in seen_contents:
            seen_contents.add(content)
            deduped_history.append(msg)
    
    history_context = ""
    for msg in deduped_history[-10:]:
        if isinstance(msg, HumanMessage):
            history_context += f"ç”¨æˆ·: {msg.content}\n"
        elif isinstance(msg, AIMessage):
            history_context += f"åŠ©æ‰‹: {msg.content}\n"
    
    new_phase_status = {**state.get("phase_status", {})}
    
    if is_producing:
        # ===== äº§å‡ºæ¨¡å¼ï¼šç”Ÿæˆæ„å›¾åˆ†æ =====
        system_prompt = prompt_engine.INTENT_PRODUCING_PROMPT
        user_prompt = f"""é¡¹ç›®èƒŒæ™¯:
{creator_profile}

å®Œæ•´å¯¹è¯å†å²:
{history_context}

ç”¨æˆ·æœ€æ–°è¾“å…¥: {user_input}

è¯·æ ¹æ®ä»¥ä¸Š3ä¸ªé—®é¢˜çš„å›ç­”ï¼Œç”Ÿæˆç»“æ„åŒ–çš„æ„å›¾åˆ†æã€‚"""
    
        messages = [
            ChatMessage(role="system", content=system_prompt),
            ChatMessage(role="user", content=user_prompt),
        ]
        
        response = await ai_client.async_chat(messages, temperature=0.7)
        # æ³¨æ„ï¼šä¸è‡ªåŠ¨è®¾ç½® completedï¼Œéœ€è¦ç”¨æˆ·ç‚¹å‡»ç¡®è®¤æŒ‰é’®åç”± advance æ¥å£è®¾ç½®
        new_phase_status["intent"] = "in_progress"
        
        # æ„å»ºå®Œæ•´çš„ prompt ç”¨äºæ—¥å¿—è®°å½•
        full_prompt = f"[System]\n{system_prompt}\n\n[User]\n{user_prompt}"
        
        # æ„å»ºç®€æ´çš„ç¡®è®¤æ¶ˆæ¯ï¼ˆå®é™…å†…å®¹ä¿å­˜åˆ°å­—æ®µï¼‰
        # æ³¨æ„ï¼šæ„å›¾åˆ†æç»“æœé€šè¿‡ agent_output ä¿å­˜åˆ° ProjectField
        # åç»­é˜¶æ®µé€šè¿‡å­—æ®µä¾èµ–è·å–ï¼Œä¸å†å­˜å…¥å…¨å±€ golden_context
        confirm_message = "âœ… å·²ç”Ÿæˆã€æ„å›¾åˆ†æã€‘ï¼Œè¯·åœ¨å·¦ä¾§å·¥ä½œå°æŸ¥çœ‹ã€‚è¾“å…¥ã€Œç»§ç»­ã€è¿›å…¥æ¶ˆè´¹è€…è°ƒç ”é˜¶æ®µã€‚"
        
        return {
            **state,
            "agent_output": response.content,  # å®Œæ•´å†…å®¹ç”¨äºä¿å­˜å­—æ®µ
            "display_output": confirm_message,  # å¯¹è¯åŒºæ˜¾ç¤ºç®€æ´ç¡®è®¤
            "messages": [AIMessage(content=confirm_message)],
            "phase_status": new_phase_status,
            "is_producing": True,
            "waiting_for_human": True,  # å…³é”®ï¼šç­‰å¾…ç”¨æˆ·ç¡®è®¤ï¼Œä¸è‡ªåŠ¨æ¨è¿›
            "tokens_in": response.tokens_in,
            "tokens_out": response.tokens_out,
            "duration_ms": response.duration_ms,
            "cost": response.cost,
            "full_prompt": full_prompt,  # å®Œæ•´ prompt ç”¨äºæ—¥å¿—
        }
    
    # ===== æé—®æ¨¡å¼ =====
    new_phase_status["intent"] = "in_progress"
    
    if next_question_num == 1:
        # ç¬¬ä¸€ä¸ªé—®é¢˜ï¼šå›ºå®šå†…å®¹ï¼Œä¸è°ƒç”¨AI
        question_text = "ã€é—®é¢˜ 1/3ã€‘ä½ è¿™æ¬¡æƒ³åšä»€ä¹ˆå†…å®¹ï¼Ÿè¯·ç®€å•æè¿°ä¸€ä¸‹ï¼ˆæ¯”å¦‚ï¼šä¸€ç¯‡æ–‡ç« ã€ä¸€ä¸ªè§†é¢‘è„šæœ¬ã€ä¸€ä»½äº§å“ä»‹ç»ã€ä¸€å¥—åŸ¹è®­è¯¾ä»¶ç­‰ï¼‰ï¼Œå¹¶è¡¥å……ä¸€å¥è¯´æ˜å®ƒçš„å¤§è‡´ä¸»é¢˜æˆ–æ–¹å‘ã€‚"
        
        return {
            **state,
            "agent_output": question_text,
            "messages": [AIMessage(content=question_text)],
            "phase_status": new_phase_status,
            "is_producing": False,
            "tokens_in": 0,
            "tokens_out": 0,
            "duration_ms": 0,
            "cost": 0.0,
            "full_prompt": "[å›ºå®šé—®é¢˜ï¼Œæ— AIè°ƒç”¨]",
        }
    
    elif next_question_num == 2:
        # ç¬¬äºŒä¸ªé—®é¢˜ï¼šåŸºäºç”¨æˆ·ç¬¬ä¸€ä¸ªå›ç­”ï¼ŒAIç”Ÿæˆä¸ªæ€§åŒ–é—®é¢˜
        system_prompt = f"""ä½ æ˜¯å†…å®¹ç­–ç•¥é¡¾é—®ã€‚ç”¨æˆ·åˆšåˆšæè¿°äº†ä»–æƒ³åšçš„å†…å®¹é¡¹ç›®ã€‚

ä½ ç°åœ¨è¦é—®ç¬¬2ä¸ªé—®é¢˜ï¼Œäº†è§£ç›®æ ‡å—ä¼—ã€‚

å¯¹è¯å†å²:
{history_context}
ç”¨æˆ·æœ€æ–°å›ç­”: {user_input}

è¯·è¾“å‡ºä¸€ä¸ªé’ˆå¯¹æ€§çš„é—®é¢˜ï¼Œæ ¼å¼å¿…é¡»æ˜¯ï¼š
ã€é—®é¢˜ 2/3ã€‘ï¼ˆé—®é¢˜å†…å®¹ï¼‰

é—®é¢˜åº”è¯¥å…³äºï¼šè¿™ä¸ªå†…å®¹ä¸»è¦ç»™è°çœ‹ï¼Ÿç›®æ ‡è¯»è€…æ˜¯è°ï¼Ÿä»–ä»¬æœ‰ä»€ä¹ˆç—›ç‚¹ï¼Ÿ

åªè¾“å‡ºé—®é¢˜ï¼Œä¸è¦æœ‰å…¶ä»–å†…å®¹ã€‚"""
        
        messages = [ChatMessage(role="system", content=system_prompt)]
        
        response = await ai_client.async_chat(messages, temperature=0.7)
        output = response.content
        
        # ç¡®ä¿æ ¼å¼æ­£ç¡®
        if "ã€é—®é¢˜ 2/3ã€‘" not in output:
            output = f"ã€é—®é¢˜ 2/3ã€‘{output}"
        
        return {
            **state,
            "agent_output": output,
            "messages": [AIMessage(content=output)],
            "phase_status": new_phase_status,
            "is_producing": False,
            "tokens_in": response.tokens_in,
            "tokens_out": response.tokens_out,
            "duration_ms": response.duration_ms,
            "cost": response.cost,
            "full_prompt": f"[System]\n{system_prompt}",
        }
    
    else:  # next_question_num == 3
        # ç¬¬ä¸‰ä¸ªé—®é¢˜ï¼šåŸºäºå‰ä¸¤ä¸ªå›ç­”ï¼ŒAIç”Ÿæˆä¸ªæ€§åŒ–é—®é¢˜
        system_prompt = f"""ä½ æ˜¯å†…å®¹ç­–ç•¥é¡¾é—®ã€‚ç”¨æˆ·å·²ç»æè¿°äº†å†…å®¹é¡¹ç›®å’Œç›®æ ‡å—ä¼—ã€‚

ä½ ç°åœ¨è¦é—®ç¬¬3ä¸ªä¹Ÿæ˜¯æœ€åä¸€ä¸ªé—®é¢˜ï¼Œäº†è§£æœŸæœ›çš„ç”¨æˆ·è¡ŒåŠ¨ã€‚

å¯¹è¯å†å²:
{history_context}
ç”¨æˆ·æœ€æ–°å›ç­”: {user_input}

è¯·è¾“å‡ºä¸€ä¸ªé’ˆå¯¹æ€§çš„é—®é¢˜ï¼Œæ ¼å¼å¿…é¡»æ˜¯ï¼š
ã€é—®é¢˜ 3/3ã€‘ï¼ˆé—®é¢˜å†…å®¹ï¼‰

é—®é¢˜åº”è¯¥å…³äºï¼šçœ‹å®Œè¿™ä¸ªå†…å®¹åï¼Œä½ æœ€å¸Œæœ›è¯»è€…é‡‡å–ä»€ä¹ˆå…·ä½“è¡ŒåŠ¨ï¼Ÿ

åªè¾“å‡ºé—®é¢˜ï¼Œä¸è¦æœ‰å…¶ä»–å†…å®¹ã€‚"""
        
        messages = [ChatMessage(role="system", content=system_prompt)]
        
        response = await ai_client.async_chat(messages, temperature=0.7)
        output = response.content
        
        # ç¡®ä¿æ ¼å¼æ­£ç¡®
        if "ã€é—®é¢˜ 3/3ã€‘" not in output:
            output = f"ã€é—®é¢˜ 3/3ã€‘{output}"
        
        return {
            **state,
            "agent_output": output,
            "messages": [AIMessage(content=output)],
            "phase_status": new_phase_status,
            "is_producing": False,
            "tokens_in": response.tokens_in,
            "tokens_out": response.tokens_out,
            "duration_ms": response.duration_ms,
            "cost": response.cost,
            "full_prompt": f"[System]\n{system_prompt}",
        }


async def research_node(state: ContentProductionState) -> ContentProductionState:
    """
    æ¶ˆè´¹è€…è°ƒç ”èŠ‚ç‚¹
    
    æ ¹æ®è®¾ç½®ä½¿ç”¨DeepResearchæˆ–å¿«é€Ÿç”Ÿæˆ
    """
    from core.tools import deep_research, quick_research
    
    creator_profile = state.get("creator_profile", "")
    project_id = state.get("project_id", "")
    use_deep = state.get("use_deep_research", True)
    
    # ä»å­—æ®µè·å–æ„å›¾åˆ†æç»“æœï¼ˆé€šè¿‡å­—æ®µä¾èµ–ï¼Œè€Œé golden_contextï¼‰
    deps = get_intent_and_research(project_id)
    intent = deps.get("intent", "")
    
    if not intent:
        # fallbackï¼šä½¿ç”¨ç”¨æˆ·è¾“å…¥ï¼ˆè¿™ä¸åº”è¯¥å‘ç”Ÿï¼Œæ„å›¾åˆ†æåº”è¯¥å…ˆå®Œæˆï¼‰
        intent = state.get("user_input", "")
        print(f"[research_node] è­¦å‘Šï¼šæœªæ‰¾åˆ°é¡¹ç›®æ„å›¾ï¼Œä½¿ç”¨ç”¨æˆ·è¾“å…¥: {intent[:50]}...")
    
    print(f"[research_node] è·å–åˆ°æ„å›¾åˆ†æç»“æœ ({len(intent)} å­—ç¬¦): {intent[:200]}...")
    
    # ä»æ„å›¾ä¸­æå–ç®€æ´çš„è°ƒç ”ä¸»é¢˜ï¼ˆä¸åµŒå…¥å®Œæ•´ intentï¼Œé¿å…åœ¨ synthesize_report ä¸­é‡å¤ï¼‰
    # intent æ ¼å¼: "**åšä»€ä¹ˆ**: ...\n**ç»™è°çœ‹**: ...\n**æ ¸å¿ƒä»·å€¼**: ..."
    import re as _re
    _do_match = _re.search(r"\*\*åšä»€ä¹ˆ\*\*[:ï¼š]\s*(.+?)(?:\n|$)", intent)
    if _do_match:
        # ä» "åšä»€ä¹ˆ" æå–æ ¸å¿ƒä¸»é¢˜ï¼ˆæˆªå–å‰80å­—ï¼Œä¿æŒç®€æ´ï¼‰
        topic = _do_match.group(1).strip()[:80]
        query = f"ç›®æ ‡ç”¨æˆ·è°ƒç ”: {topic}"
    elif intent:
        query = f"ç›®æ ‡ç”¨æˆ·è°ƒç ”: {intent[:80]}"
    else:
        query = "æ¶ˆè´¹è€…è°ƒç ”"
    
    import json
    import uuid
    
    try:
        if use_deep:
            report = await deep_research(query, intent)
        else:
            report = await quick_research(query, intent)
        
        # ä¸ºæ¯ä¸ª persona ç”Ÿæˆå”¯ä¸€ID
        personas_data = []
        for i, persona in enumerate(report.personas):
            persona_dict = {
                "id": f"persona_{i+1}_{uuid.uuid4().hex[:8]}",
                "name": persona.name,
                "basic_info": persona.basic_info if hasattr(persona, 'basic_info') and persona.basic_info else {
                    "age_range": "25-45å²",
                    "industry": "æœªçŸ¥",
                    "position": "æœªçŸ¥",
                },
                "background": persona.background,
                "pain_points": persona.pain_points,
                "selected": True,  # é»˜è®¤é€‰ä¸­
            }
            personas_data.append(persona_dict)
        
        # æ„å»ºç»“æ„åŒ–çš„è°ƒç ”æŠ¥å‘Šï¼ˆJSONæ ¼å¼ï¼Œä¾¿äºå‰ç«¯è§£æï¼‰
        report_json = {
            "summary": report.summary,
            "consumer_profile": report.consumer_profile,
            "pain_points": report.pain_points,
            "value_propositions": report.value_propositions,
            "personas": personas_data,
            "sources": report.sources if hasattr(report, 'sources') else [],
            "search_queries": report.search_queries if hasattr(report, 'search_queries') else [],
            "is_deep_research": use_deep,
        }
        
        # JSONæ ¼å¼å­˜å‚¨ï¼ˆç»™ä¸­é—´æ å­—æ®µç”¨ï¼‰
        report_content = json.dumps(report_json, ensure_ascii=False, indent=2)
        
        # ç®€æ´å±•ç¤ºæ–‡æœ¬ï¼ˆç»™å³ä¾§å¯¹è¯åŒºï¼‰
        display_text = f"âœ… å·²ç”Ÿæˆæ¶ˆè´¹è€…è°ƒç ”æŠ¥å‘Šï¼Œè¯·åœ¨å·¦ä¾§å·¥ä½œå°æŸ¥çœ‹ã€‚\n\n"
        display_text += f"**æ€»ä½“æ¦‚è¿°**: {report.summary[:100]}...\n\n"
        display_text += f"**å…¸å‹ç”¨æˆ·**: {', '.join([p['name'] for p in personas_data[:3]])}"
        
        # æ³¨æ„ï¼šæ¶ˆè´¹è€…è°ƒç ”ç»“æœé€šè¿‡ agent_output ä¿å­˜åˆ° ProjectField
        # åç»­é˜¶æ®µé€šè¿‡å­—æ®µä¾èµ–è·å–ï¼Œä¸å†å­˜å…¥å…¨å±€ golden_context
        
        # æ„å»º full_prompt ç”¨äºæ—¥å¿— - è®°å½•å®Œæ•´ä¸Šä¸‹æ–‡
        full_prompt = f"""[æ¶ˆè´¹è€…è°ƒç ” - å®Œæ•´ä¸Šä¸‹æ–‡]

=== 1. è°ƒç ”å‚æ•° ===
æŸ¥è¯¢ä¸»é¢˜: {query}
ä½¿ç”¨æ·±åº¦è°ƒç ”: {use_deep}

=== 2. åˆ›ä½œè€…ç‰¹è´¨ ===
{creator_profile or 'æœªè®¾ç½®'}

=== 3. é¡¹ç›®æ„å›¾ (å®Œæ•´) ===
{intent if intent else 'æœªè®¾ç½®'}

=== 4. DeepResearch æµç¨‹ ===
æ­¥éª¤1: è§„åˆ’æœç´¢æŸ¥è¯¢ (LLMç”Ÿæˆ3-5ä¸ªæœç´¢è¯)
æ­¥éª¤2: Tavily Search API æœç´¢+å†…å®¹æå– (æ¯ä¸ªæŸ¥è¯¢5æ¡ç»“æœ)
æ­¥éª¤3: ç»¼åˆåˆ†æç”ŸæˆæŠ¥å‘Š (å«å¼•ç”¨æ ‡æ³¨)

=== 5. ç»¼åˆåˆ†ææç¤ºè¯ ===
[System]
ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„ç”¨æˆ·ç ”ç©¶ä¸“å®¶ã€‚è¯·åŸºäºæœç´¢ç»“æœï¼Œç”Ÿæˆä¸€ä»½è¯¦ç»†çš„æ¶ˆè´¹è€…è°ƒç ”æŠ¥å‘Šã€‚

è¾“å‡ºJSONæ ¼å¼ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- summary: æ€»ä½“æ¦‚è¿°ï¼ˆ200å­—ä»¥å†…ï¼‰
- consumer_profile: æ¶ˆè´¹è€…ç”»åƒå¯¹è±¡ {{age_range, occupation, characteristics, behaviors}}
- pain_points: æ ¸å¿ƒç—›ç‚¹åˆ—è¡¨ï¼ˆ3-5ä¸ªï¼‰
- value_propositions: ä»·å€¼ä¸»å¼ åˆ—è¡¨ï¼ˆ3-5ä¸ªï¼‰
- personas: 3ä¸ªå…¸å‹ç”¨æˆ·å°ä¼ ï¼Œæ¯ä¸ªåŒ…å« {{name, background, story, pain_points}}

[User]
# è°ƒç ”ä¸»é¢˜
{query}

# é¡¹ç›®æ„å›¾
{intent if intent else 'æœªè®¾ç½®'}

# æœç´¢ç»“æœ
[... å®é™…æœç´¢ç»“æœå†…å®¹ï¼ˆæœ€å¤š15000å­—ç¬¦ï¼‰...]

=== 6. ç”Ÿæˆç»“æœ ===
æŠ¥å‘Šå·²ç”Ÿæˆï¼ŒåŒ…å« {len(personas_data)} ä¸ªç”¨æˆ·ç”»åƒ
æ¥æº: {len(report.sources) if hasattr(report, 'sources') else 0} ä¸ªç½‘é¡µ
æœç´¢æŸ¥è¯¢: {report.search_queries if hasattr(report, 'search_queries') else 'æœªè®°å½•'}
å†…å®¹é•¿åº¦: {report.content_length if hasattr(report, 'content_length') else 'æœªè®°å½•'} å­—ç¬¦"""
        
        # ç¡®è®¤æ¶ˆæ¯
        confirm_text = "âœ… å·²ç”Ÿæˆã€æ¶ˆè´¹è€…è°ƒç ”æŠ¥å‘Šã€‘ï¼Œè¯·åœ¨å·¦ä¾§å·¥ä½œå°æŸ¥çœ‹ã€‚è¾“å…¥ã€Œç»§ç»­ã€è¿›å…¥å†…æ¶µè®¾è®¡é˜¶æ®µã€‚"
        
        return {
            **state,
            "agent_output": report_content,  # JSONæ ¼å¼ï¼Œä¿å­˜åˆ°å­—æ®µ
            "display_output": confirm_text,   # å¯¹è¯åŒºæ˜¾ç¤ºç¡®è®¤æ¶ˆæ¯
            "full_prompt": full_prompt,
            "messages": [AIMessage(content=confirm_text)],
            # æ³¨æ„ï¼šä¸è‡ªåŠ¨è®¾ç½® completedï¼Œéœ€è¦ç”¨æˆ·ç¡®è®¤
            "phase_status": {**state.get("phase_status", {}), "research": "in_progress"},
            "current_phase": "research",
            "is_producing": True,  # è°ƒç ”ç»“æœåº”ä¿å­˜åˆ°å·¥ä½œå°
            "waiting_for_human": True,  # å…³é”®ï¼šç­‰å¾…ç”¨æˆ·ç¡®è®¤ï¼Œä¸è‡ªåŠ¨æ¨è¿›
            # token ä¿¡æ¯
            "tokens_in": getattr(report, 'tokens_in', 0),
            "tokens_out": getattr(report, 'tokens_out', 0),
            "duration_ms": getattr(report, 'duration_ms', 0),
            "cost": getattr(report, 'cost', 0.0),
        }
        
    except Exception as e:
        return {
            **state,
            "error": str(e),
            "agent_output": f"è°ƒç ”å¤±è´¥: {str(e)}",
            "is_producing": False,
            "messages": [AIMessage(content=f"è°ƒç ”å¤±è´¥: {str(e)}")],
        }


async def design_inner_node(state: ContentProductionState) -> ContentProductionState:
    """
    å†…æ¶µè®¾è®¡èŠ‚ç‚¹
    
    ç”Ÿæˆ3ä¸ªæ–¹æ¡ˆä¾›ç”¨æˆ·é€‰æ‹©
    
    è¾“å‡ºç»“æ„åŒ–çš„JSONæ–¹æ¡ˆï¼ŒåŒ…å«ï¼š
    - 3ä¸ªä¸åŒçš„æ–¹æ¡ˆ
    - æ¯ä¸ªæ–¹æ¡ˆåŒ…å«å­—æ®µåˆ—è¡¨å’Œä¾èµ–å…³ç³»
    - ç”¨æˆ·é€‰æ‹©åæ‰è¿›å…¥å†…æ¶µç”Ÿäº§
    """
    import json
    import re
    
    creator_profile = state.get("creator_profile", "")
    project_id = state.get("project_id", "")
    
    # ä»å­—æ®µè·å–ä¾èµ–å†…å®¹ï¼ˆæ„å›¾åˆ†æã€æ¶ˆè´¹è€…è°ƒç ”ï¼‰
    deps = get_intent_and_research(project_id)
    intent_str = normalize_intent(deps.get("intent", ""))
    personas_str = normalize_consumer_personas(deps.get("research", ""))
    
    # è·å–ç”¨æˆ·çš„è®¾è®¡åå¥½ï¼ˆå¦‚æœæœ‰ï¼‰
    design_preference = ""
    try:
        from core.database import get_session_maker
        SessionLocal = get_session_maker()
        with SessionLocal() as _db:
            pref_field = _db.query(ProjectField).filter(
                ProjectField.project_id == project_id,
                ProjectField.phase == "design_inner",
                ProjectField.name == "è®¾è®¡åå¥½",
            ).first()
            if pref_field and pref_field.content:
                design_preference = pref_field.content.strip()
    except Exception as e:
        print(f"[design_inner] è¯»å–è®¾è®¡åå¥½å¤±è´¥: {e}")
    
    # æ„å»ºä¾èµ–å†…å®¹ï¼ˆä½œä¸ºå‚è€ƒå†…å®¹æ³¨å…¥ï¼Œè€Œéå…¨å±€ä¸Šä¸‹æ–‡ï¼‰
    field_context = ""
    if intent_str:
        field_context += f"## é¡¹ç›®æ„å›¾\n{intent_str}\n\n"
    if personas_str:
        field_context += f"## ç›®æ ‡ç”¨æˆ·ç”»åƒ\n{personas_str}\n\n"
    if design_preference:
        field_context += f"## ç”¨æˆ·çš„è®¾è®¡åå¥½\n{design_preference}\n\n"
    
    context = PromptContext(
        golden_context=GoldenContext(
            creator_profile=creator_profile,
        ),
        phase_context=prompt_engine.PHASE_PROMPTS["design_inner"],
        field_context=field_context.strip(),  # ä¾èµ–å†…å®¹ä½œä¸ºå‚è€ƒ
    )
    
    system_prompt = context.to_system_prompt()
    
    # æ³¨æ„ï¼šGolden Contextï¼ˆé¡¹ç›®æ„å›¾ã€æ¶ˆè´¹è€…ç”»åƒï¼‰å·²ç»åœ¨ system_prompt ä¸­
    # user_prompt åªéœ€è¦å‘å‡ºä»»åŠ¡æŒ‡ä»¤
    pref_instruction = ""
    if design_preference:
        pref_instruction = f"\n\nç”¨æˆ·ç‰¹åˆ«è¦æ±‚ï¼š{design_preference}\nè¯·ç¡®ä¿3ä¸ªæ–¹æ¡ˆéƒ½å……åˆ†è€ƒè™‘ç”¨æˆ·çš„è®¾è®¡åå¥½ã€‚"
    
    user_prompt = f"""è¯·åŸºäºä¸Šè¿°é¡¹ç›®æ„å›¾å’Œæ¶ˆè´¹è€…ç”»åƒï¼Œè®¾è®¡3ä¸ªå†…å®¹ç”Ÿäº§æ–¹æ¡ˆã€‚{pref_instruction}

è¯·è¾“å‡ºä¸¥æ ¼çš„JSONæ ¼å¼ï¼ˆä¸è¦æ·»åŠ ```jsonæ ‡è®°ï¼‰ã€‚"""
    
    messages = [
        ChatMessage(role="system", content=system_prompt),
        ChatMessage(role="user", content=user_prompt),
    ]
    
    response = await ai_client.async_chat(messages, temperature=0.7)
    raw_output = response.content
    
    # è§£æJSONï¼ˆå¤„ç†å¯èƒ½çš„markdownä»£ç å—ï¼‰
    json_content = raw_output
    json_match = re.search(r'```(?:json)?\s*([\s\S]*?)\s*```', raw_output)
    if json_match:
        json_content = json_match.group(1)
    
    try:
        proposals_data = json.loads(json_content)
    except json.JSONDecodeError:
        # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›é”™è¯¯æç¤º
        proposals_data = {
            "proposals": [],
            "error": "AIè¾“å‡ºæ ¼å¼é”™è¯¯ï¼Œè¯·é‡è¯•"
        }
    
    # æ„å»ºç”¨äºå‰ç«¯å±•ç¤ºçš„å†…å®¹
    # æ–¹æ¡ˆå­˜å‚¨ä¸ºç»“æ„åŒ–JSONï¼Œå‰ç«¯è´Ÿè´£æ¸²æŸ“
    output_content = json.dumps(proposals_data, ensure_ascii=False, indent=2)
    
    # æ„å»ºç®€æ´çš„å±•ç¤ºæ–‡æœ¬ï¼ˆç»™å³ä¾§å¯¹è¯åŒºï¼‰
    display_text = "âœ… å·²ç”Ÿæˆ3ä¸ªå†…æ¶µè®¾è®¡æ–¹æ¡ˆï¼Œè¯·åœ¨å·¦ä¾§å·¥ä½œå°æŸ¥çœ‹å¹¶é€‰æ‹©ã€‚\n\n"
    if "proposals" in proposals_data and proposals_data["proposals"]:
        for i, p in enumerate(proposals_data["proposals"][:3], 1):
            display_text += f"**æ–¹æ¡ˆ{i}**ï¼š{p.get('name', 'æœªå‘½å')}\n"
            display_text += f"  {p.get('description', '')[:100]}\n\n"
    
    full_prompt = f"[System]\n{system_prompt}\n\n[User]\n{user_prompt}"
    
    return {
        **state,
        "agent_output": output_content,  # JSONæ ¼å¼ï¼Œå­˜å…¥å­—æ®µ
        "display_output": display_text,   # ç®€æ´å±•ç¤ºï¼Œç»™å¯¹è¯åŒº
        "messages": [AIMessage(content=display_text)],
        "phase_status": {**state.get("phase_status", {}), "design_inner": "in_progress"},  # ç­‰å¾…ç”¨æˆ·é€‰æ‹©
        "current_phase": "design_inner",
        "is_producing": True,
        "waiting_for_human": True,  # éœ€è¦ç”¨æˆ·é€‰æ‹©æ–¹æ¡ˆ
        "tokens_in": response.tokens_in,
        "tokens_out": response.tokens_out,
        "duration_ms": response.duration_ms,
        "cost": response.cost,
        "full_prompt": full_prompt,
    }


async def produce_inner_node(state: ContentProductionState) -> ContentProductionState:
    """
    å†…æ¶µç”Ÿäº§èŠ‚ç‚¹
    
    æ ¹æ®è®¾è®¡æ–¹æ¡ˆç”Ÿäº§å†…å®¹
    """
    creator_profile = state.get("creator_profile", "")
    project_id = state.get("project_id", "")
    user_input = state.get("user_input", "è¯·ç”Ÿäº§å†…æ¶µå†…å®¹ã€‚")
    
    # ä»å­—æ®µè·å–ä¾èµ–å†…å®¹ï¼ˆæ„å›¾åˆ†æã€æ¶ˆè´¹è€…è°ƒç ”ï¼‰
    deps = get_intent_and_research(project_id)
    intent_str = normalize_intent(deps.get("intent", ""))
    personas_str = normalize_consumer_personas(deps.get("research", ""))
    
    field_context = ""
    if intent_str:
        field_context += f"## é¡¹ç›®æ„å›¾\n{intent_str}\n\n"
    if personas_str:
        field_context += f"## ç›®æ ‡ç”¨æˆ·ç”»åƒ\n{personas_str}\n\n"
    
    context = PromptContext(
        golden_context=GoldenContext(
            creator_profile=creator_profile,
        ),
        phase_context=prompt_engine.PHASE_PROMPTS["produce_inner"],
        field_context=field_context.strip(),
    )
    
    system_prompt = context.to_system_prompt()
    
    messages = [
        ChatMessage(role="system", content=system_prompt),
        ChatMessage(role="user", content=user_input),
    ]
    
    response = await ai_client.async_chat(messages, temperature=0.7)
    
    confirm_text = "âœ… å·²ç”Ÿæˆã€å†…æ¶µç”Ÿäº§ã€‘ï¼Œè¯·åœ¨å·¦ä¾§å·¥ä½œå°æŸ¥çœ‹ã€‚è¾“å…¥ã€Œç»§ç»­ã€è¿›å…¥å¤–å»¶è®¾è®¡é˜¶æ®µã€‚"
    
    # æ„å»ºå®Œæ•´æç¤ºè¯ç”¨äºæ—¥å¿—
    full_prompt = f"[System]\n{system_prompt}\n\n[User]\n{user_input}"
    
    return {
        **state,
        "agent_output": response.content,
        "display_output": confirm_text,
        "full_prompt": full_prompt,  # æ·»åŠ æ—¥å¿—è®°å½•
        "messages": [AIMessage(content=confirm_text)],
        # æ³¨æ„ï¼šä¸è‡ªåŠ¨è®¾ç½® completedï¼Œéœ€è¦ç”¨æˆ·ç¡®è®¤
        "phase_status": {**state.get("phase_status", {}), "produce_inner": "in_progress"},
        "current_phase": "produce_inner",
        "is_producing": True,
        "waiting_for_human": True,  # ç­‰å¾…ç”¨æˆ·ç¡®è®¤
        "tokens_in": response.tokens_in,
        "tokens_out": response.tokens_out,
        "duration_ms": response.duration_ms,
        "cost": response.cost,
    }


async def design_outer_node(state: ContentProductionState) -> ContentProductionState:
    """
    å¤–å»¶è®¾è®¡èŠ‚ç‚¹
    
    ç±»ä¼¼å†…æ¶µè®¾è®¡ï¼Œç”Ÿæˆå¤šä¸ªæ¸ é“æ–¹æ¡ˆä¾›ç”¨æˆ·é€‰æ‹©ã€‚
    æ¯ä¸ªæ¸ é“æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„å­—æ®µï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©/å¢åˆ æ¸ é“ã€‚
    """
    creator_profile = state.get("creator_profile", "")
    project_id = state.get("project_id", "")
    
    # ä»å­—æ®µè·å–ä¾èµ–å†…å®¹
    deps = get_intent_and_research(project_id)
    intent_str = normalize_intent(deps.get("intent", ""))
    personas_str = normalize_consumer_personas(deps.get("research", ""))
    
    field_context = ""
    if intent_str:
        field_context += f"## é¡¹ç›®æ„å›¾\n{intent_str}\n\n"
    if personas_str:
        field_context += f"## ç›®æ ‡ç”¨æˆ·ç”»åƒ\n{personas_str}\n\n"
    
    # å¤–å»¶è®¾è®¡çš„ç³»ç»Ÿæç¤ºè¯
    system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†…å®¹ä¼ æ’­ç­–ç•¥å¸ˆã€‚

{creator_profile}

{field_context}

ä½ çš„ä»»åŠ¡æ˜¯ä¸ºè¿™ä¸ªå†…å®¹é¡¹ç›®æ¨èé€‚åˆçš„ä¼ æ’­æ¸ é“ã€‚"""

    # ç”¨æˆ·æç¤ºè¯ï¼šç”Ÿæˆæ¸ é“æ–¹æ¡ˆ
    user_prompt = """è¯·æ¨è5-8ä¸ªé€‚åˆçš„ä¼ æ’­æ¸ é“ï¼Œæ¯ä¸ªæ¸ é“åŒ…å«ï¼š
1. æ¸ é“åç§°ï¼ˆå¦‚ï¼šå°çº¢ä¹¦ã€æŠ–éŸ³ã€å…¬ä¼—å·ã€çŸ¥ä¹ã€Bç«™ç­‰ï¼‰
2. é€‚åˆåŸå› ï¼ˆ1-2å¥è¯ï¼Œä¸ºä»€ä¹ˆè¿™ä¸ªé¡¹ç›®é€‚åˆè¿™ä¸ªæ¸ é“ï¼‰
3. å†…å®¹å½¢å¼å»ºè®®ï¼ˆè¯¥æ¸ é“é€‚åˆçš„å†…å®¹å½¢å¼ï¼Œå¦‚ï¼šçŸ­è§†é¢‘ã€å›¾æ–‡ã€é•¿æ–‡ç­‰ï¼‰

è¯·è¾“å‡ºä¸¥æ ¼çš„JSONæ ¼å¼ï¼ˆä¸è¦æ·»åŠ ```jsonæ ‡è®°ï¼‰ï¼š
{
    "channels": [
        {
            "id": "channel_1",
            "name": "æ¸ é“åç§°",
            "reason": "ä¸ºä»€ä¹ˆé€‚åˆï¼ˆ1-2å¥è¯ï¼‰",
            "content_form": "å»ºè®®çš„å†…å®¹å½¢å¼",
            "priority": "high/medium/low"
        }
    ],
    "summary": "ä¸€å¥è¯æ¦‚æ‹¬ä¼ æ’­ç­–ç•¥æ–¹å‘"
}"""

    messages = [
        ChatMessage(role="system", content=system_prompt),
        ChatMessage(role="user", content=user_prompt),
    ]
    
    response = await ai_client.async_chat(messages, temperature=0.7)
    raw_output = response.content
    
    # è§£æJSON
    json_content = raw_output
    json_match = re.search(r'```(?:json)?\s*([\s\S]*?)\s*```', raw_output)
    if json_match:
        json_content = json_match.group(1)
    
    try:
        channels_data = json.loads(json_content)
    except json.JSONDecodeError:
        channels_data = {
            "channels": [],
            "error": "AIè¾“å‡ºæ ¼å¼é”™è¯¯ï¼Œè¯·é‡è¯•"
        }
    
    # å­˜å‚¨ç»“æ„åŒ–JSON
    output_content = json.dumps(channels_data, ensure_ascii=False, indent=2)
    
    # æ„å»ºç®€æ´çš„å±•ç¤ºæ–‡æœ¬
    display_text = "âœ… å·²ç”Ÿæˆæ¸ é“æ–¹æ¡ˆï¼Œè¯·åœ¨å·¦ä¾§å·¥ä½œå°æŸ¥çœ‹å¹¶é€‰æ‹©è¦ä½¿ç”¨çš„æ¸ é“ã€‚\n\n"
    if "channels" in channels_data and channels_data["channels"]:
        for ch in channels_data["channels"][:8]:
            priority_icon = "ğŸ”´" if ch.get("priority") == "high" else "ğŸŸ¡" if ch.get("priority") == "medium" else "âšª"
            display_text += f"{priority_icon} **{ch.get('name', 'æœªå‘½å')}**ï¼š{ch.get('reason', '')[:50]}\n"
    if channels_data.get("summary"):
        display_text += f"\nğŸ“Œ {channels_data['summary']}"
    
    full_prompt = f"[System]\n{system_prompt}\n\n[User]\n{user_prompt}"
    
    return {
        **state,
        "agent_output": output_content,  # JSONæ ¼å¼ï¼Œå­˜å…¥å­—æ®µ
        "display_output": display_text,   # ç®€æ´å±•ç¤º
        "messages": [AIMessage(content=display_text)],
        "phase_status": {**state.get("phase_status", {}), "design_outer": "in_progress"},
        "current_phase": "design_outer",
        "is_producing": True,
        "waiting_for_human": True,  # éœ€è¦ç”¨æˆ·é€‰æ‹©æ¸ é“
        "tokens_in": response.tokens_in,
        "tokens_out": response.tokens_out,
        "duration_ms": response.duration_ms,
        "cost": response.cost,
        "full_prompt": full_prompt,
    }


async def produce_outer_node(state: ContentProductionState) -> ContentProductionState:
    """å¤–å»¶ç”Ÿäº§èŠ‚ç‚¹"""
    creator_profile = state.get("creator_profile", "")
    project_id = state.get("project_id", "")
    user_input = state.get("user_input", "è¯·ç”Ÿäº§å¤–å»¶å†…å®¹ã€‚")
    
    # ä»å­—æ®µè·å–ä¾èµ–å†…å®¹
    deps = get_intent_and_research(project_id)
    intent_str = normalize_intent(deps.get("intent", ""))
    personas_str = normalize_consumer_personas(deps.get("research", ""))
    
    field_context = ""
    if intent_str:
        field_context += f"## é¡¹ç›®æ„å›¾\n{intent_str}\n\n"
    if personas_str:
        field_context += f"## ç›®æ ‡ç”¨æˆ·ç”»åƒ\n{personas_str}\n\n"
    
    context = PromptContext(
        golden_context=GoldenContext(
            creator_profile=creator_profile,
        ),
        phase_context=prompt_engine.PHASE_PROMPTS["produce_outer"],
        field_context=field_context.strip(),
    )
    
    system_prompt = context.to_system_prompt()
    
    messages = [
        ChatMessage(role="system", content=system_prompt),
        ChatMessage(role="user", content=user_input),
    ]
    
    response = await ai_client.async_chat(messages, temperature=0.7)
    
    confirm_text = "âœ… å·²ç”Ÿæˆã€å¤–å»¶ç”Ÿäº§ã€‘ï¼Œè¯·åœ¨å·¦ä¾§å·¥ä½œå°æŸ¥çœ‹ã€‚è¾“å…¥ã€Œç»§ç»­ã€è¿›å…¥æ¶ˆè´¹è€…æ¨¡æ‹Ÿé˜¶æ®µã€‚"
    full_prompt = f"[System]\n{system_prompt}\n\n[User]\n{user_input}"
    
    return {
        **state,
        "agent_output": response.content,
        "display_output": confirm_text,
        "full_prompt": full_prompt,
        "messages": [AIMessage(content=confirm_text)],
        # æ³¨æ„ï¼šä¸è‡ªåŠ¨è®¾ç½® completedï¼Œéœ€è¦ç”¨æˆ·ç¡®è®¤
        "phase_status": {**state.get("phase_status", {}), "produce_outer": "in_progress"},
        "current_phase": "produce_outer",
        "is_producing": True,
        "waiting_for_human": True,
        "tokens_in": response.tokens_in,
        "tokens_out": response.tokens_out,
        "duration_ms": response.duration_ms,
        "cost": response.cost,
    }


async def evaluate_node(state: ContentProductionState) -> ContentProductionState:
    """è¯„ä¼°èŠ‚ç‚¹"""
    creator_profile = state.get("creator_profile", "")
    project_id = state.get("project_id", "")
    user_input = "è¯·å¯¹é¡¹ç›®è¿›è¡Œå…¨é¢è¯„ä¼°ã€‚"
    
    # ä»å­—æ®µè·å–ä¾èµ–å†…å®¹
    deps = get_intent_and_research(project_id)
    intent_str = normalize_intent(deps.get("intent", ""))
    personas_str = normalize_consumer_personas(deps.get("research", ""))
    
    field_context = ""
    if intent_str:
        field_context += f"## é¡¹ç›®æ„å›¾\n{intent_str}\n\n"
    if personas_str:
        field_context += f"## ç›®æ ‡ç”¨æˆ·ç”»åƒ\n{personas_str}\n\n"
    
    context = PromptContext(
        golden_context=GoldenContext(
            creator_profile=creator_profile,
        ),
        phase_context=prompt_engine.PHASE_PROMPTS["evaluate"],
        field_context=field_context.strip(),
    )
    
    system_prompt = context.to_system_prompt()
    
    messages = [
        ChatMessage(role="system", content=system_prompt),
        ChatMessage(role="user", content=user_input),
    ]
    
    response = await ai_client.async_chat(messages, temperature=0.5)
    
    confirm_text = "âœ… å·²ç”Ÿæˆã€è¯„ä¼°æŠ¥å‘Šã€‘ï¼Œè¯·åœ¨å·¦ä¾§å·¥ä½œå°æŸ¥çœ‹ã€‚å†…å®¹ç”Ÿäº§æµç¨‹å·²å®Œæˆï¼"
    full_prompt = f"[System]\n{system_prompt}\n\n[User]\n{user_input}"
    
    return {
        **state,
        "agent_output": response.content,
        "display_output": confirm_text,
        "full_prompt": full_prompt,
        "messages": [AIMessage(content=confirm_text)],
        # æ³¨æ„ï¼šä¸è‡ªåŠ¨è®¾ç½® completedï¼Œéœ€è¦ç”¨æˆ·ç¡®è®¤
        "phase_status": {**state.get("phase_status", {}), "evaluate": "in_progress"},
        "current_phase": "evaluate",
        "is_producing": True,
        "waiting_for_human": True,  # æµç¨‹ç»“æŸï¼Œç­‰å¾…ç”¨æˆ·
        "tokens_in": response.tokens_in,
        "tokens_out": response.tokens_out,
        "duration_ms": response.duration_ms,
        "cost": response.cost,
    }


# ============== ä¿®æ”¹å’ŒæŸ¥è¯¢èŠ‚ç‚¹ ==============

async def modify_node(state: ContentProductionState) -> ContentProductionState:
    """
    å­—æ®µä¿®æ”¹èŠ‚ç‚¹
    
    æ ¹æ®ç”¨æˆ·æŒ‡ä»¤ä¿®æ”¹å·²æœ‰å­—æ®µå†…å®¹
    """
    creator_profile = state.get("creator_profile", "")
    project_id = state.get("project_id", "")
    target_field = state.get("parsed_target_field", "")
    operation = state.get("parsed_operation", "")
    referenced_contents = state.get("referenced_contents", {})
    
    # è·å–ç›®æ ‡å­—æ®µçš„åŸå§‹å†…å®¹
    original_content = referenced_contents.get(target_field, "")
    user_input = state.get("user_input", "")
    
    # å¦‚æœå¼•ç”¨ä¸­æ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•ç›´æ¥ä»æ•°æ®åº“æŸ¥æ‰¾ï¼ˆæ”¯æŒç”¨æˆ·ç›´æ¥è¾“å…¥å­—æ®µåè€Œä¸ç”¨ @ï¼‰
    if not original_content and target_field and project_id:
        try:
            field_data = get_field_content(project_id, target_field)
            if field_data and field_data.get("content"):
                original_content = field_data["content"]
                referenced_contents[target_field] = original_content
                print(f"[modify_node] é€šè¿‡æ•°æ®åº“ç›´æ¥æŸ¥æ‰¾åˆ°å­—æ®µ: {target_field}")
        except Exception as e:
            print(f"[modify_node] æŸ¥æ‰¾å­—æ®µå¤±è´¥: {e}")
    
    if not original_content:
        # åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„å­—æ®µåï¼ˆåŒæ—¶æœç´¢ ProjectField å’Œ ContentBlockï¼‰
        all_available = list(referenced_contents.keys())
        try:
            from core.database import get_db
            from core.models import ProjectField
            db = next(get_db())
            # æœç´¢ ProjectField
            pf_names = [f.name for f in db.query(ProjectField).filter(
                ProjectField.project_id == project_id
            ).all() if f.content and f.content.strip()]
            # æœç´¢ ContentBlock
            from core.models.content_block import ContentBlock
            cb_names = [b.name for b in db.query(ContentBlock).filter(
                ContentBlock.project_id == project_id,
                ContentBlock.block_type == "field",
                ContentBlock.deleted_at == None,
            ).all() if b.content and b.content.strip()]
            all_available = list(set(pf_names + cb_names))
        except Exception:
            pass
        
        references = state.get("references", [])
        debug_prompt = f"""[modify_node å¤±è´¥è°ƒè¯•ä¿¡æ¯]
ç›®æ ‡å­—æ®µ: {target_field}
ç”¨æˆ·è¾“å…¥: {user_input}
å¼•ç”¨è§£æ: {references}
å¯ç”¨å­—æ®µ: {all_available}

è§£ææ“ä½œ: {operation}
å½“å‰é˜¶æ®µ: {state.get('current_phase', 'unknown')}
"""
        return {
            **state,
            "agent_output": f"æœªæ‰¾åˆ°å­—æ®µã€Œ{target_field}ã€çš„å†…å®¹ï¼Œæ— æ³•ä¿®æ”¹ã€‚\n\nå¯ç”¨å­—æ®µ: {all_available}",
            "is_producing": False,
            "waiting_for_human": False,
            "full_prompt": debug_prompt,
        }
    
    # ä»å­—æ®µè·å–ä¾èµ–å†…å®¹
    deps = get_intent_and_research(project_id)
    intent_str = normalize_intent(deps.get("intent", ""))
    
    # æ£€æµ‹åŸå§‹å†…å®¹æ˜¯å¦æ˜¯ JSONï¼ˆä¾‹å¦‚æ–¹æ¡ˆç»“æ„ï¼‰
    is_json_content = False
    try:
        json_module.loads(original_content)
        is_json_content = True
    except (json_module.JSONDecodeError, TypeError):
        pass
    
    format_hint = ""
    if is_json_content:
        format_hint = "\n5. åŸå§‹å†…å®¹æ˜¯ JSON ç»“æ„ï¼Œè¯·è¾“å‡ºä¿®æ”¹åçš„å®Œæ•´ JSONï¼ˆä¸è¦ç”¨ markdown ä»£ç å—åŒ…è£¹ï¼‰"
    
    # æ„å»ºå‚è€ƒå†…å®¹ï¼ˆå…¶ä»– @ å¼•ç”¨çš„å­—æ®µï¼Œä¸åŒ…æ‹¬ç›®æ ‡å­—æ®µæœ¬èº«ï¼‰
    reference_section = ""
    other_refs = {name: content for name, content in referenced_contents.items()
                  if name != target_field and content}
    if other_refs:
        ref_parts = []
        for name, content in other_refs.items():
            ref_parts.append(f"### å‚è€ƒå­—æ®µã€Œ{name}ã€:\n{content}")
        reference_section = "\n\nå‚è€ƒå†…å®¹ï¼ˆç”¨æˆ·æŒ‡å®šçš„å‚è€ƒææ–™ï¼‰:\n" + "\n\n".join(ref_parts)
    
    # æ„å»ºä¿®æ”¹æç¤ºè¯
    system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†…å®¹ç¼–è¾‘ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„ä¿®æ”¹æŒ‡ä»¤ï¼Œå¯¹åŸå§‹å†…å®¹è¿›è¡Œä¿®æ”¹ã€‚

é¡¹ç›®ä¸Šä¸‹æ–‡:
{creator_profile}
{intent_str}

åŸå§‹å†…å®¹ï¼ˆå­—æ®µåï¼š{target_field}ï¼‰:
{original_content}{reference_section}

è§„åˆ™ï¼š
1. ä¸¥æ ¼æŒ‰ç…§ç”¨æˆ·çš„ä¿®æ”¹æŒ‡ä»¤è¿›è¡Œä¿®æ”¹
2. ä¿æŒå†…å®¹çš„ä¸“ä¸šæ€§å’Œä¸€è‡´æ€§
3. åªè¾“å‡ºä¿®æ”¹åçš„å®Œæ•´å†…å®¹ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Š
4. ä¿æŒåŸæœ‰çš„æ ¼å¼é£æ ¼{format_hint}

Markdown æ ¼å¼ç¡¬æ€§è¦æ±‚ï¼ˆå¦‚è¾“å‡ºåŒ…å«è¡¨æ ¼ï¼Œå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰ï¼š
- è¡¨æ ¼æ¯ä¸€è¡Œçš„åˆ—æ•°ï¼ˆ| çš„æ•°é‡ï¼‰å¿…é¡»ä¸è¡¨å¤´å®Œå…¨ä¸€è‡´ï¼Œç»å¯¹ä¸èƒ½å¤šä¹Ÿä¸èƒ½å°‘
- å¦‚æœä¸€ä¸ªå•å…ƒæ ¼å†…éœ€è¦æ”¾ç½®å¤šæ¡å†…å®¹ï¼Œè¯·åœ¨å•å…ƒæ ¼å†…éƒ¨ä½¿ç”¨ <br> æ¢è¡Œæˆ–æ•°å­—ç¼–å·ï¼ˆ1. 2. 3.ï¼‰ï¼Œä¸è¦ç”¨ | å¢åŠ åˆ—
- è¡¨æ ¼å¿…é¡»æœ‰è¡¨å¤´åˆ†éš”è¡Œï¼ˆå¦‚ | --- | --- | --- |ï¼‰
- è¡¨æ ¼æ¯è¡Œå¿…é¡»ä»¥ | å¼€å¤´ã€ä»¥ | ç»“å°¾
- ç¤ºä¾‹ï¼šåœ¨ä¸€ä¸ªå•å…ƒæ ¼ä¸­æ”¾å¤šæ¡åœºæ™¯çš„æ­£ç¡®å†™æ³•ï¼š| åœºæ™¯1æè¿° <br> åœºæ™¯2æè¿° <br> åœºæ™¯3æè¿° |"""

    # å…³é”®ä¿®å¤ï¼šä½¿ç”¨ç”¨æˆ·çœŸå®è¾“å…¥ä½œä¸º user messageï¼Œè€Œéç¡¬ç¼–ç æ¨¡æ¿
    # ä¹‹å‰ "è¯·ä¿®æ”¹ã€Œé€å­—ç¨¿1ã€çš„å†…å®¹" ä¼šä¼ é”™ç›®æ ‡ï¼ˆå–äº† references[0]ï¼‰ï¼Œ
    # ç°åœ¨ç›´æ¥ä¼ ç”¨æˆ·åŸå§‹æŒ‡ä»¤å¦‚ "å‚è€ƒ @é€å­—ç¨¿1 ä¿®æ”¹ @é€å­—ç¨¿2"
    messages = [
        ChatMessage(role="system", content=system_prompt),
        ChatMessage(role="user", content=user_input),
    ]
    
    response = await ai_client.async_chat(messages, temperature=0.5)
    modified_content = response.content
    
    # æ„å»ºå®Œæ•´ prompt ç”¨äºæ—¥å¿—è®°å½•
    full_prompt = f"[System]\n{system_prompt}\n\n[User]\n{user_input}"
    
    # è¿”å›ä¿®æ”¹åçš„å†…å®¹ï¼ˆis_producing=True ä¼šè§¦å‘ä¿å­˜ï¼‰
    return {
        **state,
        "agent_output": modified_content,
        "is_producing": True,  # æ ‡è®°ä¸ºäº§å‡ºæ¨¡å¼ï¼Œè§¦å‘å­—æ®µä¿å­˜
        "waiting_for_human": False,
        "current_phase": state.get("current_phase", "intent"),  # ä¿æŒå½“å‰é˜¶æ®µ
        "tokens_in": response.tokens_in,
        "tokens_out": response.tokens_out,
        "duration_ms": response.duration_ms,
        "cost": response.cost,
        "full_prompt": full_prompt,
        # ç‰¹æ®Šæ ‡è®°ï¼šè¿™æ˜¯ä¿®æ”¹æ“ä½œï¼Œéœ€è¦æ›´æ–°æŒ‡å®šå­—æ®µ
        "modify_target_field": target_field,
    }


async def query_node(state: ContentProductionState) -> ContentProductionState:
    """
    å­—æ®µ/æ¶æ„æŸ¥è¯¢èŠ‚ç‚¹
    
    å›ç­”å…³äºå·²æœ‰å­—æ®µæˆ–é¡¹ç›®æ¶æ„çš„é—®é¢˜
    æ”¯æŒï¼š
    1. å­—æ®µå†…å®¹æŸ¥è¯¢
    2. é¡¹ç›®æ¶æ„æŸ¥è¯¢ï¼ˆé˜¶æ®µåˆ—è¡¨ã€å­—æ®µåˆ—è¡¨ç­‰ï¼‰
    """
    creator_profile = state.get("creator_profile", "")
    project_id = state.get("project_id", "")
    target_field = state.get("parsed_target_field", "")
    operation = state.get("parsed_operation", "")
    referenced_contents = state.get("referenced_contents", {})
    
    # åˆ¤æ–­æ˜¯å¦æ˜¯æ¶æ„çº§åˆ«çš„æŸ¥è¯¢
    arch_keywords = ["é˜¶æ®µ", "å­—æ®µ", "ç›®å½•", "ç»“æ„", "æ¶æ„", "æœ‰å“ªäº›", "åˆ—è¡¨", "è¿›åº¦"]
    is_architecture_query = any(kw in operation for kw in arch_keywords)
    
    # è·å–æ¶æ„ä¿¡æ¯
    arch_info = ""
    if is_architecture_query and project_id:
        try:
            arch = get_project_architecture(project_id)
            if arch:
                arch_info = f"\n\n## é¡¹ç›®æ¶æ„ä¿¡æ¯\n{format_architecture_for_llm(arch)}"
        except Exception as e:
            arch_info = f"\n\n(æ¶æ„ä¿¡æ¯è·å–å¤±è´¥: {str(e)})"
    
    # è·å–ç›®æ ‡å­—æ®µçš„å†…å®¹
    field_content = referenced_contents.get(target_field, "")
    
    # å¦‚æœæ²¡æœ‰å¼•ç”¨ä½†æŒ‡å®šäº†å­—æ®µåï¼Œå°è¯•ç›´æ¥è·å–
    if target_field and not field_content and project_id:
        try:
            field_data = get_field_content(project_id, target_field)
            if field_data:
                field_content = field_data.get("content", "")
                referenced_contents[target_field] = field_content
        except Exception:
            pass
    
    # æ„å»ºæŸ¥è¯¢ä¸Šä¸‹æ–‡
    all_refs = "\n\n".join([
        f"### {name}\n{content}" 
        for name, content in referenced_contents.items()
    ])
    
    # ä»å­—æ®µè·å–ä¾èµ–å†…å®¹
    deps = get_intent_and_research(project_id)
    intent_str = normalize_intent(deps.get("intent", ""))
    
    system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†…å®¹åˆ†æå¸ˆã€‚è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œåˆ†æå’Œè§£é‡Šå·²æœ‰çš„å†…å®¹æˆ–é¡¹ç›®ç»“æ„ã€‚

é¡¹ç›®ä¸Šä¸‹æ–‡:
{creator_profile}
{intent_str}{arch_info}

å¼•ç”¨çš„å­—æ®µå†…å®¹:
{all_refs if all_refs else '(æ— å¼•ç”¨å­—æ®µ)'}

è§„åˆ™ï¼š
1. å‡†ç¡®å›ç­”ç”¨æˆ·çš„é—®é¢˜
2. å¦‚æœé—®çš„æ˜¯æ¶æ„/é˜¶æ®µ/å­—æ®µåˆ—è¡¨ï¼Œæ ¹æ®æ¶æ„ä¿¡æ¯å›ç­”
3. å¦‚æœæ˜¯æ€»ç»“ï¼Œç®€æ˜æ‰¼è¦
4. å¦‚æœæ˜¯è§£é‡Šï¼Œé€šä¿—æ˜“æ‡‚
5. å¦‚æœæ˜¯åˆ†æï¼Œé€»è¾‘æ¸…æ™°"""

    messages = [
        ChatMessage(role="system", content=system_prompt),
        ChatMessage(role="user", content=operation),
    ]
    
    response = await ai_client.async_chat(messages, temperature=0.7)
    
    # æ„å»ºå®Œæ•´ prompt ç”¨äºæ—¥å¿—è®°å½•
    full_prompt = f"[System]\n{system_prompt}\n\n[User]\n{operation}"
    
    return {
        **state,
        "agent_output": response.content,
        "is_producing": False,  # æŸ¥è¯¢ä¸äº§å‡ºæ–°å†…å®¹
        "waiting_for_human": False,
        "tokens_in": response.tokens_in,
        "tokens_out": response.tokens_out,
        "duration_ms": response.duration_ms,
        "cost": response.cost,
        "full_prompt": full_prompt,
    }


async def chat_node(state: ContentProductionState) -> ContentProductionState:
    """
    è‡ªç”±å¯¹è¯èŠ‚ç‚¹ï¼ˆé‡æ„ç‰ˆï¼‰
    
    å¤„ç†è‡ªç”±å¯¹è¯ï¼Œæ³¨å…¥ @ å¼•ç”¨çš„å†…å®¹
    """
    creator_profile = state.get("creator_profile", "")
    project_id = state.get("project_id", "")
    history = state.get("messages", [])
    referenced_contents = state.get("referenced_contents", {})
    references = state.get("references", [])
    
    # æ„å»ºå¼•ç”¨å†…å®¹ä¸Šä¸‹æ–‡
    ref_context = ""
    if references and referenced_contents:
        ref_parts = []
        for name in references:
            content = referenced_contents.get(name, "")
            if content:
                ref_parts.append(f"### {name}\n{content}")
        if ref_parts:
            ref_context = f"""
ç”¨æˆ·å¼•ç”¨çš„å­—æ®µå†…å®¹:
{chr(10).join(ref_parts)}
"""
    
    # ä»å­—æ®µè·å–ä¾èµ–å†…å®¹
    deps = get_intent_and_research(project_id)
    intent_str = normalize_intent(deps.get("intent", ""))
    
    # æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡
    current_phase = state.get('current_phase', 'intent')
    system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½çš„å†…å®¹ç”Ÿäº§ Agentã€‚å½“å‰æ­£åœ¨å¸®åŠ©ç”¨æˆ·è¿›è¡Œå†…å®¹ç”Ÿäº§é¡¹ç›®ã€‚

## æˆ‘çš„èƒ½åŠ›
1. **æ„å›¾åˆ†æ**: é€šè¿‡ 3 ä¸ªé—®é¢˜å¸®ä½ æ˜ç¡®å†…å®¹ç›®æ ‡ï¼ˆåšä»€ä¹ˆã€ç»™è°çœ‹ã€æœŸæœ›è¡ŒåŠ¨ï¼‰
2. **æ¶ˆè´¹è€…è°ƒç ”**: ä½¿ç”¨ DeepResearch æ·±åº¦åˆ†æç›®æ ‡ç”¨æˆ·ç”»åƒå’Œç—›ç‚¹
3. **å†…æ¶µè®¾è®¡**: è§„åˆ’å†…å®¹çš„æ ¸å¿ƒç»“æ„å’Œå­—æ®µ
4. **å†…æ¶µç”Ÿäº§**: æ ¹æ®è®¾è®¡æ–¹æ¡ˆç”Ÿæˆå…·ä½“å†…å®¹
5. **å¤–å»¶è®¾è®¡/ç”Ÿäº§**: äº§å‡ºè¥é”€å’Œè§¦è¾¾ç›¸å…³å†…å®¹
6. **æ¶ˆè´¹è€…æ¨¡æ‹Ÿ**: æ¨¡æ‹Ÿç”¨æˆ·ä¸å†…å®¹äº¤äº’ï¼Œè·å–åé¦ˆ
7. **è¯„ä¼°**: å¤šç»´åº¦è¯„ä¼°å†…å®¹è´¨é‡

## å¯ç”¨å·¥å…·
- **æ¶æ„æ“ä½œ**: æ·»åŠ /åˆ é™¤/ç§»åŠ¨å­—æ®µå’Œé˜¶æ®µ
- **å¤§çº²ç”Ÿæˆ**: å¸®ä½ è§„åˆ’å†…å®¹ç»“æ„
- **äººç‰©ç®¡ç†**: ç”Ÿæˆå’Œç®¡ç†ç”¨æˆ·ç”»åƒ
- **æŠ€èƒ½åº”ç”¨**: ä½¿ç”¨ä¸åŒå†™ä½œé£æ ¼

## é¡¹ç›®ä¸Šä¸‹æ–‡
{creator_profile or 'ï¼ˆæš‚æ— åˆ›ä½œè€…ä¿¡æ¯ï¼‰'}
{intent_str or 'ï¼ˆæš‚æ— é¡¹ç›®æ„å›¾ï¼‰'}

å½“å‰é˜¶æ®µ: {current_phase}{ref_context}

è¯·å‹å¥½åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœç”¨æˆ·è¯¢é—®ä½ èƒ½åšä»€ä¹ˆï¼Œè¯·ç®€æ´ä»‹ç»ä½ çš„èƒ½åŠ›ã€‚"""
    
    messages = [ChatMessage(role="system", content=system_prompt)]
    
    # æ·»åŠ å†å²æ¶ˆæ¯
    for msg in history[-10:]:  # åªå–æœ€è¿‘10æ¡
        if isinstance(msg, HumanMessage):
            messages.append(ChatMessage(role="user", content=msg.content))
        elif isinstance(msg, AIMessage):
            messages.append(ChatMessage(role="assistant", content=msg.content))
    
    # æ·»åŠ å½“å‰è¾“å…¥
    user_input = state.get("user_input", "")
    messages.append(ChatMessage(role="user", content=user_input))
    
    response = await ai_client.async_chat(messages, temperature=0.7)
    
    # æ„å»ºå®Œæ•´çš„ prompt ç”¨äºæ—¥å¿—è®°å½•
    full_prompt = f"[System]\n{system_prompt}\n\n"
    for msg in messages[1:-1]:  # è·³è¿‡ system å’Œæœ€åçš„ user
        full_prompt += f"[{msg.role}]\n{msg.content}\n\n"
    full_prompt += f"[User]\n{user_input}"
    
    return {
        **state,
        "agent_output": response.content,
        "messages": [AIMessage(content=response.content)],
        "is_producing": False,  # å¯¹è¯æ¨¡å¼ä¸ä¿å­˜ä¸ºå­—æ®µ
        "full_prompt": full_prompt,  # è®°å½•å®Œæ•´ prompt
        "tokens_in": response.tokens_in,
        "tokens_out": response.tokens_out,
        "duration_ms": response.duration_ms,
        "cost": response.cost,
    }


# ============== æ£€æŸ¥ç‚¹ ==============

def check_autonomy(state: ContentProductionState) -> str:
    """
    æ£€æŸ¥æ˜¯å¦éœ€è¦äººå·¥ç¡®è®¤
    
    è¯­ä¹‰ï¼šautonomy[phase] = True è¡¨ç¤ºè¯¥é˜¶æ®µè‡ªåŠ¨æ‰§è¡Œï¼ˆä¸éœ€è¦ç¡®è®¤ï¼‰
          autonomy[phase] = False è¡¨ç¤ºè¯¥é˜¶æ®µéœ€è¦äººå·¥ç¡®è®¤
    
    Returns:
        "wait_human" æˆ– "continue"
    """
    current_phase = state.get("current_phase", "intent")
    autonomy = state.get("autonomy_settings", {})
    
    # è‡ªä¸»æƒï¼šTrue = è‡ªåŠ¨æ‰§è¡Œï¼ŒFalse = éœ€è¦ç¡®è®¤
    # é»˜è®¤éœ€è¦ç¡®è®¤ï¼ˆautonomy ä¸º False æˆ–ä¸å­˜åœ¨æ—¶ï¼‰
    is_autonomous = autonomy.get(current_phase, False)
    
    if is_autonomous:
        # è‡ªåŠ¨æ‰§è¡Œï¼Œä¸éœ€è¦ç­‰å¾…
        return "continue"
    else:
        # éœ€è¦äººå·¥ç¡®è®¤
        return "wait_human"


def get_next_phase(state: ContentProductionState) -> str:
    """è·å–ä¸‹ä¸€ä¸ªé˜¶æ®µ"""
    current = state.get("current_phase", "intent")
    order = state.get("phase_order", PROJECT_PHASES)
    
    try:
        idx = order.index(current)
        if idx < len(order) - 1:
            return order[idx + 1]
    except ValueError:
        pass
    
    return "end"


# ============== å·¥å…·èŠ‚ç‚¹ ==============

async def generate_field_node(state: ContentProductionState) -> ContentProductionState:
    """
    ç”Ÿæˆå­—æ®µå·¥å…·èŠ‚ç‚¹
    
    æ ¹æ®ç”¨æˆ·è¯·æ±‚ç”ŸæˆæŒ‡å®šå­—æ®µ
    """
    from core.tools.field_generator import generate_field
    
    creator_profile = state.get("creator_profile", "")
    project_id = state.get("project_id", "")
    user_input = state.get("user_input", "")
    current_phase = state.get("current_phase", "intent")
    
    # ä»ç”¨æˆ·è¾“å…¥ä¸­æå–è¦ç”Ÿæˆçš„å­—æ®µå
    # ç®€å•å®ç°ï¼šç›´æ¥ä½¿ç”¨å½“å‰é˜¶æ®µçš„é»˜è®¤å­—æ®µ
    field_names = {
        "intent": "é¡¹ç›®æ„å›¾",
        "research": "æ¶ˆè´¹è€…è°ƒç ”æŠ¥å‘Š",
        "design_inner": "å†…æ¶µè®¾è®¡æ–¹æ¡ˆ",
        "produce_inner": "å†…æ¶µç”Ÿäº§å†…å®¹",
        "design_outer": "å¤–å»¶è®¾è®¡æ–¹æ¡ˆ",
        "produce_outer": "å¤–å»¶ç”Ÿäº§å†…å®¹",
        "evaluate": "é¡¹ç›®è¯„ä¼°æŠ¥å‘Š",
    }
    field_name = field_names.get(current_phase, "å†…å®¹")
    
    # ä»å­—æ®µè·å–ä¾èµ–å†…å®¹
    deps = get_intent_and_research(project_id)
    intent_str = normalize_intent(deps.get("intent", ""))
    personas_str = normalize_consumer_personas(deps.get("research", ""))
    
    field_context = ""
    if intent_str:
        field_context += f"## é¡¹ç›®æ„å›¾\n{intent_str}\n\n"
    if personas_str:
        field_context += f"## ç›®æ ‡ç”¨æˆ·ç”»åƒ\n{personas_str}\n\n"
    
    context = PromptContext(
        golden_context=GoldenContext(
            creator_profile=creator_profile,
        ),
        phase_context=prompt_engine.PHASE_PROMPTS.get(current_phase, ""),
        field_context=field_context.strip(),
    )
    
    messages = [
        ChatMessage(role="system", content=context.to_system_prompt()),
        ChatMessage(role="user", content=f"è¯·ç”Ÿæˆ{field_name}ã€‚ç”¨æˆ·è¡¥å……è¯´æ˜ï¼š{user_input}"),
    ]
    
    response = await ai_client.async_chat(messages, temperature=0.7)
    
    return {
        **state,
        "agent_output": response.content,
        "messages": [AIMessage(content=response.content)],
        "is_producing": True,  # ç”Ÿæˆå­—æ®µä¸€å®šæ˜¯äº§å‡ºæ¨¡å¼
    }


async def read_field_node(state: ContentProductionState) -> ContentProductionState:
    """
    è¯»å–å­—æ®µå·¥å…·èŠ‚ç‚¹
    
    è¯»å–å·²ç”Ÿæˆçš„å­—æ®µå†…å®¹ï¼ˆ@å¼•ç”¨ï¼‰
    """
    from core.database import SessionLocal
    from core.models import ProjectField
    
    user_input = state.get("user_input", "")
    project_id = state.get("project_id", "")
    
    # è§£æ@å¼•ç”¨
    import re
    refs = re.findall(r'@([\w\u4e00-\u9fff]+)', user_input)
    
    db = SessionLocal()
    try:
        results = []
        for ref in refs:
            field = db.query(ProjectField).filter(
                ProjectField.project_id == project_id,
                ProjectField.name == ref,
            ).first()
            if field:
                results.append(f"## {field.name}\n{field.content}")
            else:
                results.append(f"## {ref}\nï¼ˆæœªæ‰¾åˆ°è¯¥å­—æ®µï¼‰")
        
        output = "\n\n".join(results) if results else "æ²¡æœ‰æ‰¾åˆ°å¼•ç”¨çš„å­—æ®µã€‚"
    finally:
        db.close()
    
    return {
        **state,
        "agent_output": output,
        "messages": [AIMessage(content=output)],
        "is_producing": False,  # æŸ¥è¯¢ä¸ä¿å­˜å­—æ®µ
    }


async def update_field_node(state: ContentProductionState) -> ContentProductionState:
    """
    æ›´æ–°å­—æ®µå·¥å…·èŠ‚ç‚¹
    
    ä¿®æ”¹å·²æœ‰å­—æ®µå†…å®¹
    """
    creator_profile = state.get("creator_profile", "")
    user_input = state.get("user_input", "")
    
    # åˆ›ä½œè€…ç‰¹è´¨ä½œä¸ºå”¯ä¸€çš„å…¨å±€ä¸Šä¸‹æ–‡
    context = PromptContext(
        golden_context=GoldenContext(
            creator_profile=creator_profile,
        ),
    )
    
    messages = [
        ChatMessage(role="system", content=f"""{context.to_system_prompt()}

ä½ æ˜¯ä¸€ä¸ªå†…å®¹ç¼–è¾‘åŠ©æ‰‹ã€‚ç”¨æˆ·æƒ³ä¿®æ”¹å·²æœ‰å†…å®¹ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„è¦æ±‚ï¼Œè¾“å‡ºä¿®æ”¹åçš„å®Œæ•´å†…å®¹ã€‚"""),
        ChatMessage(role="user", content=user_input),
    ]
    
    response = await ai_client.async_chat(messages, temperature=0.7)
    
    return {
        **state,
        "agent_output": response.content,
        "messages": [AIMessage(content=response.content)],
        "is_producing": True,  # ä¿®æ”¹åéœ€è¦ä¿å­˜
    }


async def tool_node(state: ContentProductionState) -> ContentProductionState:
    """
    ç»Ÿä¸€å·¥å…·è°ƒç”¨èŠ‚ç‚¹
    
    ä½¿ç”¨ LLM è§£æç”¨æˆ·æ„å›¾ï¼Œæå–å·¥å…·å‚æ•°å¹¶æ‰§è¡Œ
    """
    from core.tools.architecture_writer import (
        add_phase, remove_phase, add_field, remove_field, 
        update_field, move_field, reorder_phases
    )
    from core.tools.outline_generator import generate_outline, apply_outline_to_project
    from core.tools.persona_manager import (
        list_personas, create_persona, generate_persona,
        select_persona, delete_persona, update_persona
    )
    from core.tools.skill_manager import list_skills, apply_skill, get_skill, create_skill
    
    intent_type = state.get("parsed_intent_type", "")
    project_id = state.get("project_id", "")
    user_input = state.get("user_input", "")
    operation = state.get("parsed_operation", user_input)
    
    output = ""
    full_prompt = f"[Tool Node]\nå·¥å…·ç±»å‹: {intent_type}\nç”¨æˆ·è¾“å…¥: {user_input}\n"
    
    try:
        if intent_type == "tool_architecture":
            output = await _llm_handle_architecture(project_id, user_input, state)
        
        elif intent_type == "tool_outline":
            output = await _llm_handle_outline(project_id, user_input, state)
        
        elif intent_type == "tool_persona":
            output = await _llm_handle_persona(project_id, user_input, state)
        
        elif intent_type == "tool_skill":
            output = await _llm_handle_skill(user_input, state)
        
        else:
            output = f"æœªçŸ¥å·¥å…·ç±»å‹: {intent_type}"
        
        full_prompt += f"\nå·¥å…·è¾“å‡º:\n{output}"
            
    except Exception as e:
        import traceback
        error_trace = traceback.format_exc()
        output = f"å·¥å…·æ‰§è¡Œé”™è¯¯: {str(e)}\n\n```\n{error_trace}\n```"
        full_prompt += f"\né”™è¯¯:\n{error_trace}"
    
    return {
        **state,
        "agent_output": output,
        "messages": [AIMessage(content=output)],
        "is_producing": False,
        "full_prompt": full_prompt,  # è®°å½•å®Œæ•´çš„å·¥å…·è°ƒç”¨ä¿¡æ¯
    }


async def _llm_handle_architecture(project_id: str, user_input: str, state: dict) -> str:
    """ä½¿ç”¨ LLM è§£ææ¶æ„æ“ä½œæ„å›¾å¹¶æ‰§è¡Œ"""
    from core.tools.architecture_writer import (
        add_phase, remove_phase, add_field, remove_field,
        update_field, move_field
    )
    from core.tools.architecture_reader import get_project_architecture, format_architecture_for_llm
    
    # è·å–å½“å‰æ¶æ„
    arch = get_project_architecture(project_id)
    arch_context = format_architecture_for_llm(arch) if arch else "æ— æ³•è·å–æ¶æ„ä¿¡æ¯"
    
    # è®© LLM è§£ææ“ä½œ
    parse_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ¶æ„æ“ä½œè§£æå™¨ã€‚æ ¹æ®ç”¨æˆ·è¯·æ±‚ï¼Œè§£æå‡ºå…·ä½“çš„æ¶æ„æ“ä½œã€‚

## å½“å‰é¡¹ç›®æ¶æ„
{arch_context}

## å¯ç”¨æ“ä½œ
1. add_phase: æ·»åŠ é˜¶æ®µ (å‚æ•°: phase_code, display_name, position)
2. remove_phase: åˆ é™¤é˜¶æ®µ (å‚æ•°: phase_name)
3. add_field: æ·»åŠ å­—æ®µ (å‚æ•°: phase, name, ai_prompt)
4. remove_field: åˆ é™¤å­—æ®µ (å‚æ•°: field_name)
5. move_field: ç§»åŠ¨å­—æ®µ (å‚æ•°: field_name, target_phase)
6. update_field: æ›´æ–°å­—æ®µ (å‚æ•°: field_name, updates)

## é˜¶æ®µä»£ç æ˜ å°„ï¼ˆé‡è¦ï¼ç”¨æˆ·è¯´ä¸­æ–‡åï¼Œä½ è¦è½¬æˆä»£ç ï¼‰
- intent = æ„å›¾åˆ†æ
- research = æ¶ˆè´¹è€…è°ƒç ”
- design_inner = å†…æ¶µè®¾è®¡
- produce_inner = å†…æ¶µç”Ÿäº§ / ç”Ÿäº§å†…æ¶µ / å†…å®¹ç”Ÿäº§
- design_outer = å¤–å»¶è®¾è®¡
- produce_outer = å¤–å»¶ç”Ÿäº§
- simulate = æ¶ˆè´¹è€…æ¨¡æ‹Ÿ
- evaluate = è¯„ä¼°

## ç”¨æˆ·è¯·æ±‚
{user_input}

## è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰
{{"action": "æ“ä½œå", "params": {{å‚æ•°å¯¹è±¡}}}}

## è§£æè§„åˆ™
1. ç”¨æˆ·è¯´"åœ¨XXé˜¶æ®µè¡¥å……/æ·»åŠ å­—æ®µ"â†’ action="add_field", params.phase=é˜¶æ®µä»£ç 
2. å¦‚æœç”¨æˆ·æ²¡è¯´å…·ä½“å­—æ®µåï¼Œæ ¹æ®ä¸Šä¸‹æ–‡ç”Ÿæˆä¸€ä¸ªåˆç†çš„å­—æ®µå
3. å¦‚æœå®åœ¨æ— æ³•æ¨æ–­å­—æ®µåï¼Œä½¿ç”¨"å¾…å‘½åå­—æ®µ"

åªè¾“å‡º JSONï¼Œä¸è¦è§£é‡Šã€‚å¦‚æœæ— æ³•è§£æï¼Œè¾“å‡º {{"action": "unknown", "reason": "åŸå› "}}"""

    messages = [
        ChatMessage(role="system", content=parse_prompt),
        ChatMessage(role="user", content="è¯·è§£ææ“ä½œ"),
    ]
    
    response = await ai_client.async_chat(messages, temperature=0.2)
    
    import json
    try:
        content = response.content.strip()
        if "```" in content:
            content = content.split("```")[1]
            if content.startswith("json"):
                content = content[4:]
        result = json.loads(content)
        action = result.get("action", "unknown")
        params = result.get("params", {})
        print(f"[_llm_handle_architecture] è§£æç»“æœ: action={action}, params={params}")
    except Exception as parse_error:
        debug_info = f"""[æ¶æ„æ“ä½œè§£æå¤±è´¥]
ç”¨æˆ·è¾“å…¥: {user_input}
LLMåŸå§‹å“åº”: {response.content[:500]}
è§£æé”™è¯¯: {str(parse_error)}"""
        print(debug_info)
        return f"æ— æ³•è§£ææ“ä½œè¯·æ±‚ã€‚è¯·å°è¯•æ›´æ˜ç¡®çš„æè¿°ï¼Œä¾‹å¦‚ï¼š\n- åœ¨å†…æ¶µç”Ÿäº§é˜¶æ®µæ·»åŠ ä¸€ä¸ªã€Œæ ¸å¿ƒè®ºç‚¹ã€å­—æ®µ\n- åˆ é™¤ã€Œæ„å›¾åˆ†æã€é˜¶æ®µ\n\nï¼ˆè°ƒè¯•: LLMå“åº”ä¸º {response.content[:100]}...ï¼‰"
    
    # æ‰§è¡Œæ“ä½œ
    if action == "add_phase":
        op_result = add_phase(
            project_id,
            params.get("phase_code", f"custom_{params.get('display_name', 'new')}"),
            params.get("display_name", "æ–°é˜¶æ®µ"),
            params.get("position")
        )
        return op_result.message
    
    elif action == "remove_phase":
        op_result = remove_phase(project_id, params.get("phase_name", ""))
        return op_result.message
    
    elif action == "add_field":
        phase = params.get("phase", "produce_inner")
        name = params.get("name", "æ–°å­—æ®µ")
        ai_prompt = params.get("ai_prompt", "")
        print(f"[add_field] phase={phase}, name={name}")
        op_result = add_field(
            project_id,
            phase,
            name,
            ai_prompt,
        )
        if op_result.success:
            return f"âœ… {op_result.message}\n\nå·²åœ¨ã€Œ{phase}ã€é˜¶æ®µæ·»åŠ å­—æ®µã€Œ{name}ã€ï¼Œè¯·åˆ·æ–°é¡µé¢æŸ¥çœ‹ã€‚"
        else:
            return f"âŒ {op_result.message}\n\nï¼ˆå°è¯•æ·»åŠ : phase={phase}, name={name}ï¼‰"
    
    elif action == "remove_field":
        op_result = remove_field(project_id, params.get("field_name", ""))
        return op_result.message
    
    elif action == "move_field":
        op_result = move_field(
            project_id,
            params.get("field_name", ""),
            params.get("target_phase", ""),
        )
        return op_result.message
    
    elif action == "update_field":
        op_result = update_field(
            project_id,
            params.get("field_name", ""),
            params.get("updates", {}),
        )
        return op_result.message
    
    else:
        reason = result.get("reason", "æ— æ³•è¯†åˆ«çš„æ“ä½œ")
        return f"æ— æ³•æ‰§è¡Œæ“ä½œ: {reason}\n\næ”¯æŒçš„æ“ä½œï¼šæ·»åŠ /åˆ é™¤é˜¶æ®µã€æ·»åŠ /åˆ é™¤/ç§»åŠ¨å­—æ®µ"


async def _llm_handle_outline(project_id: str, user_input: str, state: dict) -> str:
    """ä½¿ç”¨ LLM å¤„ç†å¤§çº²ç›¸å…³è¯·æ±‚"""
    from core.tools.outline_generator import generate_outline
    
    # è§£æç”¨æˆ·æ˜¯è¦ç”Ÿæˆè¿˜æ˜¯ç¡®è®¤å¤§çº²
    if "ç¡®è®¤" in user_input:
        # TODO: åº”ç”¨å¤§çº²åˆ°é¡¹ç›®
        return "å¤§çº²ç¡®è®¤åŠŸèƒ½å¼€å‘ä¸­ã€‚ç›®å‰è¯·æ‰‹åŠ¨åœ¨å·¦ä¾§æ·»åŠ å­—æ®µã€‚"
    
    # ç”Ÿæˆå¤§çº²
    content_type = ""
    if "è¯¾ç¨‹" in user_input:
        content_type = "è¯¾ç¨‹"
    elif "æ–‡ç« " in user_input:
        content_type = "æ–‡ç« "
    elif "è§†é¢‘" in user_input:
        content_type = "è§†é¢‘è„šæœ¬"
    
    outline = await generate_outline(project_id, content_type, user_input)
    
    if "å¤±è´¥" in outline.title:
        return f"## å¤§çº²ç”Ÿæˆé‡åˆ°é—®é¢˜\n\n{outline.summary}"
    
    output = f"## ğŸ“‹ {outline.title}\n\n{outline.summary}\n\n"
    for i, node in enumerate(outline.nodes, 1):
        output += f"### {i}. {node.name}\n{node.description}\n"
        if node.ai_prompt:
            output += f"*AIæç¤ºï¼š{node.ai_prompt[:80]}...*\n"
        if node.children:
            for j, child in enumerate(node.children, 1):
                output += f"   {i}.{j} **{child.name}**: {child.description}\n"
        output += "\n"
    
    output += f"\n---\n*é¢„è®¡ç”Ÿæˆ {outline.estimated_fields} ä¸ªå­—æ®µ*"
    return output


async def _llm_handle_persona(project_id: str, user_input: str, state: dict) -> str:
    """ä½¿ç”¨ LLM è§£æäººç‰©æ“ä½œæ„å›¾å¹¶æ‰§è¡Œ"""
    from core.tools.persona_manager import (
        list_personas, generate_persona, select_persona, 
        delete_persona, update_persona
    )
    
    # è®© LLM è§£ææ“ä½œ
    parse_prompt = f"""ä½ æ˜¯ä¸€ä¸ªäººç‰©æ“ä½œè§£æå™¨ã€‚æ ¹æ®ç”¨æˆ·è¯·æ±‚ï¼Œè§£æå‡ºå…·ä½“çš„æ“ä½œã€‚

## å¯ç”¨æ“ä½œ
1. list: åˆ—å‡ºæ‰€æœ‰äººç‰©
2. generate: ç”Ÿæˆæ–°äººç‰© (å‚æ•°: hint - ç”Ÿæˆæç¤º)
3. select: é€‰ä¸­äººç‰©ç”¨äºæ¨¡æ‹Ÿ (å‚æ•°: name)
4. deselect: å–æ¶ˆé€‰ä¸­ (å‚æ•°: name)
5. delete: åˆ é™¤äººç‰© (å‚æ•°: name)

## ç”¨æˆ·è¯·æ±‚
{user_input}

## è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰
{{"action": "æ“ä½œå", "params": {{å‚æ•°å¯¹è±¡}}}}

åªè¾“å‡º JSONã€‚"""

    messages = [
        ChatMessage(role="system", content=parse_prompt),
        ChatMessage(role="user", content="è¯·è§£ææ“ä½œ"),
    ]
    
    response = await ai_client.async_chat(messages, temperature=0.2)
    
    import json
    try:
        content = response.content.strip()
        if "```" in content:
            content = content.split("```")[1]
            if content.startswith("json"):
                content = content[4:]
        result = json.loads(content)
        action = result.get("action", "list")
        params = result.get("params", {})
    except:
        action = "list"
        params = {}
    
    if action == "list":
        op_result = list_personas(project_id)
        if op_result.personas:
            output = "## ğŸ‘¥ å½“å‰äººç‰©åˆ—è¡¨\n\n"
            for p in op_result.personas:
                status = "âœ… å·²é€‰ä¸­" if p.selected else "â¬œ æœªé€‰ä¸­"
                output += f"### {p.name} {status}\n"
                output += f"**èƒŒæ™¯**: {p.background[:100]}...\n"
                if p.pain_points:
                    output += f"**ç—›ç‚¹**: {', '.join(p.pain_points[:3])}\n"
                output += "\n"
            return output
        return "æš‚æ— äººç‰©ã€‚è¯·å…ˆå®Œæˆæ¶ˆè´¹è€…è°ƒç ”ï¼Œæˆ–è¯´ã€Œç”Ÿæˆä¸€ä¸ªæŠ€æœ¯èƒŒæ™¯çš„ç”¨æˆ·ã€æ¥åˆ›å»ºäººç‰©ã€‚"
    
    elif action == "generate":
        hint = params.get("hint", user_input)
        op_result = await generate_persona(project_id, hint)
        if op_result.success and op_result.persona:
            p = op_result.persona
            # ä½¿ç”¨ LLM ç”Ÿæˆè‡ªç„¶çš„å›å¤
            summary_prompt = f"""ç”¨ä¸€å¥è¯è‡ªç„¶åœ°å‘Šè¯‰ç”¨æˆ·ä½ å®Œæˆäº†ä»€ä¹ˆä»»åŠ¡ã€‚

ç”¨æˆ·è¯·æ±‚: {user_input}
å®Œæˆç»“æœ: æˆåŠŸåˆ›å»ºäº†äººç‰©ã€Œ{p.name}ã€ï¼Œ{p.basic_info.get('age', '')}å²ï¼Œ{p.basic_info.get('occupation', '')}

è¦æ±‚ï¼š
- ç”¨å‹å¥½ã€å£è¯­åŒ–çš„è¯­æ°”
- æåˆ°äººç‰©å§“åå’Œå…³é”®ç‰¹å¾
- æé†’ç”¨æˆ·å¯ä»¥åœ¨å·¦ä¾§å·¥ä½œå°æŸ¥çœ‹
- ä¸è¦ç”¨æ¨¡æ¿åŒ–çš„æ ¼å¼ï¼Œè¦è‡ªç„¶

åªè¾“å‡ºè¿™ä¸€å¥è¯ã€‚"""
            
            summary_messages = [ChatMessage(role="user", content=summary_prompt)]
            summary_response = await ai_client.async_chat(summary_messages, temperature=0.8)
            natural_reply = summary_response.content.strip()
            
            # é™„å¸¦è¯¦ç»†ä¿¡æ¯
            detail = f"\n\n---\n**{p.name}**\n- èƒŒæ™¯: {p.background[:150]}...\n- æ ¸å¿ƒç—›ç‚¹: {', '.join(p.pain_points[:2])}"
            
            return natural_reply + detail
        return op_result.message
    
    elif action == "select":
        op_result = select_persona(project_id, params.get("name", ""), True)
        return op_result.message
    
    elif action == "deselect":
        op_result = select_persona(project_id, params.get("name", ""), False)
        return op_result.message
    
    elif action == "delete":
        op_result = delete_persona(project_id, params.get("name", ""))
        return op_result.message
    
    return "æ— æ³•è¯†åˆ«äººç‰©æ“ä½œã€‚è¯•è¯•è¯´ï¼šã€ŒæŸ¥çœ‹äººç‰©ã€ã€Œç”Ÿæˆä¸€ä¸ªç¨‹åºå‘˜ç”¨æˆ·ã€ã€Œé€‰ä¸­ææ˜ã€"


async def _llm_handle_skill(user_input: str, state: dict) -> str:
    """ä½¿ç”¨ LLM è§£ææŠ€èƒ½æ“ä½œæ„å›¾å¹¶æ‰§è¡Œ"""
    from core.tools.skill_manager import list_skills, apply_skill, get_skill
    
    # è®© LLM è§£ææ“ä½œ
    parse_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæŠ€èƒ½æ“ä½œè§£æå™¨ã€‚æ ¹æ®ç”¨æˆ·è¯·æ±‚ï¼Œè§£æå‡ºå…·ä½“çš„æ“ä½œã€‚

## å¯ç”¨æ“ä½œ
1. list: åˆ—å‡ºæ‰€æœ‰æŠ€èƒ½
2. apply: åº”ç”¨æŠ€èƒ½ (å‚æ•°: skill_name, task - è¦æ‰§è¡Œçš„ä»»åŠ¡æè¿°)
3. get: æŸ¥çœ‹æŠ€èƒ½è¯¦æƒ… (å‚æ•°: skill_name)

## é¢„ç½®æŠ€èƒ½
- ä¸“ä¸šæ–‡æ¡ˆ: ç”Ÿæˆä¸“ä¸šã€æƒå¨çš„æ–‡æ¡ˆ
- æ•…äº‹åŒ–è¡¨è¾¾: å°†å†…å®¹è½¬åŒ–ä¸ºæ•…äº‹
- å†…å®¹ç®€åŒ–: ç®€åŒ–å¤æ‚å†…å®¹
- æ‰¹åˆ¤æ€§åˆ†æ: æ‰¹åˆ¤æ€§åˆ†æå†…å®¹

## ç”¨æˆ·è¯·æ±‚
{user_input}

## è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰
{{"action": "æ“ä½œå", "params": {{å‚æ•°å¯¹è±¡}}}}

åªè¾“å‡º JSONã€‚"""

    messages = [
        ChatMessage(role="system", content=parse_prompt),
        ChatMessage(role="user", content="è¯·è§£ææ“ä½œ"),
    ]
    
    response = await ai_client.async_chat(messages, temperature=0.2)
    
    import json
    try:
        content = response.content.strip()
        if "```" in content:
            content = content.split("```")[1]
            if content.startswith("json"):
                content = content[4:]
        result = json.loads(content)
        action = result.get("action", "list")
        params = result.get("params", {})
    except:
        action = "list"
        params = {}
    
    if action == "list":
        op_result = list_skills()
        if op_result.skills:
            output = "## ğŸ› ï¸ å¯ç”¨æŠ€èƒ½åˆ—è¡¨\n\n"
            for s in op_result.skills:
                tag = "ğŸ”§ ç³»ç»Ÿ" if s.is_system else "ğŸ‘¤ è‡ªå®šä¹‰"
                output += f"- **{s.name}** ({tag})\n  {s.description}\n\n"
            output += "\n*ä½¿ç”¨æ–¹å¼ï¼šç”¨ã€Œä¸“ä¸šæ–‡æ¡ˆã€å¸®æˆ‘å†™ä¸€æ®µå…³äºXXçš„ä»‹ç»*"
            return output
        return "æš‚æ— å¯ç”¨æŠ€èƒ½"
    
    elif action == "apply":
        skill_name = params.get("skill_name", "")
        task = params.get("task", user_input)
        
        if not skill_name:
            return "è¯·æŒ‡å®šè¦ä½¿ç”¨çš„æŠ€èƒ½ï¼Œä¾‹å¦‚ï¼šç”¨ã€Œä¸“ä¸šæ–‡æ¡ˆã€å†™ä¸€æ®µäº§å“ä»‹ç»"
        
        op_result = await apply_skill(skill_name, {"task": task, "content": task})
        if op_result.success:
            return f"## ğŸ¯ æŠ€èƒ½ã€Œ{skill_name}ã€è¾“å‡º\n\n{op_result.output}"
        return op_result.message
    
    elif action == "get":
        skill_name = params.get("skill_name", "")
        op_result = get_skill(skill_name)
        if op_result.success and op_result.skill:
            s = op_result.skill
            return f"## ğŸ“– æŠ€èƒ½è¯¦æƒ…: {s.name}\n\n{s.description}\n\n**ç±»åˆ«**: {s.category}\n**ä½¿ç”¨æ¬¡æ•°**: {s.usage_count}\n\n**æç¤ºè¯æ¨¡æ¿**:\n```\n{s.prompt_template}\n```"
        return op_result.message
    
    return "æ— æ³•è¯†åˆ«æŠ€èƒ½æ“ä½œã€‚è¯•è¯•è¯´ï¼šã€Œæœ‰å“ªäº›æŠ€èƒ½ã€ã€Œç”¨ä¸“ä¸šæ–‡æ¡ˆå¸®æˆ‘å†™XXã€"


# ============== è·¯ç”±å‡½æ•° ==============

def route_by_intent(state: ContentProductionState) -> str:
    """æ ¹æ®æ„å›¾è·¯ç”±åˆ°ç›¸åº”èŠ‚ç‚¹"""
    target = state.get("route_target", "chat")
    phase_order = state.get("phase_order", PROJECT_PHASES)
    current_phase = state.get("current_phase", phase_order[0] if phase_order else "intent")
    target_field = state.get("parsed_target_field", "")  # å¯èƒ½åŒ…å«ç›®æ ‡é˜¶æ®µ
    
    # å¦‚æœ current_phase ä¸åœ¨ phase_order ä¸­ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªé˜¶æ®µ
    if current_phase not in phase_order and phase_order:
        current_phase = phase_order[0]
    
    if target == "phase_current":
        # ä¿æŒåœ¨å½“å‰é˜¶æ®µ
        return f"phase_{current_phase}"
    elif target == "advance_phase":
        # æ¨è¿›é˜¶æ®µï¼šå¯èƒ½æŒ‡å®šäº†ç›®æ ‡é˜¶æ®µï¼Œä¹Ÿå¯èƒ½æ˜¯ä¸‹ä¸€é˜¶æ®µ
        # å…ˆæ£€æŸ¥æ˜¯å¦æŒ‡å®šäº†å…·ä½“é˜¶æ®µï¼ˆå¦‚ "è¿›å…¥å¤–å»¶è®¾è®¡é˜¶æ®µ"ï¼‰
        if target_field:
            # é˜¶æ®µåç§°æ˜ å°„
            phase_name_map = {
                "intent": "intent", "æ„å›¾åˆ†æ": "intent", "æ„å›¾": "intent",
                "research": "research", "æ¶ˆè´¹è€…è°ƒç ”": "research", "è°ƒç ”": "research",
                "design_inner": "design_inner", "å†…æ¶µè®¾è®¡": "design_inner",
                "produce_inner": "produce_inner", "å†…æ¶µç”Ÿäº§": "produce_inner",
                "design_outer": "design_outer", "å¤–å»¶è®¾è®¡": "design_outer",
                "produce_outer": "produce_outer", "å¤–å»¶ç”Ÿäº§": "produce_outer",
                "simulate": "evaluate", "æ¶ˆè´¹è€…æ¨¡æ‹Ÿ": "evaluate", "æ¨¡æ‹Ÿ": "evaluate",
                "evaluate": "evaluate", "è¯„ä¼°": "evaluate",
            }
            target_phase = phase_name_map.get(target_field.lower().strip(), "")
            if target_phase and target_phase in phase_order:
                print(f"[route_by_intent] è·³è½¬åˆ°æŒ‡å®šé˜¶æ®µ: {target_phase}")
                return f"phase_{target_phase}"
        
        # æ²¡æœ‰æŒ‡å®šç›®æ ‡é˜¶æ®µï¼Œæ¨è¿›åˆ°ä¸‹ä¸€é˜¶æ®µ
        try:
            idx = phase_order.index(current_phase)
            if idx < len(phase_order) - 1:
                next_phase = phase_order[idx + 1]
                print(f"[route_by_intent] æ¨è¿›åˆ°ä¸‹ä¸€é˜¶æ®µ: {next_phase}")
                return f"phase_{next_phase}"
        except ValueError:
            pass
        return f"phase_{current_phase}"
    elif target == "research":
        return "research"
    elif target == "generate":
        return "generate_field"
    elif target == "modify":
        # è·¯ç”±åˆ°æ–°çš„ modify_nodeï¼ˆå¤„ç† @ å¼•ç”¨å­—æ®µä¿®æ”¹ï¼‰
        return "modify"
    elif target == "query":
        # è·¯ç”±åˆ°æ–°çš„ query_nodeï¼ˆå¤„ç† @ å¼•ç”¨å­—æ®µæŸ¥è¯¢ï¼‰
        return "query"
    elif target == "evaluate":
        return "phase_evaluate"
    elif target == "tool" or target in ("tool_architecture", "tool_outline", "tool_persona", "tool_skill"):
        # è·¯ç”±åˆ°å·¥å…·èŠ‚ç‚¹
        return "tool"
    else:
        # chat å’Œå…¶ä»– -> é€šç”¨å¯¹è¯èŠ‚ç‚¹
        return "chat"


def route_after_phase(state: ContentProductionState) -> str:
    """é˜¶æ®µå®Œæˆåçš„è·¯ç”±"""
    # ===== å…ˆæ£€æŸ¥æ˜¯å¦æœ‰å¾…å¤„ç†çš„æ„å›¾ =====
    pending = state.get("pending_intents", [])
    if pending:
        print(f"[route_after_phase] æ£€æµ‹åˆ° {len(pending)} ä¸ªå¾…å¤„ç†æ„å›¾ï¼Œç»§ç»­æ‰§è¡Œ")
        return "continue_pending"
    
    # ===== æ ¸å¿ƒæ£€æŸ¥ï¼šå¦‚æœ is_producing=Falseï¼ˆå¯¹è¯æ¨¡å¼ï¼‰ï¼Œç»“æŸæœ¬è½® =====
    # æ„å›¾åˆ†æç­‰éœ€è¦å¤šè½®å¯¹è¯çš„é˜¶æ®µï¼Œä¸ä¼šè‡ªåŠ¨æ¨è¿›
    is_producing = state.get("is_producing", False)
    if not is_producing:
        return END  # å¯¹è¯æ¨¡å¼ï¼šç»“æŸæœ¬è½®ï¼Œç­‰å¾…ç”¨æˆ·ä¸‹ä¸€æ¡æ¶ˆæ¯
    
    # ===== æ£€æŸ¥æ˜¯å¦æ˜ç¡®è¦æ±‚ç­‰å¾…äººå·¥ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰ =====
    if state.get("waiting_for_human", False):
        return END  # èŠ‚ç‚¹æ˜ç¡®è¦æ±‚ç­‰å¾…ç”¨æˆ·ç¡®è®¤ï¼Œç»“æŸæœ¬è½®
    
    # äº§å‡ºæ¨¡å¼ï¼šæ£€æŸ¥è‡ªä¸»æƒå†³å®šæ˜¯å¦ç­‰å¾…äººå·¥ç¡®è®¤
    if check_autonomy(state) == "wait_human":
        return "wait_human"
    else:
        next_phase = get_next_phase(state)
        if next_phase == "end":
            return END
        return f"phase_{next_phase}"


def route_after_tool(state: ContentProductionState) -> str:
    """
    å·¥å…·æ‰§è¡Œå®Œæˆåçš„è·¯ç”±
    
    æ£€æŸ¥æ˜¯å¦æœ‰å¾…å¤„ç†çš„æ„å›¾ï¼ˆå¤šæ„å›¾é¡ºåºæ‰§è¡Œï¼‰
    """
    pending = state.get("pending_intents", [])
    
    if pending:
        print(f"[route_after_tool] æ£€æµ‹åˆ° {len(pending)} ä¸ªå¾…å¤„ç†æ„å›¾ï¼Œç»§ç»­æ‰§è¡Œ")
        return "continue_pending"
    else:
        return END


async def continue_pending_node(state: ContentProductionState) -> ContentProductionState:
    """
    å¤„ç†å¾…å¤„ç†æ„å›¾çš„èŠ‚ç‚¹
    
    ä» pending_intents ä¸­å–å‡ºç¬¬ä¸€ä¸ªï¼Œè®¾ç½®ä¸ºå½“å‰æ„å›¾ï¼Œ
    ç„¶åé‡æ–°è·¯ç”±
    """
    pending = state.get("pending_intents", [])
    
    if not pending:
        return {**state, "route_target": "chat"}
    
    # å–å‡ºç¬¬ä¸€ä¸ªå¾…å¤„ç†æ„å›¾
    next_intent = pending[0]
    remaining = pending[1:]
    
    intent_type = next_intent.get("type", "chat")
    target = next_intent.get("target", "")
    operation = next_intent.get("operation", "")
    
    print(f"[continue_pending] å¤„ç†ä¸‹ä¸€ä¸ªæ„å›¾: type={intent_type}, target={target}")
    
    # ç¡®å®šè·¯ç”±ç›®æ ‡
    if intent_type.startswith("tool_"):
        route_target = intent_type
    elif intent_type == "phase_action":
        route_target = "phase_current"
    elif intent_type == "advance_phase":
        route_target = "advance_phase"
    else:
        route_target = intent_type
    
    return {
        **state,
        "route_target": route_target,
        "parsed_intent_type": intent_type,
        "parsed_target_field": target,
        "parsed_operation": operation,
        "pending_intents": remaining,  # æ›´æ–°å‰©ä½™å¾…å¤„ç†æ„å›¾
    }


# ============== å›¾æ„å»º ==============

def create_content_production_graph():
    """
    åˆ›å»ºå†…å®¹ç”Ÿäº§Agentå›¾
    
    Returns:
        ç¼–è¯‘åçš„LangGraph
    """
    # åˆ›å»ºå›¾
    graph = StateGraph(ContentProductionState)
    
    # æ·»åŠ èŠ‚ç‚¹
    graph.add_node("router", route_intent)
    graph.add_node("chat", chat_node)
    graph.add_node("research", research_node)
    
    # @ å¼•ç”¨ç›¸å…³èŠ‚ç‚¹ï¼ˆæ–°å¢ï¼‰
    graph.add_node("modify", modify_node)
    graph.add_node("query", query_node)
    
    # é˜¶æ®µèŠ‚ç‚¹
    graph.add_node("phase_intent", intent_analysis_node)
    graph.add_node("phase_research", research_node)
    graph.add_node("phase_design_inner", design_inner_node)
    graph.add_node("phase_produce_inner", produce_inner_node)
    graph.add_node("phase_design_outer", design_outer_node)
    graph.add_node("phase_produce_outer", produce_outer_node)
    graph.add_node("phase_evaluate", evaluate_node)
    
    # å·¥å…·èŠ‚ç‚¹ï¼ˆè®¾è®¡æ–‡æ¡£ä¸­çš„ Toolsï¼‰
    graph.add_node("generate_field", generate_field_node)
    graph.add_node("read_field", read_field_node)
    graph.add_node("update_field", update_field_node)
    graph.add_node("tool", tool_node)  # ç»Ÿä¸€å·¥å…·è°ƒç”¨èŠ‚ç‚¹
    
    # ç­‰å¾…äººå·¥ç¡®è®¤èŠ‚ç‚¹ï¼ˆå®é™…ä¸Šåªæ˜¯è¿”å›çŠ¶æ€ï¼‰
    graph.add_node("wait_human", lambda s: {**s, "waiting_for_human": True})
    
    # å¤šæ„å›¾å¤„ç†èŠ‚ç‚¹ï¼ˆæ–°å¢ï¼‰
    graph.add_node("continue_pending", continue_pending_node)
    
    # è®¾ç½®å…¥å£
    graph.set_entry_point("router")
    
    # æ·»åŠ æ¡ä»¶è¾¹ï¼šä»routeråˆ†å‘
    graph.add_conditional_edges(
        "router",
        route_by_intent,
        {
            # é˜¶æ®µèŠ‚ç‚¹
            "phase_intent": "phase_intent",
            "phase_research": "phase_research",
            "phase_design_inner": "phase_design_inner",
            "phase_produce_inner": "phase_produce_inner",
            "phase_design_outer": "phase_design_outer",
            "phase_produce_outer": "phase_produce_outer",
            "phase_evaluate": "phase_evaluate",
            # @ å¼•ç”¨èŠ‚ç‚¹ï¼ˆæ–°å¢ï¼‰
            "modify": "modify",
            "query": "query",
            # å·¥å…·èŠ‚ç‚¹
            "research": "research",
            "generate_field": "generate_field",
            "read_field": "read_field",
            "update_field": "update_field",
            "tool": "tool",  # ç»Ÿä¸€å·¥å…·èŠ‚ç‚¹
            # å¯¹è¯èŠ‚ç‚¹
            "chat": "chat",
        }
    )
    
    # æ„å»ºé˜¶æ®µè·¯ç”±æ˜ å°„ï¼ˆç”¨äºè‡ªä¸»æƒåˆ¤æ–­ï¼‰
    phase_routing_map = {
        "wait_human": "wait_human", 
        "continue_pending": "continue_pending",  # æ–°å¢ï¼šå¤šæ„å›¾å¤„ç†
        END: END
    }
    for p in PROJECT_PHASES:
        phase_routing_map[f"phase_{p}"] = f"phase_{p}"
    
    # ä»å„é˜¶æ®µèŠ‚ç‚¹åˆ°ç­‰å¾…/ç»§ç»­ï¼ˆæ ¹æ®è‡ªä¸»æƒè®¾ç½®å†³å®šï¼‰
    for phase in PROJECT_PHASES:
        graph.add_conditional_edges(
            f"phase_{phase}",
            route_after_phase,
            phase_routing_map
        )
    
    # ä»researchåˆ°ç­‰å¾…ï¼ˆæ ¹æ®è‡ªä¸»æƒè®¾ç½®ï¼‰
    graph.add_conditional_edges(
        "research",
        route_after_phase,
        phase_routing_map
    )
    
    # å·¥å…·èŠ‚ç‚¹ä½¿ç”¨æ¡ä»¶è¾¹ï¼ˆæ”¯æŒå¤šæ„å›¾é¡ºåºæ‰§è¡Œï¼‰
    tool_routing_map = {"continue_pending": "continue_pending", END: END}
    graph.add_conditional_edges("tool", route_after_tool, tool_routing_map)
    graph.add_conditional_edges("generate_field", route_after_tool, tool_routing_map)
    graph.add_conditional_edges("read_field", route_after_tool, tool_routing_map)
    graph.add_conditional_edges("update_field", route_after_tool, tool_routing_map)
    
    # @ å¼•ç”¨èŠ‚ç‚¹ä½¿ç”¨æ¡ä»¶è¾¹ï¼ˆæ”¯æŒå¤šæ„å›¾ï¼‰
    graph.add_conditional_edges("modify", route_after_tool, tool_routing_map)
    graph.add_conditional_edges("query", route_after_tool, tool_routing_map)
    
    # ä»chatåˆ°ç»“æŸï¼ˆchat ä¸è§¦å‘åç»­æ„å›¾ï¼‰
    graph.add_edge("chat", END)
    
    # ä»ç­‰å¾…åˆ°ç»“æŸï¼ˆç”¨æˆ·ä¼šåœ¨è¿™é‡Œç»§ç»­ï¼‰
    graph.add_edge("wait_human", END)
    
    # continue_pending èŠ‚ç‚¹ -> é‡æ–°è·¯ç”±åˆ°ä¸‹ä¸€ä¸ªæ„å›¾
    graph.add_conditional_edges(
        "continue_pending",
        route_by_intent,
        {
            # é˜¶æ®µèŠ‚ç‚¹
            "phase_intent": "phase_intent",
            "phase_research": "phase_research",
            "phase_design_inner": "phase_design_inner",
            "phase_produce_inner": "phase_produce_inner",
            "phase_design_outer": "phase_design_outer",
            "phase_produce_outer": "phase_produce_outer",
            "phase_evaluate": "phase_evaluate",
            # @ å¼•ç”¨èŠ‚ç‚¹
            "modify": "modify",
            "query": "query",
            # å·¥å…·èŠ‚ç‚¹
            "research": "research",
            "generate_field": "generate_field",
            "read_field": "read_field",
            "update_field": "update_field",
            "tool": "tool",
            # å¯¹è¯èŠ‚ç‚¹
            "chat": "chat",
        }
    )
    
    # ç¼–è¯‘
    memory = MemorySaver()
    compiled = graph.compile(checkpointer=memory)
    
    return compiled


# ============== Agentç±» ==============

class ContentProductionAgent:
    """
    å†…å®¹ç”Ÿäº§Agent
    
    å°è£…LangGraphï¼Œæä¾›ç®€å•çš„æ¥å£
    """
    
    def __init__(self):
        self.graph = create_content_production_graph()
    
    async def run(
        self,
        project_id: str,
        user_input: str,
        current_phase: str = "intent",
        creator_profile: str = "",  # é‡æ„ï¼šç”¨ creator_profile æ›¿ä»£ golden_context
        autonomy_settings: Optional[dict] = None,
        use_deep_research: bool = True,
        thread_id: Optional[str] = None,
        chat_history: Optional[list] = None,
        phase_status: Optional[dict] = None,
        phase_order: Optional[List[str]] = None,  # é¡¹ç›®å®é™…çš„é˜¶æ®µé¡ºåº
        references: Optional[List[str]] = None,  # @ å¼•ç”¨çš„å­—æ®µå
        referenced_contents: Optional[Dict[str, str]] = None,  # å¼•ç”¨å­—æ®µçš„å†…å®¹
    ) -> ContentProductionState:
        """
        è¿è¡ŒAgent
        
        Args:
            project_id: é¡¹ç›®ID
            user_input: ç”¨æˆ·è¾“å…¥
            current_phase: å½“å‰é˜¶æ®µ
            creator_profile: åˆ›ä½œè€…ç‰¹è´¨ï¼ˆå…¨å±€æ³¨å…¥åˆ°æ¯ä¸ª LLM è°ƒç”¨ï¼‰
            autonomy_settings: è‡ªä¸»æƒè®¾ç½®
            use_deep_research: æ˜¯å¦ä½¿ç”¨DeepResearch
            thread_id: çº¿ç¨‹IDï¼ˆç”¨äºçŠ¶æ€æŒä¹…åŒ–ï¼‰
            chat_history: å†å²å¯¹è¯è®°å½•
            phase_status: é¡¹ç›®ç°æœ‰çš„é˜¶æ®µçŠ¶æ€
            phase_order: é¡¹ç›®å®é™…çš„é˜¶æ®µé¡ºåºï¼ˆçµæ´»æ¶æ„å¯èƒ½ä¸åŒäºé»˜è®¤ï¼‰
            references: @ å¼•ç”¨çš„å­—æ®µååˆ—è¡¨
            referenced_contents: å¼•ç”¨å­—æ®µçš„å®é™…å†…å®¹
        
        Returns:
            æœ€ç»ˆçŠ¶æ€
        """
        # ä½¿ç”¨ä¼ å…¥çš„ phase_orderï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤
        actual_phase_order = phase_order if phase_order else PROJECT_PHASES.copy()
        
        # æ„å»ºæ¶ˆæ¯å†å²ï¼ˆå…¼å®¹ä¸¤ç§æ ¼å¼ï¼šå­—å…¸ æˆ– LangChain Message å¯¹è±¡ï¼‰
        messages = []
        if chat_history:
            for msg in chat_history:
                # å¦‚æœæ˜¯ LangChain Message å¯¹è±¡
                if isinstance(msg, (HumanMessage, AIMessage)):
                    messages.append(msg)
                # å¦‚æœæ˜¯å­—å…¸æ ¼å¼
                elif isinstance(msg, dict):
                    if msg.get("role") == "user":
                        messages.append(HumanMessage(content=msg.get("content", "")))
                    elif msg.get("role") == "assistant":
                        messages.append(AIMessage(content=msg.get("content", "")))
        # æ·»åŠ å½“å‰ç”¨æˆ·è¾“å…¥
        messages.append(HumanMessage(content=user_input))
        
        # ä½¿ç”¨ä¼ å…¥çš„ phase_statusï¼Œå¦åˆ™åˆå§‹åŒ–ä¸º pending
        existing_phase_status = phase_status or {p: "pending" for p in actual_phase_order}
        
        initial_state: ContentProductionState = {
            "project_id": project_id,
            "current_phase": current_phase,
            "phase_order": actual_phase_order,  # ä½¿ç”¨é¡¹ç›®å®é™…çš„é˜¶æ®µé¡ºåºï¼
            "phase_status": existing_phase_status,
            "autonomy_settings": autonomy_settings or {p: True for p in actual_phase_order},
            "creator_profile": creator_profile,  # åˆ›ä½œè€…ç‰¹è´¨æ˜¯å”¯ä¸€çš„å…¨å±€ä¸Šä¸‹æ–‡
            "fields": {},
            "messages": messages,  # ä½¿ç”¨å®Œæ•´å†å²
            "user_input": user_input,
            "agent_output": "",
            "waiting_for_human": False,
            "route_target": "",
            "use_deep_research": use_deep_research,
            "is_producing": False,  # é»˜è®¤ä¸æ˜¯äº§å‡ºæ¨¡å¼
            "error": None,
            # API è°ƒç”¨ç»Ÿè®¡åˆå§‹å€¼
            "tokens_in": 0,
            "tokens_out": 0,
            "duration_ms": 0,
            "cost": 0.0,
            # å®Œæ•´ prompt åˆå§‹å€¼
            "full_prompt": "",
            # @ å¼•ç”¨ç›¸å…³
            "references": references or [],
            "referenced_contents": referenced_contents or {},
            # è§£æåçš„æ„å›¾
            "parsed_intent_type": "",
            "parsed_target_field": None,
            "parsed_operation": "",
            # ä¿®æ”¹æ“ä½œç›®æ ‡å­—æ®µ
            "modify_target_field": None,
            # å¤šæ„å›¾æ”¯æŒ
            "pending_intents": [],
        }
        
        config = {"configurable": {"thread_id": thread_id or project_id}}
        
        result = await self.graph.ainvoke(initial_state, config)
        
        return result
    
    async def stream(
        self,
        project_id: str,
        user_input: str,
        **kwargs,
    ):
        """
        æµå¼è¿è¡ŒAgent
        
        Yields:
            çŠ¶æ€æ›´æ–°
        """
        initial_state: ContentProductionState = {
            "project_id": project_id,
            "current_phase": kwargs.get("current_phase", "intent"),
            "phase_order": PROJECT_PHASES.copy(),
            "phase_status": {p: "pending" for p in PROJECT_PHASES},
            "autonomy_settings": kwargs.get("autonomy_settings", {p: True for p in PROJECT_PHASES}),
            "creator_profile": kwargs.get("creator_profile", ""),  # åˆ›ä½œè€…ç‰¹è´¨
            "fields": {},
            "messages": [HumanMessage(content=user_input)],
            "user_input": user_input,
            "agent_output": "",
            "waiting_for_human": False,
            "route_target": "",
            "use_deep_research": kwargs.get("use_deep_research", True),
            "is_producing": False,
            "error": None,
            # å¤šæ„å›¾æ”¯æŒ
            "pending_intents": [],
        }
        
        config = {"configurable": {"thread_id": kwargs.get("thread_id", project_id)}}
        
        async for event in self.graph.astream(initial_state, config):
            yield event


# å•ä¾‹
content_agent = ContentProductionAgent()

