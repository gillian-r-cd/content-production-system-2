# Eval V2 重设计：从第一性原理出发的完整方案

> **创建时间**：2026-02-18
> **状态**：设计方案（待确认后实施）
> **核心改动**：评估体系从"按 Simulator 类型组织"重构为"按评估目标组织"

---

## 一、第一性原理

### 1.1 Eval 2 在系统中的位置

```
Agent 实时伴随（伴随性评估）      Eval 2（总结性评估）
────────────────────           ──────────────────
"我这一步对不对？"               "这个内容能不能出门？"
生产过程中随时给反馈              内容成型后做结构化判定
低成本、高频次                   多视角、可量化、可比较
由 Agent 模式提供                由独立评估系统提供
```

### 1.2 "能不能出门"的四个子问题

| # | 子问题 | 本质 | 关键词 |
|---|--------|------|--------|
| 1 | 内容本身质量如何？ | 对**文本**的直接判定 | 准确、结构、语言、风格 |
| 2 | 目标受众能 get 到吗？ | 对**人-内容关系**的测试 | 理解、共鸣、价值感、行动 |
| 3 | 能卖得动吗？ | 对**交易可能性**的模拟 | 转化、异议、信任、定价 |
| 4 | 有没有盲区？ | 对**已有评估结果**的元分析 | 系统性缺陷、优先级 |

这四个子问题**要求完全不同的认知方法**。不应该用同一套 `simulator_type × interaction_mode` 来配置。

### 1.3 核心设计原则

1. **按评估目标分形态**：用户选的是"我想知道什么"，不是"用什么技术组件"
2. **Task 是容器，Trial 是最小可配置单元**：一个 Task 下可以自由组合不同形态的 Trial
3. **Grader 是一等公民**：可独立执行，不必绑定 Simulator
4. **Probe 可选但推荐**：有 Probe 让评估更聚焦、更可复现；无 Probe 则通用探索
5. **Task 独立执行，Report 聚合展示**：加新 Task 不需要重跑已有 Task
6. **分数只到 Task 级别**：跑完 Task 后内容可能变更，跨 Task 聚合无意义
7. **所有角色统一管理**：消费者、审查角色、自定义角色都在人物画像 ContentBlock 管理
8. **所有提示词可编辑 + AI 可生成**：透明、可控、可迭代
9. **UI 干净极简**：列表 + 弹窗，不内联展开，不花里胡哨
10. **沉淀在 ContentBlock 模板中**：评估配置和报告都是 ContentBlock，可跨项目复用模板

---

## 二、五种评估形态

### 2.1 形态总览

| 形态 | 图标 | 目的 | 需要角色? | 需要 Probe? | 成本 | 特点 |
|------|------|------|----------|------------|------|------|
| **直接判定** | 📊 | 用评分标准直接评内容 | ❌ | 可选 | 低 | 只跑 Grader，最简单 |
| **视角审查** | 👁️ | 从专业视角审读内容 | ✅（审查角色，从画像选） | 可选 | 中 | 角色出定性反馈，可附加 Grader |
| **消费体验** | 👤 | 分块探索，逐块评价与关切匹配度 | ✅（消费者，从画像选） | 可选 | 高 | **多步 LLM 调用**，逐内容块评价 |
| **场景模拟** | 💼 | 双角色对话模拟 | ✅（角色 A + B，从画像选） | 可选 | 高 | 每次发言 ≤50 字 |

> **注意**：综合诊断不再是独立形态，而是 Task 级的"跨 Trial 分析"功能（见 R5）。
> **Probe 全部可选**：有 Probe 让评估更聚焦可复现，无 Probe 则通用评估。

### 2.2 形态 A：直接判定（Assessment）

**认知方法**：`f(content, criteria) → judgment`

像老师阅卷——给内容，给标准，出分数。不模拟任何人，不产生任何交互。

**必需输入**：
- 内容范围（哪些 ContentBlock）
- Grader（评分器，含提示词和维度）

**可选输入**：
- 焦点（如"重点评估第三章"）

**执行流程**：
```
收集目标内容 → 填充 Grader 提示词中的 {content} → 调用 LLM → 解析分数和反馈
```

**典型场景**：
- "帮我检查一下所有内容的写作质量"
- "这个策略方向和项目意图对齐吗？"
- "内容事实准确性如何？"

**默认 Grader**：内容质量评分器、策略对齐评分器（见第六节）

### 2.3 形态 B：视角审查（Review）

**认知方法**：`f(content, perspective, focus?) → professional_feedback`

让一个"角色"用它的视角审读内容，给出开放式反馈。和直接判定的区别：Grader 按标准打分，Reviewer 用专业直觉发现问题——它可能指出你**根本没想到的**维度。

**必需输入**：
- 内容范围
- 审查角色（**从人物画像列表中选择**——策略教练、资深编辑、领域专家等，含角色系统提示词）

**可选输入**：
- 审查焦点/Probe（如"重点看第三章到第五章的衔接逻辑"——可选但推荐）
- 附加 Grader（审查完成后再用 Grader 打分）

**执行流程**：
```
收集目标内容 → 从画像中获取角色提示词 + 注入焦点/Probe → 调用 LLM 获得审查意见
→ [可选] 用附加 Grader 对审查过程评分
```

**典型场景**：
- "让编辑看看结构和文笔"
- "让策略教练评估方向是否正确"
- "让领域专家验证专业内容的准确性"
- "让自定义角色（如竞品分析师）从另一个视角审读"

**默认预装角色**：策略教练、资深编辑、领域专家（见 5.2 画像 Tab）

### 2.4 形态 C：消费体验测试（Experience）—— 分块探索

**认知方法**：`f(content_blocks[], persona, probe?) → per_block_evidence + overall_assessment`

让一个"消费者"带着真实的背景和需求来**逐块探索**内容，每个内容块独立评价"和自己的关切是否一致"。这不是对内容的判定，是对内容的**分块测试**。

**必需输入**：
- 内容范围（**按 ContentBlock 分块**——用户在配置时通过 `target_block_ids` 指定要探索哪些内容块）
- 人物画像（Persona：从画像列表中选择，一个提示词描述该人物的一切）

**可选输入**：
- 测试焦点/Probe（如"关注定价合理性"——让探索更聚焦，但不填则通用探索）
- 附加 Grader（探索结束后评分）

**执行流程**（多次 LLM 调用）：
```
第一步：探索规划（1 次 LLM 调用）
  输入：persona + probe(可选) + 内容块列表（只有标题/摘要）
  输出：探索计划（先看哪个块、为什么、期望找到什么）

第二步：逐块探索（N 次 LLM 调用，每个内容块一次）
  输入：persona + probe(可选) + 该块完整内容 + 前面块的探索记忆
  输出：{
    "关切匹配度": "这部分是否回应了我的关切？怎么回应的？",
    "发现": "有什么有价值的信息？",
    "疑虑": "什么地方让我不确定或失望？",
    "missing": "我期望看到但没看到的内容",
    "feeling": "作为 [persona]，读完这部分的真实感受",
    "score": 1-10
  }

第三步：总结评价（1 次 LLM 调用）
  输入：persona + 所有块的探索结果
  输出：{
    "overall_impression": "总体印象",
    "key_concerns_addressed": ["被回应的关切"],
    "key_concerns_unaddressed": ["未被回应的关切"],
    "would_recommend": true/false,
    "summary": "作为 [persona]，总体评价"
  }

第四步：Grader 评分（可选，M 次 LLM 调用）
  输入：content + 探索结果
  输出：各维度分数 + 改进建议
```

**为什么必须多次调用，不能一次全塞**：
- 单次调用把所有内容塞进去，LLM 对后面的内容关注度会降低
- 分块评价让每个块都得到公平、深入的评估
- 逐块的"关切匹配度"是这个形态最核心的价值——用户能看到"第三章完全没回应定价问题"
- 探索记忆让后续块的评价有上下文（"前面已经看过 XX，现在看这个补充了什么"）

**和已有 `_run_exploration` 的关系**：
- 已有实现（eval_engine.py 345-573 行）是单次 LLM 输出完整探索，升级为真正的多步探索
- 保留探索计划 + 逐步记录的结构
- 新增：按 ContentBlock 分块、逐块独立评价、探索记忆传递

**Probe 示例**（可选但推荐）：
- "关注定价合理性"→ 每个块都会评价"这部分对定价问题有帮助吗"
- "关注实操可行性"→ 每个块都会评价"这部分对上手操作有帮助吗"
- 不填 → 消费者按自身背景和需求自由探索

**默认 Grader**：消费者体验评分器（见第六节）

### 2.5 形态 D：场景模拟（Scenario）—— 双角色对话

**认知方法**：`f(content, role_A, role_B, probe?) → dialogue_evidence`

在一个有目标的**场景**中，两个角色进行对话，测试内容在交互中的表现。最典型的是销售场景，但不限于此。

和形态 C 的区别：C 是**消费者自主逐块探索内容**（单角色），D 是**两个角色围绕内容对话**（双角色）。

**必需输入**：
- 内容范围
- 角色 A（如卖方/服务方/教练，从画像列表选）
- 角色 B（如买方/客户/学员，从画像列表选）

**可选输入**：
- 场景 Probe（如"测试能否回应定价异议"——让对话更聚焦，不填则通用对话）
- 附加 Grader
- 最大对话轮数（默认 5）

**执行流程**：
```
构建角色 A 提示词（注入 content + 角色定位）
→ 构建角色 B 提示词（注入 persona + probe(可选)）
→ 多轮对话（A ↔ B，每次发言 ≤50 字，模拟真实口语对话）
→ [可选] 用附加 Grader 对整个过程评分
```

> **每次发言 ≤50 字**：强制简洁，模拟真实的口语交流。长篇大论不是对话。

**不再有"转化成功/失败"的二元判定**——那是 Grader 该做的事。场景模拟只负责生成对话证据，评分交给 Grader 根据维度灵活打分。

**典型场景**：
- 销售：卖方 vs 有价格顾虑的买方
- 咨询：顾问 vs 需要建议的客户
- 教学：教练 vs 对方法论有疑虑的学员
- 辩论：创作者 vs 严厉的批评者

**默认 Grader**：场景对话评分器（见第六节）

### 2.6 跨 Trial 分析（Task 级功能，非独立形态）

> 综合诊断不再是独立的评估形态，而是 **Task 级的可选功能**。

**认知方法**：`f(task_all_trial_results) → pattern_findings + action_items`

当一个 Task 的所有 Trial 执行完毕后，用户可以**可选地**触发一次跨 Trial 分析：

**输入**：该 Task 的所有 Trial 结果（分数、反馈、对话记录等）
**输出**：
- 反复出现的共性问题（如"3 次消费者测试都在定价部分卡住"）
- 按严重程度排序的**逐条修改建议**
- 每条建议旁有 [🤖 让 Agent 修改] 按钮

**触发方式**：Task 报告块底部的 [🔍 生成跨 Trial 分析] 按钮，非自动触发。

**和旧"综合诊断"的区别**：
- 旧：跨 Task 分析所有结果 → 太重、内容可能已变、聚合无意义
- 新：只分析**一个 Task 内**的所有 Trial → 内容一致、结果可靠、建议可操作

---

## 三、数据模型

### 3.0 ContentBlock 模板集成

系统**已有** 3 个评估专用 ContentBlock（`special_handler`），在评估阶段自动创建：

```
ContentBlock (eval_persona_setup)    ContentBlock (eval_task_config)    ContentBlock (eval_report)
──────────────────────────────       ──────────────────────────────     ──────────────────────────
"人物画像设置"                        "评估任务配置"                      "评估报告"
管理评估中使用的人物画像              配置各形态的评估 Task               展示所有执行记录和诊断
单独一个 ContentBlock                 按形态分组的 Task 列表              按 Task 分组的执行历史
点"生成"= 从调研中提取画像            点"生成"= 生成默认 Task 配置        点"生成"= 执行所有待执行 Task
前端渲染: 画像 Tab                    前端渲染: 配置 Tab                  前端渲染: 报告 Tab
```

**沿用并改造**——不新增 ContentBlock 类型，只重构这 3 个已有类型的数据结构和渲染逻辑。

**模板化的意义**：
- 可以创建"标准评估模板"（如"知识付费标准评估"），包含预设的画像 + Task 组合
- 新项目可从模板一键导入一组评估任务和画像
- 模板中包含 Grader 引用、默认 Persona、默认 Probe 等
- 模板可跨项目复用和分享

**内容树中只有这 3 个块**（执行评估不会增加新块）：
```
项目
  ├── 意图分析 (content block)
  ├── 消费者调研 (content block)
  ├── 内容生产
  │   ├── 第一章 (content block)
  │   └── ...
  ├── 🧑 人物画像设置 (eval_persona_setup)  ← 画像 Tab
  ├── 📋 评估任务配置 (eval_task_config)    ← 配置 Tab
  └── 📊 评估报告 (eval_report)             ← 报告 Tab
```

### 3.1 模型关系

```
Project
  ├── ContentBlock (eval_task_config)  ← 内容树中的锚点
  ├── ContentBlock (eval_report)       ← 内容树中的锚点
  │
  └── EvalTask[]  (评估任务，直接属于项目)
        ├── form_type: assessment | review | experience | scenario
        ├── config: 形态特有配置
        ├── grader_ids: [Grader ID, ...]
        ├── repeat_count: int
        └── EvalTrial[]  (执行记录，存在 DB 中，在报告 Tab 中展示)
              ├── batch_id: 同一次"执行"的所有 trial 共享
              ├── trial_index: 0, 1, 2...
              └── scores, nodes, grader_results, ...

TaskAnalysis  (Task 级跨 Trial 分析结果，可选，存在 DB 中)
  ├── task_id: 关联的 Task
  ├── batch_id: 关联的执行批次
  ├── patterns: 共性模式列表
  ├── suggestions: 逐条修改建议
  └── summary: 一段话总结

Persona  (人物画像，项目级 eval_persona_setup ContentBlock 管理)
  ├── id: str  (preset_coach / preset_editor / preset_expert / p1 / p2 ...)
  ├── name: str
  ├── prompt: str  ← 奥卡姆剃刀：就一个提示词框
  └── source: str  ← preset / research / manual / ai

Grader  (全局/项目级评分器，在后台设置管理)
Simulator  (全局/项目级模拟器，在后台设置管理)
```

### 3.2 EvalTask 新模型

```python
class EvalTask(BaseModel):
    """
    评估任务 — 按评估目标组织的配置单元
    
    form_type 决定了这个 Task 用哪种认知方法来评估内容。
    不同 form_type 需要不同的配置字段。
    """
    __tablename__ = "eval_tasks"

    project_id: str          # 直接属于项目（不再通过 EvalRun 间接关联）
    name: str                # 任务名称（如"内容质量检查"、"张晨-定价疑虑"）
    form_type: str           # assessment | review | experience | scenario
    
    # ---- 通用配置 ----
    target_block_ids: list   # 要评估的内容块 ID（空=全部）
    grader_ids: list         # 关联的 Grader ID 列表（可多个）
    repeat_count: int = 1    # 每次执行生成几个 trial（用于统计聚合）
    order_index: int = 0     # 排序
    
    # ---- 形态特有配置（JSON） ----
    form_config: dict = {
        # assessment: { focus?: str }
        # review:     { persona_id: str, focus?: str }
        #               persona_id 引用画像中的角色（策略教练/编辑/专家/自定义）
        # experience: { persona_id: str, probe?: str,
        #               target_block_ids: list[str] }
        #               分块探索：按 target_block_ids 逐块评价
        # scenario:   { role_a_persona_id: str, role_b_persona_id: str,
        #               probe?: str, max_turns: int }
        #               双角色对话，每次发言 ≤50 字
        #
        # 说明：所有 persona_id 引用 eval_persona_setup ContentBlock 中的画像
        #       probe 全部可选（有则更聚焦，无则通用评估）
        #       如果填了 persona_id，persona_prompt 从画像自动填入
    }
    
    # ---- 执行状态 ----
    status: str = "pending"       # pending | running | completed | stale
    content_hash: str = ""        # 目标内容的 hash，用于检测"过期"
    last_executed_at: datetime?   # 最后执行时间
    
    # ---- 最新聚合结果（冗余存储，快速读取） ----
    latest_scores: dict = {}      # {维度: {mean, std, min, max}}
    latest_overall: float? = None # 聚合总分
    latest_batch_id: str = ""     # 最新 batch
```

### 3.3 EvalTrial 更新

```python
class EvalTrial(BaseModel):
    """
    评估试验 — 一个 EvalTask 的一次执行
    同一 batch_id 下的多个 trial 是同一次"执行"的重复
    """
    __tablename__ = "eval_trials"

    eval_task_id: str        # 属于哪个 Task
    project_id: str          # 冗余，方便查询
    batch_id: str            # 同一次执行的 trial 共享此 ID
    trial_index: int         # 在 batch 中的序号（0-based）
    
    form_type: str           # 冗余存储，方便查询
    
    # ---- 交互数据 ----
    nodes: list = []         # 交互节点（对话记录），直接判定无此数据
    
    # ---- 评分结果 ----
    # 来自 Simulator 内建自评（仅交互式形态有）
    simulator_scores: dict = {}     # {维度: 分数}
    simulator_comments: dict = {}   # {维度: 评语}
    simulator_summary: str = ""
    
    # 来自各 Grader 的独立评分
    grader_results: list = []
    # [{ grader_id, grader_name, scores: {维度: 分数}, 
    #    comments: {维度: 评语}, feedback: str }]
    
    # 场景结果（仅 scenario 形态）
    scenario_outcome: dict = {}
    # { converted: bool, blocked_at_step?: int, blocking_reason?: str }
    
    # ---- 综合分 ----
    overall_score: float?    # 本 trial 的综合分（按第四节公式计算）
    
    # ---- LLM 调用日志 ----
    llm_calls: list = []
    tokens_in: int = 0
    tokens_out: int = 0
    cost: float = 0.0
    
    status: str = "pending"
    error: str = ""
```

### 3.4 TaskAnalysis（替代旧 EvalDiagnosis）

```python
class TaskAnalysis(BaseModel):
    """
    Task 级跨 Trial 分析结果 — 可选功能
    
    当一个 Task 的所有 Trial 执行完后，用户可选择运行跨 Trial 分析。
    分析只在一个 Task 内进行（内容一致），不跨 Task。
    输出共性模式 + 逐条修改建议，每条建议可一键发到 Agent。
    """
    __tablename__ = "task_analyses"
    
    task_id: str        # 关联的 Task
    batch_id: str       # 关联的执行批次
    
    # 输出
    patterns: list = []
    # [{ title: str, frequency: str,
    #    evidence: [str], severity: "high"|"medium"|"low" }]
    
    suggestions: list = []
    # [{ title: str, severity: "high"|"medium"|"low",
    #    detail: str, related_patterns: [str] }]
    
    strengths: list = []   # ["所有 Trial 共同认可的优点"]
    summary: str = ""      # 一段话总结
    
    # LLM 调用
    llm_calls: list = []
    cost: float = 0.0
```

> **和旧 EvalDiagnosis 的区别**：
> - 旧：跨 Task（project_id），输出 readiness + findings → 已废弃
> - 新：单 Task 内（task_id + batch_id），输出 patterns + suggestions → 每条可 → Agent

### 3.5 现有模型处置

| 现有模型 | 处置 | 说明 |
|---------|------|------|
| `EvalRun` | **废弃** | 其职责被 Project → EvalTask[] 直接关系替代 |
| `EvalTask`（旧） | **重构** | 移除 eval_run_id，加 project_id + form_type + form_config 等新字段 |
| `EvalTrial`（旧） | **重构** | 移除 eval_run_id，加 batch_id + trial_index + form_type 等新字段 |
| `Grader` | **保留+增强** | 增加更多预置 Grader，加 form_type 标记适用形态 |
| `Simulator` | **保留+增强** | 增加面向不同形态的预置 Simulator |

---

## 四、评分计算体系

### 4.1 两层评分结构

> **设计决策**：分数只计算到 Task 级别。
> 理由：跑完一个 Task 后内容可能已经修改，再计算跨 Task 的聚合分数没有意义。
> Task 内部可选运行"跨 Trial 分析"找共性模式（定性），不做跨 Task 的数值聚合。

```
Layer 2:  任务分 (Task Score)     ← 最高层级，即最终展示的分数
            ↑ 统计聚合（mean ± std）
Layer 1:  试验分 (Trial Score)
            ↑ 加权合并
          Grader 维度分 + Simulator 自评分 + 场景结果
```

### 4.2 Layer 1：Trial 分数

#### 直接判定（Assessment）

```
trial_score = Σ(grader_weight[i] × grader_avg[i]) / Σ(grader_weight[i])

其中 grader_avg[i] = mean(grader_i 的各维度分数)
```

只有 Grader 分数，无 Simulator 自评。默认所有 Grader 权重相等 = 1。

#### 视角审查（Review）

```
如果有附加 Grader：
  trial_score = grader_avg  (Grader 打分)

如果没有附加 Grader（纯定性审查）：
  trial_score = NULL  (不产生分数，只有文字反馈)
```

审查本身是定性的。附加 Grader 使其定量。无 Grader 时该 Task 不产生分数，但反馈内容参与跨 Trial 分析。

#### 消费体验（Experience）

```
trial_score = exploration_weight × exploration_score + grader_weight × grader_score

其中：
  exploration_score = mean(各内容块的 per_block_score)  # 消费者逐块评分的均值
  grader_score = mean(附加 Grader 各维度)
  
默认权重：exploration_weight = 0.4, grader_weight = 0.6
（Grader 更客观，消费者逐块自评有主观偏差）
```

如果没有附加 Grader，则 `trial_score = exploration_score`。

**注意**：消费体验的 exploration_score 来自多步探索中每个内容块的独立评分，不是单次 LLM 输出。

#### 场景模拟（Scenario）

```
trial_score = sim_weight × sim_score + grader_weight × grader_score

其中：
  sim_score = mean(对话过程中各维度评分)
  grader_score = mean(附加 Grader 各维度)

默认权重：sim_weight = 0.4, grader_weight = 0.6
```

**不再有"转化成功/失败"的二元结果分**——场景模拟只产生对话证据，评分完全交给 Grader 根据维度灵活打分。如果确实需要判定"转化成功"，可以在 Grader 维度中加一个"转化信号强度"维度。

### 4.3 Layer 2：Task 分数（跨 Trial 聚合）

一个 Task 配置了 `repeat_count = N`，执行后产生 N 个 Trial。

```
task_score = {
  mean:  mean(trial_scores),             # 均值
  std:   std(trial_scores),              # 标准差（置信度指标）
  min:   min(trial_scores),              # 最低分
  max:   max(trial_scores),              # 最高分
}

显示为：  "7.5 ± 0.8"  (mean ± std)
```

**各维度也独立聚合**：
```
dim_scores["语言质量"] = {
  mean: mean(各 trial 的"语言质量"分),
  std:  std(各 trial 的"语言质量"分),
}
```

**不再有场景模拟的"转化率"指标**——转化判定交给 Grader 维度灵活处理。

### 4.4 "过期"检测

```python
content_hash = hashlib.md5(
    "||".join(sorted(target_block_contents))
).hexdigest()

is_stale = task.content_hash != compute_current_hash(task.target_block_ids)
```

当 Task 的目标内容发生变化后：
- Task 状态变为 `stale`
- 分数显示为灰色 + ⚠️ 图标
- 提示"内容已变更，建议重新运行"
- 旧分数保留可见（作为参考），不从聚合中移除

---

## 五、前端面板设计

### 5.1 整体架构：三个 Tab，对应三个 ContentBlock

评估面板在"评估"阶段作为主内容区呈现，由三个 Tab 组成，分别对应三个已有的 ContentBlock：

```
┌──────────────────────────────────────────────────────────────┐
│  [🧑 人物画像]    [📋 任务配置]    [📊 评估报告]              │
│       ↑                ↑                 ↑                   │
│  eval_persona_setup  eval_task_config  eval_report           │
│  ContentBlock        ContentBlock      ContentBlock          │
│                                                              │
│  "用谁来测试"        "要测什么"        "测了什么结果"          │
│  画像管理            配置+启动          执行记录+看板+诊断     │
└──────────────────────────────────────────────────────────────┘
```

- **画像 Tab**：扁平列表管理所有角色（不区分预装/用户），每个画像 = 名称 + 单提示词框
- **配置 Tab**：扁平 Task 列表（不按形态分组），每个 Task 含多个 Trial，点 [▶] 执行
- **报告 Tab**：扁平执行记录列表（一次执行 = 一条记录），点击 → 弹窗看板

### 5.2 画像 Tab（eval_persona_setup ContentBlock）

管理评估中使用的**所有角色**——消费者、审查角色、自定义角色——统一在一个扁平列表中，不做分类。

每个画像只有**一个提示词框**（奥卡姆剃刀）。项目创建时自动预装 3 个审查角色（和用户画像完全平等，可编辑可删除）。

```
┌──────────────────────────────────────────────────────────────┐
│  🧑 人物画像                                  [🤖 AI 生成画像] │
│                                                              │
│  ┌──────────────────────────────────────────────────────┐   │
│  │  策略教练                          [预装] [✏️][🗑️]    │   │
│  │  你是一位资深的内容策略教练。你带过上百个...            │   │
│  └──────────────────────────────────────────────────────┘   │
│                                                              │
│  ┌──────────────────────────────────────────────────────┐   │
│  │  资深编辑                          [预装] [✏️][🗑️]    │   │
│  │  你是一位有15年经验的资深内容编辑...                    │   │
│  └──────────────────────────────────────────────────────┘   │
│                                                              │
│  ┌──────────────────────────────────────────────────────┐   │
│  │  领域专家                          [预装] [✏️][🗑️]    │   │
│  │  你是这个内容所涉及领域的资深专家...                    │   │
│  └──────────────────────────────────────────────────────┘   │
│                                                              │
│  ┌──────────────────────────────────────────────────────┐   │
│  │  张晨                              [调研] [✏️][🗑️]    │   │
│  │  你是张晨，28岁，互联网产品经理，工作3年...            │   │
│  └──────────────────────────────────────────────────────┘   │
│                                                              │
│  ┌──────────────────────────────────────────────────────┐   │
│  │  李明                              [调研] [✏️][🗑️]    │   │
│  │  你是李明，32岁，数据分析师，工作5年...                │   │
│  └──────────────────────────────────────────────────────┘   │
│                                                              │
│  ┌──────────────────────────────────────────────────────┐   │
│  │  严厉的竞品分析师                  [手动] [✏️][🗑️]    │   │
│  │  你是一位...                                           │   │
│  └──────────────────────────────────────────────────────┘   │
│                                                              │
│  [+ 手动添加画像]                                            │
└──────────────────────────────────────────────────────────────┘
```

点击 [✏️] → **弹窗**编辑画像（名称 + 单个提示词框）。

**画像数据结构**（存在 `eval_persona_setup` ContentBlock 的 content JSON 中）：
```json
{
  "personas": [
    { "id": "preset_coach", "name": "策略教练", "prompt": "你是一位资深的内容策略教练...", "source": "preset" },
    { "id": "preset_editor", "name": "资深编辑", "prompt": "你是一位有15年经验的资深内容编辑...", "source": "preset" },
    { "id": "preset_expert", "name": "领域专家", "prompt": "你是这个内容所涉及领域的资深专家...", "source": "preset" },
    { "id": "p1", "name": "张晨", "prompt": "你是张晨，28岁...", "source": "research" },
    { "id": "p2", "name": "李明", "prompt": "你是李明，32岁...", "source": "research" },
    { "id": "p3", "name": "严厉的竞品分析师", "prompt": "你是一位...", "source": "manual" }
  ]
}
```

**关键说明**：
- **不区分预装和用户画像**——所有画像一溜排下来，完全平等
- 每个画像 = 一个名称 + 一个提示词框，什么角色都能描述
- `source` 字段只是标记来源（`"preset"` / `"research"` / `"manual"` / `"ai"`），不影响显示分组
- 预装角色可编辑、可删除；用户也可手动添加任何角色
- [🤖 AI 生成画像] = 根据项目意图生成新画像
- 从消费者调研 ContentBlock 提取画像（沿用已有的 `_handle_persona_setup` 逻辑）

### 5.3 任务配置 Tab（eval_task_config ContentBlock）

任务配置页是**扁平的 Task 列表**（不按形态分组）。每个 Task 是一个命名容器，内含多个 Trial 配置。

```
┌──────────────────────────────────────────────────────────────┐
│  📋 任务配置                      [▶ 全部运行]  [+ 添加任务]  │
│                                                              │
│  ┌ 内容综合评估 ── 4个Trial ──────────────────── [▶][✏️] ─┐ │
│  │ 内容质量(判定) · 编辑审查(审查) · 张晨(体验,×3) · 李明(体验,×3) │ │
│  └───────────────────────────────────────────────────────┘ │
│                                                              │
│  ┌ 消费者测试 ──── 2个Trial ──────────────────── [▶][✏️] ─┐ │
│  │ 张晨-定价(体验,×3) · 李明-实操(体验,×3)                  │ │
│  └───────────────────────────────────────────────────────┘ │
│                                                              │
│  ┌ 销售就绪度 ──── 1个Trial ──────────────────── [▶][✏️] ─┐ │
│  │ 销售对话-王芳(场景,×3)                                   │ │
│  └───────────────────────────────────────────────────────┘ │
└──────────────────────────────────────────────────────────────┘
```

点击 **[✏️]** → **弹窗**编辑 Task：

```
┌──────────────────────────────────────────────────────────────┐
│  编辑任务: 内容综合评估                               [保存]  │
│                                                              │
│  任务名称                                                    │
│  ┌────────────────────────────────────────────────────────┐ │
│  │ 内容综合评估                                            │ │
│  └────────────────────────────────────────────────────────┘ │
│                                                              │
│  Trial 列表:                                                 │
│                                                              │
│  ┌ 内容质量检查 ─── 判定 ──── ×1 ──────── [✏️][🗑️] ──────┐ │
│  │ 评分器: 内容质量评分器                                  │ │
│  └────────────────────────────────────────────────────────┘ │
│                                                              │
│  ┌ 编辑审查 ─── 审查 ──── ×1 ────────── [✏️][🗑️] ──────┐  │
│  │ 角色: 资深编辑 · 焦点: 第三到五章衔接                   │ │
│  └────────────────────────────────────────────────────────┘ │
│                                                              │
│  ┌ 张晨-定价 ─── 体验 ──── ×3 ────────── [✏️][🗑️] ──────┐ │
│  │ 画像: 张晨 · 评分器: 消费者体验评分器                   │ │
│  └────────────────────────────────────────────────────────┘ │
│                                                              │
│  ┌ 李明-实操 ─── 体验 ──── ×3 ────────── [✏️][🗑️] ──────┐ │
│  │ 画像: 李明 · 评分器: 消费者体验评分器                   │ │
│  └────────────────────────────────────────────────────────┘ │
│                                                              │
│  [+ 添加 Trial]                                              │
└──────────────────────────────────────────────────────────────┘
```

**关键交互**：
- 点 **[▶]** = 执行该 Task（运行所有 TrialConfig × repeat_count），结果写入 DB，在报告 Tab 中产生一条新记录
- 点 **[✏️]** = 弹窗编辑 Task（修改名称、增删 Trial）
- 点 **[▶ 全部运行]** = 并行执行所有 Task
- 点 **[+ 添加任务]** = 添加 Task + 第一个 Trial（见 5.5）
- 任何配置变更 = 自动保存到 eval_task_config ContentBlock
- **不显示任何分数**——配置页只管配置和启动，分数在报告 Tab 看

### 5.4 评估报告 Tab（eval_report ContentBlock）

报告页是**扁平的 Task 执行记录列表**。每次在配置 Tab 点 [▶] 就在这里产生一条新记录，旧记录保留。

结构是**两层**：

```
报告 Tab
  ├── Task 执行记录（扁平列表，每次执行 = 一条记录）
  │     点击 → 弹窗看板（总分 + 各 Trial 详情 + 跨 Trial 分析）
  ├── Task 执行记录...
  └── Task 执行记录...
```

**第一层：扁平的执行记录列表**

```
┌──────────────────────────────────────────────────────────────┐
│  📊 评估报告                                                  │
│                                                              │
│  内容综合评估    7.8   10分钟前                               │
│  消费者测试      6.8   1小时前                                │
│  内容综合评估    7.2   2小时前                                │
│  销售就绪度      6.5   昨天                                   │
│  内容综合评估    6.5   昨天     ⚠️ 内容已变更                 │
│                                                              │
│  （只有执行过的 Task 出现在这里。报告页只读，无 [▶] 按钮。）  │
└──────────────────────────────────────────────────────────────┘
```

- 每条记录 = 一次 Task 执行，显示 Task 名称 + 总分 + 时间
- 同一个 Task 执行多次 → 多条记录，按时间排序
- 内容已变更的旧记录显示 ⚠️ 灰色
- **报告页只读，不可执行**——执行只在配置 Tab

**第二层：点击记录 → 弹窗看板**

看板展示这次执行的总分、各 Trial 详情、跨 Trial 分析：

```
┌──────────────────────────────────────────────────────────────────┐
│  内容综合评估  ──  总分: 7.8  ──  10分钟前                [×]   │
│                                                                  │
│  维度得分:                                                       │
│    需求匹配度 6.5    信息完整性 7.0    价值感知 6.2    体验流畅性 7.5  │
│                                                                  │
│  ── Trial 列表 ─────────────────────────────────────────────── │
│                                                                  │
│  ┌─ 内容质量检查 (判定)  ── 8.2 ─────────────────────────────┐ │
│  │  结构合理性 8.5 · 语言质量 8.0 · 信息准确性 8.3 · 可读性 8.0│ │
│  │  建议:                                                      │ │
│  │   ① "第三章结尾缺少过渡段落"          [🤖 让Agent修改]      │ │
│  │   ② "术语表建议增加到附录"            [🤖 让Agent修改]      │ │
│  └──────────────────────────────────────────────────────────┘ │
│                                                                  │
│  ┌─ 编辑审查 (审查: 资深编辑)  ── 7.5 ──────────────────────┐ │
│  │  审查反馈: "整体结构清晰，第三到五章衔接可以更自然..."      │ │
│  │  建议:                                                      │ │
│  │   ① "第三章结尾增加承上启下的段落"    [🤖 让Agent修改]      │ │
│  │   ② "第五章开头回顾前文要点"          [🤖 让Agent修改]      │ │
│  └──────────────────────────────────────────────────────────┘ │
│                                                                  │
│  ┌─ 张晨-定价 #1 (体验, repeat 1/3)  ── 7.2 ────────────────┐ │
│  │  探索过程:                                                  │ │
│  │   📋 探索计划: 先看课程大纲，再看定价，最后看评价           │ │
│  │   🔍 第一章: 关切匹配 高 · 课程结构清晰 (8分)              │ │
│  │   🔍 第二章: 关切匹配 中 · 定价论述不够有力 (5分)          │ │
│  │   🔍 第三章: 关切匹配 高 · ROI案例有说服力 (8分)           │ │
│  │   📝 总结: 整体有价值，但定价章节需要加强                  │ │
│  │  Grader: 需求匹配 7.0 · 信息完整 7.5 · 价值感知 7.0       │ │
│  │  建议:                                                      │ │
│  │   ① "定价章节缺少竞品对比数据"        [🤖 让Agent修改]      │ │
│  │   ② "增加ROI计算示例"                 [🤖 让Agent修改]      │ │
│  └──────────────────────────────────────────────────────────┘ │
│                                                                  │
│  ┌─ 张晨-定价 #2 (体验, repeat 2/3)  ── 6.5 ────────────────┐ │
│  │  探索过程: ... (折叠)                                       │ │
│  │  建议:                                                      │ │
│  │   ① "高级分析技能的实操指引不够具体"  [🤖 让Agent修改]      │ │
│  └──────────────────────────────────────────────────────────┘ │
│                                                                  │
│  ┌─ 张晨-定价 #3 (体验, repeat 3/3)  ── 6.8 ── (折叠) ──────┐ │
│  └──────────────────────────────────────────────────────────┘ │
│                                                                  │
│  ┌─ 李明-实操 #1 (体验, repeat 1/3)  ── 7.8 ── (折叠) ──────┐ │
│  └──────────────────────────────────────────────────────────┘ │
│  (... 更多 Trial ...)                                           │
│                                                                  │
│  ── 💡 跨 Trial 分析 ─────────── [🔍 生成跨 Trial 分析] ──── │
│                                                                  │
│  共性模式:                                                       │
│   • 🔴 3次消费者测试均在"性价比"上犹豫（Trial 3/4/5）          │
│   • 🟡 定价章节关切匹配度偏低（均值 5.3）                       │
│                                                                  │
│  逐条修改建议:                                                   │
│   1. 🔴 在内容中增加竞品对比段落      [🤖 让Agent修改]          │
│   2. 🟡 补充 ROI 计算示例和案例       [🤖 让Agent修改]          │
│   3. 🟢 加强"为什么值这个价"的论述    [🤖 让Agent修改]          │
│                                                                  │
│  ── LLM 调用统计 ────────────────────────────────────────── │
│  共 42 次 · Token: 12.5K in / 8.2K out · 费用: ¥0.35            │
└──────────────────────────────────────────────────────────────────┘
```

**两层关系总结**：

```
报告 Tab
│
├── 第一层：扁平执行记录列表（每次执行 Task = 一条记录，按时间排序）
│     │     同一 Task 多次执行 → 多条记录（旧的保留可对比趋势）
│     │
│     └── 点击记录 → 第二层：弹窗看板
│           ├── 总分 + 维度得分（纯数字，不用进度条）
│           ├── 各 Trial 详情（repeat 展开为独立行）
│           │     └── 每个 Grader 建议逐条列出，每条旁有 [🤖 让Agent修改]
│           ├── 跨 Trial 分析（可选触发，逐条建议旁有 [🤖 让Agent修改]）
│           └── LLM 调用统计
```

**核心设计原则**：
- **报告 = 扁平列表**——没有 Task 分组，没有执行记录嵌套，一次执行就是一条记录
- **旧记录永远保留**——同一 Task 的多次执行可以通过记录时间看到趋势
- **点击记录 → 弹窗看板**——看板内展示该次执行的所有 Trial 及其每个 repeat
- **分数只用数字**——不用进度条/柱状图，干净简洁
- **每条 Grader 建议 → 独立的 [🤖 让Agent修改] 按钮** → 点击直接生成 EditCard 发到 Agent 面板
- **跨 Trial 分析**在看板底部，点 [🔍 生成跨 Trial 分析] 可选触发
- **没有跨 Task 分析**——内容可能在 Task 间变更，聚合无意义
- 过期记录（内容已变更）显示灰色 + ⚠️
- **评估→修改闭环**：详见 suggestion_card_design.md 的 M3（Eval Report 桥接）

### 5.4a 评估→修改闭环（Agent 桥接）

> 参见 `suggestion_card_design.md` 的 M3（Eval Report 桥接）。

评估的终极价值不止于"发现问题"，而是"推动修改"。报告 Tab 中有两个位置可以一键触发修改：

| 位置 | 按钮 | 触发条件 |
|------|------|---------|
| Trial 级 Grader 建议（每个 Trial 展开后的 feedback） | [🤖 让Agent修改] | Grader 输出了具体的改进建议 |
| Task 级跨 Trial 分析建议（Task 看板底部） | [🤖 让Agent修改] | 跨 Trial 分析输出了逐条修改建议 |

> **不再有"综合诊断"入口**——跨 Task 分析因内容可能已变更而无意义（见 R5）。

**交互流程**：
```
用户点击 [🤖 让Agent修改]
    ↓
系统构造消息（包含建议描述 + 改进方向 + 来源 Task/Trial/Grader 信息）
    ↓
消息自动发送到 Agent 面板
    ↓
Agent 分析建议 → 调用 propose_edit → 输出 Suggestion Card
    ↓
用户在 Agent 面板中确认/拒绝/追问修改
```

**技术实现**：复用 `suggestion_card_design.md` 已建立的通信链：
- `EvalReportPanel` → `onSendToAgent` prop → 层层透传到 `WorkspacePage` → `AgentPanel`
- Agent 面板接收消息后正常走对话流，自然触发 `propose_edit`
- 不增加额外 API，纯粹是"往 Agent 对话里发一条带上下文的消息"

### 5.5 添加 Task 流程

点击配置 Tab 的 **[+ 添加任务]** 弹出 Modal。核心思路：**创建 Task 的第一步是配置 Trial**，因为 Task 只是一个命名容器，Trial 才是评估的最小单元。

所有弹窗的上半部分（Task 名称 + 形态选择）是固定的，选择形态后下半部分动态展开。

**通用上半部分**（四种形态共享）：

```
┌──────────────────────────────────────────────────────────────┐
│  添加评估任务                                                 │
│                                                              │
│  任务名称*                                                    │
│  ┌────────────────────────────────────────────────────────┐ │
│  │ (输入 Task 名称)                                        │ │
│  └────────────────────────────────────────────────────────┘ │
│                                                              │
│  ── 第一个 Trial ────────────────────────────────────────── │
│                                                              │
│  评估形态*                                                    │
│  ┌────────────────────────────────────────────────────────┐ │
│  │ ▼ 选择                                                  │ │
│  ├────────────────────────────────────────────────────────┤ │
│  │  📊 直接判定  ── 用评分标准直接评分                     │ │
│  │  👁️ 视角审查  ── 让专家角色审读内容                     │ │
│  │  👤 消费体验  ── 模拟人物分块探索内容                   │ │
│  │  💼 场景模拟  ── 模拟双方对话交互                       │ │
│  └────────────────────────────────────────────────────────┘ │
│                                                              │
│  ── 选择形态后展开下方配置 ──────────────────────────────── │
│  ...                                                         │
└──────────────────────────────────────────────────────────────┘
```

---

**形态 A：📊 直接判定**（最简单——只需评分器）

```
┌──────────────────────────────────────────────────────────────┐
│  添加评估任务                                                 │
│                                                              │
│  任务名称*    [内容质量评估                              ]    │
│                                                              │
│  ── 第一个 Trial ────────────────────────────────────────── │
│  评估形态*    [📊 直接判定 ▼]                                 │
│                                                              │
│  ── 直接判定配置 ─────────────────────────────────────────── │
│                                                              │
│  Trial 名称*                                                  │
│  ┌────────────────────────────────────────────────────────┐ │
│  │ 内容质量检查                                            │ │
│  └────────────────────────────────────────────────────────┘ │
│                                                              │
│  评分器*（必选，至少一个）                    [+ 添加更多]    │
│  ┌────────────────────────────────────────────────────────┐ │
│  │ ☑ 内容质量评分器 (预置)                                 │ │
│  │ ☐ 策略对齐评分器 (预置)                                 │ │
│  └────────────────────────────────────────────────────────┘ │
│                                                              │
│  焦点 (Probe)                                      ← 可选   │
│  ┌────────────────────────────────────────────────────────┐ │
│  │ 重点检查第三章的逻辑连贯性和论据支撑                    │ │
│  └────────────────────────────────────────────────────────┘ │
│                                                              │
│  执行次数  [1]                                                │
│  内容范围  ◉ 全部  ○ 指定内容块                              │
│                                                              │
│         [取消]                              [创建任务]        │
└──────────────────────────────────────────────────────────────┘
```

> 直接判定是最轻量的形态：没有角色、没有对话、没有探索。Grader 直接拿内容评分。

---

**形态 B：👁️ 视角审查**（选角色 + 可选评分器）

```
┌──────────────────────────────────────────────────────────────┐
│  添加评估任务                                                 │
│                                                              │
│  任务名称*    [内容综合评估                              ]    │
│                                                              │
│  ── 第一个 Trial ────────────────────────────────────────── │
│  评估形态*    [👁️ 视角审查 ▼]                                │
│                                                              │
│  ── 视角审查配置 ─────────────────────────────────────────── │
│                                                              │
│  Trial 名称*                                                  │
│  ┌────────────────────────────────────────────────────────┐ │
│  │ 编辑审查                                                │ │
│  └────────────────────────────────────────────────────────┘ │
│                                                              │
│  审查角色*（从画像 Tab 选择）                                 │
│  ┌────────────────────────────────────────────────────────┐ │
│  │ ▼ 资深编辑                                              │ │
│  ├────────────────────────────────────────────────────────┤ │
│  │   策略教练  [预装]                                      │ │
│  │   资深编辑  [预装]  ✓                                   │ │
│  │   领域专家  [预装]                                      │ │
│  │   严厉的竞品分析师  [手动]                              │ │
│  └────────────────────────────────────────────────────────┘ │
│                                                              │
│  角色提示词                          [🤖 AI生成]             │
│  ┌────────────────────────────────────────────────────────┐ │
│  │ 你是一位有15年经验的资深内容编辑。你审查过大量商业      │ │
│  │ 文案和知识付费内容。你的审查标准：结构逻辑、语言质量、  │ │
│  │ 信息准确性、读者体验...                                 │ │
│  └────────────────────────────────────────────────────────┘ │
│  ↑ 选择角色后自动填入该画像的提示词，可在此处微调          │
│                                                              │
│  焦点 (Probe)                                      ← 可选   │
│  ┌────────────────────────────────────────────────────────┐ │
│  │ 重点审查第三章到第五章的衔接逻辑                        │ │
│  └────────────────────────────────────────────────────────┘ │
│                                                              │
│  评分器                                      [+ 添加更多]    │
│  ┌────────────────────────────────────────────────────────┐ │
│  │ ☑ 内容质量评分器 (预置)                       ← 可选   │ │
│  └────────────────────────────────────────────────────────┘ │
│  ↑ 审查本身产出定性反馈；附加 Grader 使其定量              │
│                                                              │
│  执行次数  [1]                                                │
│  内容范围  ◉ 全部  ○ 指定内容块                              │
│                                                              │
│         [取消]                              [创建任务]        │
└──────────────────────────────────────────────────────────────┘
```

> 视角审查 = 让一个角色（编辑/教练/专家/自定义）审读内容并给出反馈。角色从画像 Tab 选，提示词自动带入但可微调。Grader 可选——有 Grader 则产出分数，无 Grader 则纯定性反馈。

---

**形态 C：👤 消费体验**（选画像 + 分块探索）

```
┌──────────────────────────────────────────────────────────────┐
│  添加评估任务                                                 │
│                                                              │
│  任务名称*    [消费者测试                                ]    │
│                                                              │
│  ── 第一个 Trial ────────────────────────────────────────── │
│  评估形态*    [👤 消费体验 ▼]                                 │
│                                                              │
│  ── 消费体验配置 ─────────────────────────────────────────── │
│                                                              │
│  Trial 名称*                                                  │
│  ┌────────────────────────────────────────────────────────┐ │
│  │ 张晨-定价疑虑                                           │ │
│  └────────────────────────────────────────────────────────┘ │
│                                                              │
│  人物画像*（从画像 Tab 选择）                                 │
│  ┌────────────────────────────────────────────────────────┐ │
│  │ ▼ 张晨                                                  │ │
│  ├────────────────────────────────────────────────────────┤ │
│  │   张晨      [调研]  ✓                                   │ │
│  │   李明      [调研]                                      │ │
│  │   王芳      [调研]                                      │ │
│  └────────────────────────────────────────────────────────┘ │
│                                                              │
│  消费者提示词                        [🤖 AI生成]             │
│  ┌────────────────────────────────────────────────────────┐ │
│  │ 你是张晨，28岁，互联网产品经理，工作3年。你最近在考虑  │ │
│  │ 职业转型到数据分析方向。你的核心顾虑是：课程3999的      │ │
│  │ 定价是否值得？你担心学完之后实际工作中用不上...          │ │
│  └────────────────────────────────────────────────────────┘ │
│  ↑ 选择画像后自动填入，可在此处微调                        │
│                                                              │
│  测试焦点 (Probe)                                  ← 可选   │
│  ┌────────────────────────────────────────────────────────┐ │
│  │ 测试张晨能否从内容中找到关于课程定价合理性的说服力论据  │ │
│  └────────────────────────────────────────────────────────┘ │
│                                                              │
│  评分器                                      [+ 添加更多]    │
│  ┌────────────────────────────────────────────────────────┐ │
│  │ ☑ 消费者体验评分器 (预置)                     ← 可选   │ │
│  └────────────────────────────────────────────────────────┘ │
│                                                              │
│  执行次数  [3]                                                │
│  内容范围  ◉ 全部  ○ 指定内容块                              │
│                                                              │
│         [取消]                              [创建任务]        │
└──────────────────────────────────────────────────────────────┘
```

> 消费体验 = 让一个人物画像分块探索内容（非对话）。执行时自动进行：探索规划 → 逐块阅读评价 → 总结 → Grader 评分。多次执行可看同一画像在不同运行中的差异。

---

**形态 D：💼 场景模拟**（双方角色 + 对话 + 50字限制）

```
┌──────────────────────────────────────────────────────────────┐
│  添加评估任务                                                 │
│                                                              │
│  任务名称*    [销售就绪度                                ]    │
│                                                              │
│  ── 第一个 Trial ────────────────────────────────────────── │
│  评估形态*    [💼 场景模拟 ▼]                                 │
│                                                              │
│  ── 场景模拟配置 ─────────────────────────────────────────── │
│                                                              │
│  Trial 名称*                                                  │
│  ┌────────────────────────────────────────────────────────┐ │
│  │ 销售-价格异议                                           │ │
│  └────────────────────────────────────────────────────────┘ │
│                                                              │
│  角色 A*（从画像 Tab 选择）                                   │
│  ┌────────────────────────────────────────────────────────┐ │
│  │ ▼ 课程顾问（自定义）                                   │ │
│  └────────────────────────────────────────────────────────┘ │
│                                                              │
│  角色 A 提示词                       [🤖 AI生成]             │
│  ┌────────────────────────────────────────────────────────┐ │
│  │ 你是一位资深课程顾问。你对课程内容非常熟悉，需要        │ │
│  │ 基于课程实际内容回答客户的疑问。你的目标是帮助客户      │ │
│  │ 理解课程价值，解答其关于定价、内容深度等问题...          │ │
│  └────────────────────────────────────────────────────────┘ │
│                                                              │
│  角色 B*（从画像 Tab 选择）                                   │
│  ┌────────────────────────────────────────────────────────┐ │
│  │ ▼ 王芳                                                  │ │
│  └────────────────────────────────────────────────────────┘ │
│                                                              │
│  角色 B 提示词                       [🤖 AI生成]             │
│  ┌────────────────────────────────────────────────────────┐ │
│  │ 你是王芳，35岁，某公司培训负责人，需要为团队选一门      │ │
│  │ 数据分析课程。你的主要顾虑是课程能否满足团队不同        │ │
│  │ 层次的需求，以及价格是否合理（预算有限）...              │ │
│  └────────────────────────────────────────────────────────┘ │
│                                                              │
│  测试焦点 (Probe)                                  ← 可选   │
│  ┌────────────────────────────────────────────────────────┐ │
│  │ 价格异议能否被有效化解                                  │ │
│  └────────────────────────────────────────────────────────┘ │
│                                                              │
│  最大对话轮数  [8]                                            │
│                                                              │
│  评分器                                      [+ 添加更多]    │
│  ┌────────────────────────────────────────────────────────┐ │
│  │ ☑ 场景对话评分器 (预置)                       ← 可选   │ │
│  └────────────────────────────────────────────────────────┘ │
│                                                              │
│  执行次数  [3]                                                │
│  内容范围  ◉ 全部  ○ 指定内容块                              │
│                                                              │
│         [取消]                              [创建任务]        │
└──────────────────────────────────────────────────────────────┘
```

> 场景模拟 = 唯一的对话式形态。两个角色轮流发言，模拟真实商业场景交互。角色不限于"卖方/买方"，可以是任何场景双方（课程顾问↔学员、客服↔用户等）。
> **50 字限制**写在角色提示词里（如"每次回复不超过50字"），不在 UI 层面硬编码。预置的场景模拟提示词模板已包含此约束（见 6.4）。

---

创建后，可在 Task 编辑弹窗中继续 **[+ 添加 Trial]** 来添加更多 Trial（可以是不同形态）。

**各形态的 Trial 配置字段差异**：

| 字段 | 直接判定 | 视角审查 | 消费体验 | 场景模拟 |
|------|---------|---------|---------|---------|
| Trial 名称 | ✅ | ✅ | ✅ | ✅ |
| 评分器选择 | ✅ 必选 | 可选 | 可选 | 可选 |
| 内容范围 | ✅ | ✅ | ✅ | ✅ |
| 执行次数 | ✅ | ✅ | ✅ | ✅ |
| 人物画像 | — | ✅ 从画像选 | ✅ 从画像选 | ✅ 从画像选(双方) |
| 焦点 (Probe) | 可选 | 可选 | 可选 | 可选 |
| 最大对话轮数 | — | — | — | ✅ |
| 提示词 | — | 角色提示词 [🤖 AI生成] | 消费者提示词 [🤖 AI生成] | 角色A/B提示词 [🤖 AI生成] |

### 5.6 AI 生成功能

> **设计决策**：只提供 **AI 生成**，不提供"AI 优化"或"AI 建议 Probe"。
> 理由：保持简单。用户需要的是"帮我从零生成"，而不是"帮我改进已有的"。

在配置过程中，以下位置有 [🤖 AI 生成] 按钮：

| 位置 | 功能 | 说明 |
|------|------|------|
| 人物画像 | AI 生成画像 | 根据项目意图和调研，生成一个人物画像提示词 |
| 消费者/买方提示词 | AI 生成提示词 | 根据形态和画像，生成对话系统提示词 |
| 内容方/卖方提示词 | AI 生成提示词 | 根据形态和内容，生成对话系统提示词 |
| 审查角色提示词 | AI 生成提示词 | 根据角色描述，生成审查系统提示词 |
| 评分器提示词 | AI 生成提示词 | 根据维度和评估目标，生成评分提示词 |

**统一的 AI 生成接口**：

```
POST /api/eval/prompts/generate

请求: { prompt_type: str, context: dict }
响应: { generated_prompt: str }

prompt_type 可选值:
  - "persona"            → 生成人物画像
  - "consumer_prompt"    → 生成消费者对话提示词
  - "representative_prompt" → 生成内容方对话提示词
  - "seller_prompt"      → 生成卖方提示词
  - "buyer_prompt"       → 生成买方提示词
  - "reviewer_prompt"    → 生成审查角色提示词
  - "grader_prompt"      → 生成评分器提示词
```

---

## 六、预置评分器和提示词

### 6.1 直接判定（Assessment）默认 Grader

#### Grader A1：内容质量评分器

```
名称: 内容质量评分器
类型: content_only
适用形态: assessment
维度: [结构合理性, 语言质量, 信息准确性, 可读性]

提示词:
───────────────────────────────────────────────
你是一位资深内容编辑和质量评审专家，拥有 15 年以上的出版行业经验。
你对文字的结构、逻辑、表达有极高的专业标准。

请对以下内容进行严格、客观的质量评审。

【被评估内容】
{content}

【评估维度与标准】
1. 结构合理性 (1-10)
   - 9-10: 结构清晰，层层递进，读者可以自然跟随逻辑
   - 7-8: 结构基本清晰，个别段落衔接可以更好
   - 5-6: 结构存在问题，部分内容位置不当或逻辑跳跃
   - 1-4: 结构混乱，读者难以跟随

2. 语言质量 (1-10)
   - 9-10: 表达精准流畅，无冗余无歧义，读起来是享受
   - 7-8: 表达清晰，偶有冗余或不够精准之处
   - 5-6: 表达勉强能懂，但有明显的啰嗦、重复或表意不清
   - 1-4: 表达混乱，多处读不懂

3. 信息准确性 (1-10)
   - 9-10: 所有事实陈述可验证、数据有来源、无误导性表述
   - 7-8: 基本准确，个别表述不够严谨但不构成误导
   - 5-6: 有明显的事实错误或缺乏根据的断言
   - 1-4: 大量错误或胡编乱造

4. 可读性 (1-10)
   - 9-10: 目标读者轻松理解，专业术语有解释，例子恰当
   - 7-8: 大部分可理解，个别地方对目标读者偏难或偏简
   - 5-6: 对目标读者来说理解有困难
   - 1-4: 目标读者基本读不懂

请严格输出以下 JSON 格式，不要输出其他内容：
{{"scores": {{"结构合理性": 分数, "语言质量": 分数, "信息准确性": 分数, "可读性": 分数}}, "comments": {{"结构合理性": "具体评语（引用原文说明问题）", "语言质量": "具体评语", "信息准确性": "具体评语", "可读性": "具体评语"}}, "feedback": "整体评价、最严重的问题、最重要的改进方向（150-250字）"}}
───────────────────────────────────────────────
```

#### Grader A2：策略对齐评分器

```
名称: 策略对齐评分器
类型: content_only
适用形态: assessment
维度: [意图对齐度, 受众匹配度, 差异化, 完整性]

提示词:
───────────────────────────────────────────────
你是一位资深内容策略顾问。你不看文字好不好，你看方向对不对。
你的任务是判断：这些内容是否在正确的方向上？是否服务于正确的目标？

请对以下内容进行策略层面的评估。

【被评估内容】
{content}

【评估维度与标准】
1. 意图对齐度 (1-10)
   - 这些内容是否清晰地服务于一个可辨识的目标？
   - 每一段内容是否都有存在的理由？有没有"凑数"的部分？
   - 9-10: 每段内容都精准指向核心目标
   - 5-6: 目标模糊，内容有偏离
   - 1-4: 看不出内容要达成什么

2. 受众匹配度 (1-10)
   - 内容的难度、语气、关注点是否匹配目标受众？
   - 9-10: 感觉就是为目标读者量身定制的
   - 5-6: 受众定位不清，时而太专业时而太浅
   - 1-4: 完全不适合目标受众

3. 差异化 (1-10)
   - 和同类内容相比，这些内容有什么独特的角度、方法、见解？
   - 9-10: 有明显的独特视角或方法论
   - 5-6: 内容正确但缺乏新意
   - 1-4: 完全可以被现有免费内容替代

4. 完整性 (1-10)
   - 从策略目标来看，该覆盖的主题是否都覆盖了？
   - 9-10: 完整覆盖，无遗漏
   - 5-6: 有明显的主题缺失
   - 1-4: 严重不完整

请严格输出以下 JSON 格式，不要输出其他内容：
{{"scores": {{"意图对齐度": 分数, "受众匹配度": 分数, "差异化": 分数, "完整性": 分数}}, "comments": {{"意图对齐度": "评语", "受众匹配度": "评语", "差异化": "评语", "完整性": "评语"}}, "feedback": "策略层面的总体判断和最关键的建议（150-250字）"}}
───────────────────────────────────────────────
```

### 6.2 视角审查（Review）默认角色提示词

> **注意**：以下提示词作为画像 Tab 中的**预装角色**默认提示词。用户可以在画像 Tab 中自由修改。
> 视角审查执行时，从画像列表中选一个角色，其提示词就是这里的内容。

#### 预装角色 R1：策略教练

```
名称: 策略教练
角色: coach
适用形态: review

系统提示词:
───────────────────────────────────────────────
你是一位资深的内容策略教练。你带过上百个内容创作者，见过无数"方向正确但执行偏了"和"执行精良但方向错误"的案例。

你的审查视角是**战略层面**——你不纠结文字好不好看，你关心的是：
1. 这些内容的核心命题是什么？这个命题有没有市场？
2. 目标受众是否被准确定义？内容是否真的在解决他们的痛点？
3. 和市面上的竞品内容相比，差异化够不够？
4. 有没有战略性的遗漏？（比如只讲方法不讲为什么，或者只讲知识不讲应用）

{focus}

请阅读以下内容，然后给出你作为策略教练的审查意见。
不需要面面俱到，集中在**最关键的 2-3 个战略性发现**上。

【被审查内容】
{content}

请以JSON格式输出：
{{"findings": [{{"title": "发现标题", "severity": "high/medium/low", "detail": "详细说明（引用具体内容）", "recommendation": "改进建议"}}], "strategic_summary": "一句话总体战略判断", "scores": {{"策略清晰度": 分数(1-10), "方向正确性": 分数(1-10), "差异化潜力": 分数(1-10)}}}}
───────────────────────────────────────────────

焦点占位符说明:
{focus} 替换规则：
  - 如果用户填写了审查焦点，替换为:
    "本次审查的特定焦点: [用户输入的焦点]。请优先从这个角度评估。"
  - 如果未填写焦点，替换为空字符串
```

#### 预装角色 R2：资深编辑

```
名称: 资深编辑
角色: editor
适用形态: review

系统提示词:
───────────────────────────────────────────────
你是一位在顶级出版社工作了 20 年的资深编辑。你对文字有近乎偏执的标准——不是追求"完美"，而是追求"准确"：每个词都用在最对的地方，每段话都在最该出现的位置。

你的审查视角是**手艺层面**：
1. 结构：文章的骨架是否合理？段落之间是否有清晰的逻辑递进？
2. 语言：表达是否精准？有没有啰嗦、重复、或含糊的地方？
3. 节奏：读起来的节奏感如何？长短句搭配是否得当？
4. 风格：整体风格是否一致？有没有突然跳tone的地方？
5. 开头和结尾：开头是否吸引人？结尾是否有力？

{focus}

请阅读以下内容，然后给出你作为编辑的审查意见。
请直接指出具体问题，**引用原文**并给出修改建议。
不需要夸奖，集中在改进点上。

【被审查内容】
{content}

请以JSON格式输出：
{{"findings": [{{"title": "问题标题", "severity": "high/medium/low", "original_text": "原文引用", "issue": "问题说明", "suggestion": "修改建议"}}], "editorial_summary": "编辑总评（100字以内）", "scores": {{"结构": 分数(1-10), "语言": 分数(1-10), "风格一致性": 分数(1-10), "可读性": 分数(1-10)}}}}
───────────────────────────────────────────────
```

#### 预装角色 R3：领域专家

```
名称: 领域专家
角色: expert
适用形态: review

系统提示词:
───────────────────────────────────────────────
你是这个内容所涉及领域的资深专家。你拥有该领域 10 年以上的实践经验和深厚的理论功底。

你的审查视角是**专业层面**：
1. 事实准确性：内容中的每个事实陈述是否正确？有没有过时或错误的信息？
2. 专业深度：对核心概念的理解是否到位？是"懂了说"还是"看了转述"？
3. 实践相关性：给出的方法、建议是否在实践中可行？有没有"听起来正确但做起来不行"的内容？
4. 关键遗漏：从专业角度看，有哪些重要但被遗漏的主题？

{focus}

请阅读以下内容，然后以领域专家的身份给出审查意见。
对于每个问题，请明确指出它在实践中可能造成的后果。

【被审查内容】
{content}

请以JSON格式输出：
{{"findings": [{{"title": "发现标题", "severity": "high/medium/low", "detail": "详细说明", "practical_impact": "在实践中的后果", "recommendation": "改进建议"}}], "expert_summary": "专家总评（100字以内）", "scores": {{"事实准确性": 分数(1-10), "专业深度": 分数(1-10), "实践可行性": 分数(1-10)}}}}
───────────────────────────────────────────────
```

### 6.3 消费体验（Experience）默认提示词——分块探索三步流程

> **注意**：消费体验不再是对话模式。没有"内容方"角色。
> 流程：探索规划 → 逐块评价 → 总结评价 → [可选] Grader 评分。

#### 第一步：探索规划提示词

```
名称: 探索规划提示词
角色: consumer_explorer
适用形态: experience
步骤: plan

系统提示词:
───────────────────────────────────────────────
你是一位真实的消费者/用户。

【你的身份】
{persona}

{probe_section}

你面前有一份内容产品，包含以下章节：
{block_list}

请以你的身份思考：你会按什么顺序阅读这些章节？为什么？你期望从每个章节中找到什么？

请输出 JSON：
{{"plan": [{{"block_id": "xxx", "block_title": "章节标题", "reason": "为什么先看这个", "expectation": "期望找到什么"}}], "overall_goal": "你读这份内容的核心目的（1句话）"}}
───────────────────────────────────────────────
```

> `{probe_section}` 占位符：如果配置了 Probe，则为 `【你的核心关切】\n{probe}`；如果没有配置，则为空。

#### 第二步：逐块评价提示词（每个内容块调用一次）

```
名称: 逐块探索提示词
角色: consumer_explorer
适用形态: experience
步骤: per_block

系统提示词:
───────────────────────────────────────────────
你是一位真实的消费者/用户。

【你的身份】
{persona}

{probe_section}

你正在逐章阅读一份内容产品。

【之前的阅读记忆】
{exploration_memory}

【现在你正在阅读的章节】
标题：{block_title}
内容：
{block_content}

请以你的真实身份，评价这个章节：

输出 JSON：
{{"concern_match": "这部分是否回应了你的关切？怎么回应的？（如果没有关切则评价是否对你有价值）", "discovery": "你从中发现了什么有价值的信息？", "doubt": "什么地方让你不确定或失望？", "missing": "你期望在这里看到但没看到的内容", "feeling": "作为 {persona_name}，读完这部分的真实感受（2-3句话，用第一人称）", "score": 分数(1-10)}}
───────────────────────────────────────────────
```

> `{exploration_memory}` = 之前已阅读章节的摘要，格式如：
> "已读《章节A》：发现定价信息不够详细（6分）；已读《章节B》：方法论讲解清晰（8分）"

#### 第三步：总结评价提示词

```
名称: 探索总结提示词
角色: consumer_explorer
适用形态: experience
步骤: summary

系统提示词:
───────────────────────────────────────────────
你是一位真实的消费者/用户。

【你的身份】
{persona}

{probe_section}

你已经读完了所有章节，以下是你逐章的阅读笔记：
{all_block_results}

请综合所有章节的阅读体验，给出你的最终评价：

输出 JSON：
{{"overall_impression": "总体印象（3-5句话）", "concerns_addressed": ["被回应的关切/需求"], "concerns_unaddressed": ["未被回应的关切/需求"], "strongest_sections": ["最有价值的章节及原因"], "weakest_sections": ["最让你失望的章节及原因"], "would_recommend": true/false, "recommendation_reason": "推荐/不推荐的理由", "summary": "作为 {persona_name}，100-200字总体评价"}}
───────────────────────────────────────────────
```

#### 消费者体验评分器（Grader）

```
名称: 消费者体验评分器
类型: content_and_exploration
适用形态: experience
维度: [需求匹配度, 信息完整性, 价值感知, 内容结构]

提示词:
───────────────────────────────────────────────
你是一位用户体验评审专家。请基于以下消费者的分块探索结果，评估内容对消费者需求的满足程度。

注意：你评估的是**内容本身的价值**，不是消费者表述的技巧。关键问题是：内容是否包含了解决消费者问题所需的信息？

【被评估内容】
{content}

【消费者探索结果】
{exploration_results}

【评估维度】
1. 需求匹配度 (1-10): 内容是否解决了消费者的核心关切？
   - 9-10: 消费者的核心关切在多个章节中被充分回应
   - 5-6: 部分回应了，但关键关切未被打消
   - 1-4: 内容完全不能回应消费者的需求

2. 信息完整性 (1-10): 消费者期望的信息，内容中是否都覆盖了？
   - 9-10: 各章节完整覆盖了消费者关心的主题
   - 5-6: 有明显的信息缺口（消费者在多个章节标注了"missing"）
   - 1-4: 内容中缺少大量消费者需要的信息

3. 价值感知 (1-10): 消费者整体是否感受到了明确的价值？
   - 9-10: 消费者在总结中明确表达了"这个对我有用/值得/会推荐"
   - 5-6: 消费者态度模糊
   - 1-4: 消费者表达了失望

4. 内容结构 (1-10): 信息的分布和组织是否合理？
   - 9-10: 消费者的探索路径顺畅，每个章节都有明确价值
   - 5-6: 部分章节价值不明确或内容分布不均
   - 1-4: 消费者在多个章节感到困惑或信息冗余

请严格输出以下 JSON 格式：
{{"scores": {{"需求匹配度": 分数, "信息完整性": 分数, "价值感知": 分数, "内容结构": 分数}}, "comments": {{"需求匹配度": "评语", "信息完整性": "评语", "价值感知": "评语", "内容结构": "评语"}}, "content_gaps": ["消费者标注的缺失信息汇总"], "strongest_blocks": ["最有价值的章节"], "weakest_blocks": ["最需要改进的章节"], "feedback": "整体评价和改进建议（150-250字）"}}
───────────────────────────────────────────────
```

### 6.4 场景模拟（Scenario）默认提示词

> **注意**：场景模拟是双角色对话。角色 A 持有内容（如卖方/服务方），角色 B 是对方（如买方/客户）。
> **每次发言 ≤50 字**，模拟真实口语对话。
> 角色 A / B 的身份由画像定义，以下是**通用的对话模板提示词**。

#### 角色 A（持有内容方）提示词

```
名称: 场景角色A提示词
角色: scenario_role_a
适用形态: scenario

系统提示词:
───────────────────────────────────────────────
{persona}

你掌握以下内容，这是你对话的知识基础：
{content}

{probe_section}

【对话规则】
1. 每次回复最多 50 字。像真人聊天一样简洁。
2. 基于上面的内容回答对方的问题。不要编造内容中没有的信息。
3. 如果对方的问题超出了内容覆盖范围，诚实地说"这方面我不太确定"。
4. 用具体的事实和细节来回应，避免空泛的描述和保证。
5. 不要暴露你是AI。

请用第一人称说话，自然、口语化。
───────────────────────────────────────────────
```

#### 角色 B（对话对方）提示词

```
名称: 场景角色B提示词
角色: scenario_role_b
适用形态: scenario

系统提示词:
───────────────────────────────────────────────
{persona}

{probe_section}

【对话规则】
1. 每次回复最多 50 字。像真人聊天一样简洁。
2. 带着你的目的和顾虑主动提问和回应。
3. 如果对方的回答没有解决你的疑虑，追问。不要轻易被说服。
4. 如果对方的回答确实有价值，可以自然地表达认可。
5. 不要暴露你是AI。

请用第一人称说话，自然、口语化。
───────────────────────────────────────────────
```

> `{probe_section}` 占位符：如果配置了 Probe，则为 `【对话重点】\n{probe}`；如果没有，则为空。

#### 场景对话评分器（Grader）

```
名称: 场景对话评分器
类型: content_and_process
适用形态: scenario
维度: [价值传达, 需求匹配, 异议处理, 信任建立]

提示词:
───────────────────────────────────────────────
你是一位场景对话效果评审专家。请评估以下对话过程中，内容作为对话工具的有效性。

注意：你评估的是**内容能否支撑对话目标**，不是对话技巧本身。

【被评估内容（销售代表可用的素材）】
{content}

【销售对话过程】
{process}

【评估维度】
1. 价值传达 (1-10): 内容的核心价值是否被有效传达给买方？
   - 9-10: 买方清楚地理解了产品的核心价值
   - 5-6: 价值传达不够清晰，买方仍有模糊感
   - 1-4: 买方始终不理解为什么需要这个产品

2. 需求匹配 (1-10): 是否成功将买方需求和内容优势匹配？
   - 9-10: 精准匹配，买方感觉"这就是我要的"
   - 5-6: 部分匹配，但有重要需求未被覆盖
   - 1-4: 严重错位

3. 异议处理 (1-10): 买方的顾虑/异议是否被内容中的事实有效回应？
   - 9-10: 所有异议都用具体事实化解
   - 5-6: 部分异议被回应，但核心异议未解决
   - 1-4: 异议被忽略或回应无力

4. 信任建立 (1-10): 整个过程是否建立了买方对产品的信任？
   - 9-10: 买方表达了明确的信任和购买意向
   - 5-6: 买方态度中性，既不信任也不排斥
   - 1-4: 买方表达了不信任或排斥

请严格输出以下 JSON 格式：
{{"scores": {{"价值传达": 分数, "需求匹配": 分数, "异议处理": 分数, "信任建立": 分数}}, "comments": {{"价值传达": "评语", "需求匹配": "评语", "异议处理": "评语", "信任建立": "评语"}}, "key_moments": ["对话中的关键转折点"], "unresolved_concerns": ["对话结束时仍未解决的顾虑"], "feedback": "整体评价和改进建议（150-250字）"}}
───────────────────────────────────────────────
```

### 6.5 跨 Trial 分析（Task 级）提示词

```
名称: 跨 Trial 分析提示词
适用: Task 级，在一个 Task 的所有 Trial 执行完后可选触发

系统提示词:
───────────────────────────────────────────────
你是一位评估分析专家。以下是同一个评估任务（Task）下多次执行（Trial）的结果汇总。

请分析这些 Trial 结果，找出共性模式和可操作的改进建议。

【Task 信息】
{task_name}
{task_description}

【所有 Trial 结果】
{all_trial_results}

请完成以下分析：

1. **共性模式**：哪些问题/发现在多个 Trial 中反复出现？
   - 如果多个 Trial 都在同一个点上给出低分或负面反馈，那是系统性问题
   - 如果只在一个 Trial 中出现的问题，可能是个别情况

2. **逐条修改建议**：按严重程度排序，每条建议都要：
   - 明确指出问题所在（引用具体 Trial 的证据）
   - 给出具体的修改方向（不是"建议改进"这种空话）
   - 标注严重程度（high/medium/low）

请以JSON格式输出：
{{"patterns": [{{"title": "共性问题标题", "frequency": "出现在几个 Trial 中", "evidence": ["Trial 1 中的具体表现", "Trial 2 中的具体表现"], "severity": "high/medium/low"}}], "suggestions": [{{"title": "修改建议标题", "severity": "high/medium/low", "detail": "具体修改方向（50-100字）", "related_patterns": ["关联的共性问题标题"]}}], "strengths": ["所有 Trial 共同认可的优点"], "summary": "一段话总结（100字以内）"}}
───────────────────────────────────────────────
```

> **和旧"综合诊断"的区别**：旧版是跨 Task 分析（内容可能已变、聚合无意义），新版只分析**一个 Task 内**的 Trial。输出的 `suggestions` 每一条都可以通过 [🤖 让 Agent 修改] 按钮发送到 Agent 面板。

---

## 七、后台设置

### 7.1 设置页面扩展

在后台设置页面（`/settings`）中，增加"评估提示词"Tab：

```
现有 Tab:
  📝 传统流程提示词 | 👤 创作者画像 | 📋 字段模板 | 🔄 阶段模板
  📢 渠道管理 | 🎭 模拟器 | 📊 评分器 | 🤖 Agent | 📄 日志

改为:
  📝 传统流程提示词 | 👤 创作者画像 | 📋 字段模板 | 🔄 阶段模板
  📢 渠道管理 | 🎭 模拟器 | 📊 评分器 | 🧪 评估提示词 | 🤖 Agent | 📄 日志
                                          ↑ 新增
```

> **人物画像（含审查角色）不在后台设置页面管理**。
> 所有角色（消费者、审查角色、自定义角色）统一在项目级 `eval_persona_setup` ContentBlock（画像 Tab）管理。
> 系统预装的策略教练/编辑/专家角色会在创建评估阶段时自动添加到画像中。
> 详见 **5.2 画像 Tab** 和 **R2**。
> 这里的"评估提示词"只管理**流程模板提示词**（消费体验三步流程、场景模拟、跨Trial分析、Grader 模板）。

### 7.2 评估提示词管理 Tab

> **审查角色（策略教练/编辑/专家等）不在这里管理**——它们统一在项目级画像 Tab（5.2）中配置。
> 这里只管理**通用的评估流程提示词模板**（每种形态的执行模板）。

```
┌──────────────────────────────────────────────────────────────┐
│  🧪 评估提示词管理                                            │
│  管理评估流程中使用的提示词模板。                              │
│  角色画像的提示词在项目级「人物画像」Tab 管理。                │
│                                                              │
│  Tab: [消费体验] [场景模拟] [跨Trial分析] [Grader模板]       │
│                                                              │
│  ─── 消费体验（分块探索三步流程）───                          │
│                                                              │
│  ┌──────────────────────────────────────────────────────┐   │
│  │  📋 探索规划提示词（默认）          [预置] [编辑]      │   │
│  │  用途: 消费体验步骤1——制定探索计划                     │   │
│  │  占位符: {persona} {probe_section} {block_list}       │   │
│  └──────────────────────────────────────────────────────┘   │
│                                                              │
│  ┌──────────────────────────────────────────────────────┐   │
│  │  🔍 逐块探索提示词（默认）          [预置] [编辑]      │   │
│  │  用途: 消费体验步骤2——逐内容块评价                     │   │
│  │  占位符: {persona} {probe_section} {exploration_memory}│   │
│  │          {block_title} {block_content}                 │   │
│  └──────────────────────────────────────────────────────┘   │
│                                                              │
│  ┌──────────────────────────────────────────────────────┐   │
│  │  📝 探索总结提示词（默认）          [预置] [编辑]      │   │
│  │  用途: 消费体验步骤3——综合评价                         │   │
│  │  占位符: {persona} {probe_section} {all_block_results} │   │
│  └──────────────────────────────────────────────────────┘   │
│                                                              │
│  ─── 场景模拟 ───                                            │
│  （角色 A / 角色 B 的对话模板提示词）                         │
│                                                              │
│  ─── 跨 Trial 分析 ───                                       │
│  （Task 级跨 Trial 分析模板提示词）                           │
│                                                              │
│  ─── Grader 模板 ───                                         │
│  （各形态默认 Grader 的提示词模板）                           │
│                                                              │
└──────────────────────────────────────────────────────────────┘
```

### 7.3 提示词编辑器（通用组件）

每个提示词的编辑界面统一为：

```
┌──────────────────────────────────────────────────────────────┐
│  编辑提示词: 策略教练                                         │
│                                                              │
│  名称: [策略教练                              ]              │
│  描述: [资深策略顾问视角，关注方向而非执行    ]              │
│                                                              │
│  占位符说明:                                                  │
│  {content} = 被评估内容  {focus} = 审查焦点  {persona} = 画像 │
│  {probe} = 测试焦点  {process} = 互动过程  {max_turns} = 轮数 │
│                                                              │
│  提示词:                                                      │
│  ┌────────────────────────────────────────────────────────┐  │
│  │ 你是一位资深的内容策略教练。你带过上百个内容创作者，   │  │
│  │ 见过无数"方向正确但执行偏了"和"执行精良但方向错误"  │  │
│  │ 的案例。                                               │  │
│  │ ...                                                    │  │
│  └────────────────────────────────────────────────────────┘  │
│                                                              │
│  [🤖 AI 生成提示词]  [📋 重置为默认]                         │
│                                                              │
│  占位符检查:                                                  │
│    ✅ {content} 已包含                                       │
│    ✅ {focus} 已包含                                         │
│    ⚠️ 建议包含 JSON 输出格式说明                              │
│                                                              │
│         [取消]                              [保存]           │
└──────────────────────────────────────────────────────────────┘
```

### 7.4 AI 生成提示词

> **只有"AI 生成"，没有"AI 优化"。** 用户要的是从零生成，不是"帮我改"。

```python
# 后端 API: POST /api/eval/prompts/generate
async def generate_prompt(request):
    """
    统一的 AI 生成提示词接口
    输入: prompt_type, context (形态、角色描述、维度、项目背景等)
    输出: generated_prompt
    """
    meta_prompt = f"""你是一位 AI 提示词工程专家。
请为以下评估场景生成一个专业的提示词。

【提示词类型】{prompt_type_name}
【评估形态】{form_type_name}
【角色/场景描述】{description}
【项目背景（如有）】{project_context}

请生成一个完整的提示词，包含：
1. 清晰的角色身份定义
2. 具体的评估标准或行为要求
3. 如果是评分类，包含评分锚点（1-10分的具体标准）
4. 结构化 JSON 输出格式
5. 必要的占位符: {required_placeholders}"""
```

```python
# 后端 API: POST /api/eval/personas/generate
async def generate_persona(request):
    """
    AI 生成人物画像
    输入: project_id (用于获取项目意图和调研内容)
    输出: { name: str, prompt: str }
    """
    meta_prompt = f"""根据以下项目信息，生成一个有代表性的人物画像。

【项目意图】
{project_intent}

【已有画像（避免重复）】
{existing_persona_names}

【消费者调研摘要（如有）】
{research_summary}

请生成一个具体的人物画像，输出 JSON:
{{"name": "姓名", "prompt": "完整的人物描述提示词（包含身份、背景、性格、需求、顾虑等）"}}"""
```

---

## 八、API 设计

### 8.1 API 端点

```
# ---- Task CRUD ----
GET    /api/eval/tasks/{project_id}              # 获取项目的所有 task
POST   /api/eval/tasks/{project_id}              # 创建 task
PUT    /api/eval/task/{task_id}                   # 更新 task
DELETE /api/eval/task/{task_id}                   # 删除 task

# ---- 执行 ----
POST   /api/eval/task/{task_id}/execute           # 执行单个 task（生成 repeat_count 个 trial）
POST   /api/eval/tasks/{project_id}/execute-all   # 执行所有 task
POST   /api/eval/task/{task_id}/diagnose            # 运行 Task 级跨 Trial 分析

# ---- 结果查看 ----
GET    /api/eval/task/{task_id}/trials            # 获取 task 的所有 trial
GET    /api/eval/task/{task_id}/latest             # 获取 task 的最新 batch 结果
GET    /api/eval/tasks/{project_id}/report         # 获取报告数据（所有 task 的最新结果列表）
GET    /api/eval/task/{task_id}/diagnosis            # 获取 Task 的跨 Trial 分析结果

# ---- AI 生成（只有生成，没有优化） ----
POST   /api/eval/prompts/generate                 # AI 生成提示词
POST   /api/eval/personas/generate                # AI 生成人物画像

# ---- 人物画像 CRUD ----
GET    /api/eval/personas/{project_id}            # 获取项目可用的画像
POST   /api/eval/personas/{project_id}            # 创建画像
PUT    /api/eval/persona/{persona_id}             # 更新画像
DELETE /api/eval/persona/{persona_id}             # 删除画像

# ---- 配置 ----
GET    /api/eval/config                           # 获取可用的形态/角色/评分器列表
GET    /api/eval/presets/{form_type}              # 获取某形态的预置提示词
```

### 8.2 关键 API 数据格式

**POST /api/eval/tasks/{project_id}** 请求体（以消费体验为例）：

```json
{
  "name": "张晨-定价疑虑",
  "form_type": "experience",
  "target_block_ids": [],
  "grader_ids": ["grader_uuid_1"],
  "repeat_count": 3,
  "form_config": {
    "persona_id": "persona_uuid_1",
    "persona_prompt": "你是张晨，28岁，互联网产品经理，工作3年。你最近在考虑职业转型到数据分析方向。你的核心顾虑是：课程3999的定价是否值得？你担心学完之后实际工作中用不上。",
    "probe": "测试张晨能否从内容中找到关于课程定价合理性的说服力论据",
    "max_turns": 5,
    "consumer_prompt": "",
    "representative_prompt": ""
  }
}
```

**GET /api/eval/tasks/{project_id}/report** 响应：

> **注意：没有 project_score 和 form_scores** — 分数只到 Task 级别。

```json
{
  "tasks": [
    {
      "id": "task_uuid",
      "name": "内容质量检查",
      "form_type": "assessment",
      "status": "completed",
      "is_stale": false,
      "latest_scores": {
        "overall": { "mean": 8.2, "std": 0.3 },
        "dimensions": {
          "结构合理性": { "mean": 8.5, "std": 0.2 },
          "语言质量": { "mean": 8.0, "std": 0.5 },
          "信息准确性": { "mean": 8.5, "std": 0.1 },
          "可读性": { "mean": 7.8, "std": 0.4 }
        }
      },
      "trial_count": 3,
      "last_executed_at": "2026-02-18T10:30:00Z"
    },
    {
      "id": "task_uuid_2",
      "name": "张晨-定价疑虑",
      "form_type": "experience",
      "status": "completed",
      "is_stale": false,
      "latest_scores": {
        "overall": { "mean": 6.8, "std": 0.5 },
        "dimensions": {
          "需求匹配度": { "mean": 6.5, "std": 0.8 },
          "信息完整性": { "mean": 7.0, "std": 0.5 },
          "价值感知": { "mean": 6.2, "std": 1.0 },
          "体验流畅性": { "mean": 7.5, "std": 0.3 }
        }
      },
      "trial_count": 3,
      "last_executed_at": "2026-02-18T11:00:00Z"
    }
  ],
  "diagnosis": {
    "readiness": "needs_work",
    "readiness_summary": "定价内容需加强，其余基本达标",
    "findings": [...]
  }
}
```

---

## 九、实施计划

### Phase 1：数据模型与基础 API（2-3天）

- [x] 新增 `EvalTaskV2` / `EvalTrialConfigV2` / `EvalTrialResultV2` / `TaskAnalysisV2` 模型（并行新链路，独立表）
- [x] 实现 Task CRUD API（`/api/eval/tasks/{project_id}`、`/api/eval/task/{task_id}/v2`、`/api/eval/task/{task_id}`）
- [x] 实现 Task 独立执行 API（`/api/eval/task/{task_id}/execute`，支持 TrialConfig × repeat_count）
- [x] 实现批量执行 API（`/api/eval/tasks/{project_id}/execute-all`）
- [x] 实现结果查询 API（`/api/eval/task/{task_id}/trials`、`/latest`、`/api/eval/tasks/{project_id}/report`）
- [x] 实现内容 hash 计算工具与聚合评分工具（`eval_v2_service.py`）
- [x] Persona 独立 CRUD API（`POST /api/eval/personas/{project_id}`、`PUT/DELETE /api/eval/persona/{persona_id}`，持久化到 `eval_persona_setup` ContentBlock）
- [x] 数据迁移脚本（旧 EvalRun/EvalTask → 新结构，支持 dry-run 与幂等）

### Phase 2：评估引擎重构（2-3天）

- [x] 新增 V2 执行编排：按 `TrialConfig.form_type` 分发执行（`assessment/review/experience/scenario`）
- [x] 实现 Grader 独立执行（Assessment 直接判定仅跑 Grader）
- [x] 实现 Probe 注入（在 `system_prompt` 中注入本次焦点）
- [x] 实现统一评分体系（grader 权重 + task 级 mean/std/min/max 聚合）
- [x] 实现 Task 级跨 Trial 分析 API（`/api/eval/task/{task_id}/diagnose` + `/diagnosis`）
- [x] Experience 三步分块探索提示词与完整流程模板化（已落地独立执行器）
- [x] 全量预置 Grader/Simulator 模板与后台配置同步（新增幂等同步接口 + 设置页入口）

### Phase 3：前端面板（3-4天）

- [x] 重写 `eval-phase-panel.tsx`：三 Tab 架构（画像 Tab + 配置 Tab + 报告 Tab）
- [x] 画像 Tab：接入 `eval_persona_setup`（复用现有编辑器）
- [x] 配置 Tab：扁平 Task 列表（V2 API）+ [运行] + Modal 编辑 Task 与 Trial
- [x] 报告 Tab：扁平 Task 列表 + 点击进入弹窗看板（最新执行）
- [x] 看板弹窗：总分(数字) + Trial 详情 + Grader 反馈拆条 [让Agent修改]
- [x] 看板底部：Task 级跨 Trial 分析 + 每条建议 [让Agent修改]
- [x] 实现"添加 Task"Modal（Task 名称 + 第一个 Trial 配置）
- [x] 复用 suggestion_card_design.md M3 桥接机制（`onSendToAgent`）
- [x] 评估结果仅通过 DB/API 展示，不新增目录树节点
- [x] 报告页升级为“执行记录级扁平列表”（同 Task 多批次并排展示）
- [x] Persona Tab 增加 AI 生成功能按钮

### Phase 4：后台设置与 AI 辅助（1-2天）

- [x] 新增"评估提示词"Tab 到后台设置页（人物画像在项目级画像 Tab 管理，不在后台设置）
- [x] 实现提示词编辑器组件（含占位符检查）
- [x] 实现 AI 生成提示词 API（统一接口，按 prompt_type 分发）
- [x] 实现 AI 生成画像 API
- [x] 为所有提示词编辑器添加 [🤖 AI 生成] 按钮

### Phase 5：收尾（1天）

- [x] 清除旧代码（前端旧 `eval-panel.tsx` 已移除，旧 EvalRun 前端入口已下线；后端 EvalRun API 暂保留兼容层）
- [x] 端到端测试（每种形态至少跑一次完整流程，API级E2E已覆盖 assessment/review/experience/scenario）
- [x] 更新 `eval_system_design.md` 和 `first_principles.md`

**预估总工时：9-13 天**

---

## 十、穷尽检查清单

### 10.1 是否闭环？

| 环节 | 是否覆盖 | 说明 |
|------|---------|------|
| 定义评估目标 | ✅ | 五种形态覆盖所有评估子问题 |
| 配置评估任务 | ✅ | 配置 Tab：按形态分组，每种有独立配置界面 |
| 执行评估 | ✅ | 单 Task [▶] 独立执行 + [▶ 全部运行] |
| 多次执行聚合 | ✅ | repeat_count + 统计聚合（mean ± std） |
| 分数计算 | ✅ | **两层**聚合（trial → task），无 form/project 层 |
| 结果展示 | ✅ | 报告 Tab：Task 列表 → 点击展开看板 |
| 过期检测 | ✅ | content_hash 变化 → stale + 灰色显示 |
| 增量执行 | ✅ | 新增 Task 不影响已有 Task 结果 |
| 提示词可编辑 | ✅ | 角色提示词在画像 Tab 可编辑；流程模板在后台"评估提示词"Tab 可编辑 |
| AI 辅助 | ✅ | AI 生成提示词 + AI 生成画像（无优化/无建议Probe） |
| 人物画像管理 | ✅ | 画像 Tab 统一管理所有角色（含预装审查角色），单提示词框，AI 可生成 |
| ContentBlock 集成 | ✅ | eval_persona_setup + eval_task_config + eval_report 三个 ContentBlock |
| 不增加目录树内容块 | ✅ | 执行结果只在报告块内展示，不创建新 ContentBlock |
| 评估→行动 | ✅ | Trial 级 Grader 建议 + Task 级跨 Trial 分析建议均附 [🤖 让Agent修改] 按钮（→ suggestion_card_design.md M3） |

### 10.2 所有提示词是否穷尽列出？

| 提示词 | 形态 | 管理位置 | 可编辑 | AI 可生成 |
|--------|------|---------|--------|----------|
| 内容质量评分器 prompt | Assessment | 评分器设置 | ✅ | ✅ |
| 策略对齐评分器 prompt | Assessment | 评分器设置 | ✅ | ✅ |
| 策略教练 system prompt | Review | **画像 Tab（预装角色）** | ✅ | ✅ |
| 资深编辑 system prompt | Review | **画像 Tab（预装角色）** | ✅ | ✅ |
| 领域专家 system prompt | Review | **画像 Tab（预装角色）** | ✅ | ✅ |
| 探索规划提示词 | Experience | 评估提示词 > 消费体验 | ✅ | ✅ |
| 逐块探索提示词 | Experience | 评估提示词 > 消费体验 | ✅ | ✅ |
| 探索总结提示词 | Experience | 评估提示词 > 消费体验 | ✅ | ✅ |
| 消费者体验评分器 prompt | Experience | 评分器设置 | ✅ | ✅ |
| 角色 A system prompt | Scenario | 评估提示词 > 场景模拟 | ✅ | ✅ |
| 角色 B system prompt | Scenario | 评估提示词 > 场景模拟 | ✅ | ✅ |
| 场景对话评分器 prompt | Scenario | 评分器设置 | ✅ | ✅ |
| 跨 Trial 分析提示词 | Task 级功能 | 评估提示词 > 跨 Trial 分析 | ✅ | ✅ |
| AI 生成提示词 meta prompt | 辅助 | 代码内 | — | — |
| AI 生成画像 meta prompt | 辅助 | 代码内 | — | — |

### 10.3 每种形态是否有默认 Grader？

| 形态 | 默认 Grader | 默认维度 |
|------|------------|---------|
| Assessment | 内容质量评分器 + 策略对齐评分器 | 结构/语言/准确/可读 + 意图/受众/差异化/完整 |
| Review | （审查角色出定性反馈，可附加 Grader） | 由角色决定 |
| Experience | 消费者体验评分器 | 需求匹配/信息完整/价值感知/内容结构 |
| Scenario | 场景对话评分器 | 价值传达/需求匹配/异议处理/信任建立 |
| 跨Trial分析 | （非形态，Task 级功能，自身输出建议不打分） | 共性模式 + 逐条建议 |

### 10.4 用户反馈对照

| 用户反馈 | 是否已纳入 | 如何体现 |
|---------|-----------|---------|
| 分数不需要 task 以上 | ✅ | 两层评分，无 form/project 聚合 |
| 只要 AI 生成，不要优化/建议 Probe | ✅ | 只有 generate API，无 optimize/suggest |
| AI 生成画像保留 | ✅ | POST /api/eval/personas/generate |
| 人物画像设置不限于消费者 | ✅ | 画像 Tab 统一管理所有角色（含审查角色预装） |
| 审查角色也在人物画像里配置 | ✅ | 预装策略教练/编辑/专家 + 可编辑/删除/新增（R2） |
| task 是报告 block 里的块 | ✅ | 报告 Tab 中每个 Task 是一个块 |
| 点开看板展示所有 trial 分数 | ✅ | 看板：Task 总分 + 维度 + 各 Trial 详情 |
| 配置在配置页，报告在报告页 | ✅ | 三 Tab 架构（画像 + 配置 + 报告） |
| 不增加目录树内容块 | ✅ | 结果只存 DB，在报告 Tab 内展示 |
| 沉淀在 ContentBlock 模板中 | ✅ | eval_persona_setup + eval_task_config + eval_report 三个 ContentBlock |
| 消费体验非单次调用，分块评价 | ✅ | 三步流程：探索规划 → 逐块评价 → 总结评价（R3） |
| 跨 Trial 分析找模式+逐条建议 | ✅ | Task 级 [🔍 生成跨 Trial 分析]，每条可 → Agent（R5） |
| eval→Agent 闭环 | ✅ | Trial 级 Grader 建议 + Task 级分析建议均可 [🤖 让Agent修改]（5.4a） |

---

## 十一、和旧系统的差异对照

| 维度 | 旧系统 | 新系统 |
|------|--------|--------|
| 组织方式 | EvalRun → EvalTask（按 Simulator 类型） | Project → EvalTask（按评估目标/形态） |
| 前端架构 | 单一面板混合配置和结果 | 三 Tab：画像 Tab + 配置 Tab + 报告 Tab |
| 执行方式 | 全量执行所有 trial | 单 Task 独立执行 + 批量执行 |
| Grader 定位 | 绑定在 Simulator 后面 | 独立一等公民，可单独执行（直接判定） |
| Probe/焦点 | 不存在 | 所有形态可选，让评估更聚焦可复现 |
| 多次执行 | 不支持 | repeat_count + 统计聚合 |
| 分数体系 | Trial 平均分 → Run 总分 | **只到 Task 级**（trial → task），无 form/project 层 |
| 过期检测 | 不支持 | content_hash 自动检测 |
| 角色管理 | "消费者画像"独立 + 审查角色散落 | **统一画像管理**：消费者/审查角色/自定义角色都在画像 Tab，预装教练/编辑/专家 |
| 消费体验 | 消费者↔内容方对话 / 单次 LLM 探索 | **分块探索**：规划 → 逐块评价 → 总结（多次 LLM 调用） |
| 场景模拟 | 含"转化成功/失败"二元结果 | 只产生对话证据，评分全部交 Grader |
| 提示词管理 | 部分可编辑 | **所有**提示词可编辑 + AI 生成 |
| AI 辅助 | 无 | AI 生成提示词 + AI 生成画像 |
| 跨 Trial 分析 | 简单的跨 trial 分析 | Task 级：找共性模式 + 逐条修改建议，每条可 [🤖 让Agent修改] |
| 评估→修改闭环 | 无 | Trial 级 Grader 建议 + Task 级分析建议均可一键发送到 Agent 面板（参见 suggestion_card_design.md M3） |
| 结果存储 | 部分作为 ContentBlock | 结果存 DB，在报告 Tab 内展示，不增加目录树 |
| ContentBlock | 不明确 | eval_persona_setup + eval_task_config + eval_report 三个 ContentBlock |

---

## 十二、第二轮反馈分析与修改计划

> **时间**：2026-02-18
> **触发**：用户第二轮详细反馈（11 大项，14 处结构性问题）
> **策略**：先全部分析 → 讨论争议点 → 逐项修改主文档 → 每项打勾

### 12.0 根本性认知纠正

**当前文档的核心错误**：把 `form_type`（评估形态）放在了 Task 级别，导致一个 Task 内所有 Trial 只能是同一种形态。

**用户要求的正确模型**：

```
EvalTask（命名容器，代表一个评估目标）
  ├── TrialConfig A  (form=assessment, grader=内容质量, repeat=1)
  ├── TrialConfig B  (form=experience, persona=张晨, repeat=3)
  └── TrialConfig C  (form=scenario, persona=王芳, repeat=2)
      ↑ 每个 Trial 独立配置形态、组件、重复次数
```

执行一个 Task = 运行所有 TrialConfig × 各自的 repeat_count → 产生一批 TrialResult

**这意味着文档第三节到第九节都需要围绕这个新模型重写。**

### 12.1 逐项分析

#### R1：所有 Probe 改为可选

**用户原话**：所有的 probe 都是可选，如果没有填写 probe，则直接调用所有的提示词即可。

**当前文档问题**：2.4 消费体验和 2.5 场景模拟标注 Probe 为 **必填**。

**修改方案**：
- 所有形态的 Probe 统一改为可选
- 无 Probe 时：提示词中 `{probe}` 替换为空字符串或通用探索指令
- 有 Probe 时：注入到提示词中，让评估更聚焦

**影响范围**：第二节形态总览表、2.4、2.5、第六节提示词模板

---

#### R2：所有角色统一放在人物画像中（含审查角色）

**用户反馈**：后台目前没有设置审查角色的地方。希望都放在人物画像里，可以有几个默认加载的，但必须要能够在人物画像配置和修改。

**当前文档问题**：将画像和审查角色分开管理（画像在项目级 ContentBlock，审查角色在后台设置），用户认为后台没有这个入口，应统一。

**修正后的理解**：**所有用于评估的角色/人物——无论是消费者、买家、还是编辑、教练、专家——都在人物画像（`eval_persona_setup` ContentBlock）中统一管理。**

| 角色类型 | 示例 | 在人物画像中的形态 | 来源 |
|---------|------|------------------|------|
| 消费者 | 张晨（28岁产品经理） | 用户自建 / 从调研提取 / AI 生成 | research / manual / AI |
| 买家 | 王芳（35岁培训负责人） | 同上 | 同上 |
| 编辑 | 资深编辑 | **系统默认预装** + 可编辑 | preset |
| 策略教练 | 策略教练 | **系统默认预装** + 可编辑 | preset |
| 领域专家 | 领域专家 | **系统默认预装** + 可编辑 | preset |
| 自定义角色 | 严厉的竞品分析师 | 用户自建 | manual |

**默认预装机制**：
- 项目创建评估阶段时，`eval_persona_setup` 自动预装 3 个角色：资深编辑、策略教练、领域专家
- 这些预装角色的提示词和自建画像一样——**都是单个提示词框**
- 用户可以修改、删除预装角色，也可以添加更多
- `source` 字段标记来源：`"preset"` / `"research"` / `"manual"` / `"ai"`

**影响范围**：
- 5.2 画像 Tab：增加预装角色说明
- 5.3 配置 Tab：Trial 配置中"选择角色"统一从画像列表选择，不区分"选画像"和"选审查角色"
- 7.1 后台设置：不再需要单独的"评估提示词>审查角色"管理
- 6.2 审查角色提示词：改为画像的默认预装提示词

---

#### R3：消费体验改为探索式——分块阅读 + 逐块评价（非对话，非单次调用）

**用户原话**：消费体验测试，我希望和内容场景模拟不同，这种更像是过去版本中的那种探索式模拟器，消费者自行对内容进行"探索"并输出探索结果，而不是和"内容代表"对话。后者没有任何意义。
**补充**：不是单次调用！是用户自定义看内容的哪些部分，然后分块评价和自己的关切是否一致。

**当前文档问题**：2.4 消费体验设计为消费者↔内容方的多轮对话。

**已有实现参考**：`eval_engine.py` 的 `_run_exploration`（345-573行）已实现探索模式的基础版本：
- 消费者根据自身痛点制定探索计划（先看什么、再看什么）
- 逐步记录探索过程（每步有 action/reason/finding/feeling）
- 输出结构化结果（注意力焦点、困难、缺失信息、是否找到答案）
- 最后由 Grader 评分

**修改方案——基于已有实现升级**：

消费体验 = **分块探索**，核心区别于场景模拟（对话）：
1. **没有"内容代表"角色**，删除 `representative_prompt`
2. **内容按 ContentBlock 分块**（用户在配置时通过 `target_block_ids` 指定要探索哪些内容块）
3. **多次 LLM 调用**（非单次）——逐块评价：

**执行流程**：
```
第一步：探索规划（1 次 LLM 调用）
  输入：persona + probe(可选) + 内容块列表（只有标题/摘要）
  输出：探索计划（先看哪个块、为什么、期望找到什么）

第二步：逐块探索（N 次 LLM 调用，每个内容块一次）
  输入：persona + probe(可选) + 该块完整内容 + 前面块的探索记忆
  输出：{
    "关切匹配度": "这部分是否回应了我的关切？怎么回应的？",
    "发现": "有什么有价值的信息？",
    "疑虑": "什么地方让我不确定或失望？",
    "missing": "我期望看到但没看到的内容",
    "feeling": "作为 [persona]，读完这部分的真实感受",
    "score": 1-10
  }

第三步：总结评价（1 次 LLM 调用）
  输入：persona + 所有块的探索结果
  输出：{
    "overall_impression": "总体印象",
    "key_concerns_addressed": ["被回应的关切"],
    "key_concerns_unaddressed": ["未被回应的关切"],
    "would_recommend": true/false,
    "summary": "作为 [persona]，总体评价"
  }

第四步：Grader 评分（可选，M 次 LLM 调用）
  输入：content + 探索结果
  输出：各维度分数 + 改进建议
```

**为什么必须多次调用**：
- 单次调用把所有内容塞进去，LLM 对后面的内容关注度会降低
- 分块评价让每个块都得到公平、深入的评估
- 逐块的"关切匹配度"是这个形态最核心的价值——用户能看到"第三章完全没回应定价问题"
- 探索记忆让后续块的评价有上下文（"前面已经看过 XX，现在看这个补充了什么"）

**和已有 `_run_exploration` 的关系**：
- 已有实现是"单次 LLM 输出完整探索"，升级为"真正的多步探索"
- 保留探索计划 + 逐步记录的结构
- 新增：按 ContentBlock 分块、逐块独立评价、探索记忆传递

**影响范围**：2.4 形态定义重写、6.3 提示词全部重写（消费者探索提示词 × 3 步 + 删除内容方提示词）、6.3 体验评分器（调整为评探索结果）、4.2 消费体验评分计算、TrialResult 的 process 结构

---

#### R4：场景模拟 = 对话式，每次发言 ≤50 字

**用户原话**：场景模拟比较明确就是一个对话式的模拟，要设置的是两个角色的提示词。双方对话中，每次说话不得超过50字。

**修改方案**：
- 场景模拟是**唯一的对话式形态**
- 两个角色：角色 A + 角色 B（不限于"卖方/买方"，可以是任何场景双方）
- **每次发言限制 ≤50 字**（在提示词中强调）
- 需要配置：角色 A 提示词、角色 B 提示词、最大轮数

**影响范围**：2.5 形态定义、6.4 卖方/买方提示词（改为角色 A/B + 50 字限制）

---

#### R5：诊断的位置和意义（⚠️ 需讨论）

**用户原话**：改进建议，我在想是只能在综合诊断这里提出还是也可以在所有 trial 的报告中提出。我觉得其实在 trial 中提出要更加明确一些。如果是这样，综合诊断的意义是什么？

**我的分析——三层改进建议的定位**：

| 级别 | 来源 | 性质 | 示例 |
|------|------|------|------|
| Trial 级 | 每个 Grader 的 `feedback` 字段 | **具体、可操作**——基于一次具体评估 | "第三章定价论述缺少竞品对比数据" |
| Task 级 | 跨 Trial 模式分析 | **模式发现**——多个 Trial 的共性问题 | "4 次消费者测试中 3 次在定价段卡住" |
| Report 级 | 跨 Task 元分析 | ~~系统性判断~~ | 用户已否定（跨 Task 内容可能已变） |

**用户确认：方案 B——Trial 级建议 + Task 级跨 Trial 分析** ✅

最终方案：

**① Trial 级**：每个 Grader 的 `feedback` 必须包含具体的改进建议
- 最有价值、最可操作
- 每条建议都有明确的评估来源
- 每条建议旁有 [🤖 让Agent修改] 按钮

**② Task 级**：跨 Trial 分析（找模式 + 逐条修改建议）
- 在 Task 报告块底部有 [🔍 生成跨 Trial 分析] 按钮
- 点击后对该 Task 的所有 Trial 结果做一次 LLM 分析
- 输出：
  - 反复出现的共性问题（如"3 次消费者测试都在定价部分卡住"）
  - 按严重程度排序的**逐条修改建议**
  - 每条建议旁有 [🤖 让Agent修改] 按钮 → 可一键发到 Agent 面板
- 非强制，用户自行决定是否需要

**③ Report 级综合诊断 → 移除**
- 跨 Task 的数值聚合和诊断都不做
- 理由不变：跨 Task 内容可能已变，聚合无意义

**影响范围**：
- 2.6 综合诊断形态 → 简化为"Task 级跨 Trial 分析"的功能描述
- 3.4 EvalDiagnosis 模型 → 改为 TaskAnalysis，挂在 Task 下
- 5.4 报告 Tab → 移除底部的"综合诊断"区域，诊断入口改为 Task 块底部
- 5.4a Agent 桥接 → 调整：按钮位置从"综合诊断"+"跨 Trial 发现"改为"Trial 级 Grader 建议"+"Task 级分析建议"

---

#### R6：UI 必须干净——弹窗而非内联展开

**用户原话**：三个内容块的模板的 UI 一定要干净，不要有花里胡哨的东西。每个卡片可以做成弹窗而不是直接在卡片下面展开，收起来都不好收。

**修改方案**：
- 配置页：Task 列表是干净的行列表，点击 Task → **弹窗**编辑 Task 及其 Trial
- 报告页：Task 是干净的行列表，点击 Trial → **弹窗**展示详情
- 画像页：画像是干净的行列表，点击画像 → **弹窗**编辑
- **所有详情/编辑 = Modal 弹窗**，不在列表中内联展开
- 极简风格：减少图标、减少边框装饰、干净的间距

**影响范围**：5.2、5.3、5.4、5.5 所有 UI 示意图重画

---

#### R7：核心结构重构——Trial 有 form_type，Task 是容器

**用户原话**：每次 trial 都可以自己设置到底是哪种 form。我要的是一个 task 里可以有多个 trial，每个 trial 的每个组件都是可以自由组合的！

**新数据模型**：

```python
class EvalTask:
    """评估任务——命名容器，聚合一组 Trial"""
    project_id: str
    name: str                    # "内容综合评估"、"消费者测试"
    description: str = ""
    order_index: int = 0
    status: str = "pending"      # pending | completed | stale
    content_hash: str = ""
    last_executed_at: datetime?

class EvalTrialConfig:
    """Trial 配置——评估的最小可配置单元"""
    task_id: str                 # 属于哪个 Task
    name: str                    # "内容质量检查"、"张晨-定价"
    form_type: str               # assessment | review | experience | scenario
    
    # 通用
    target_block_ids: list = []  # 空=全部
    grader_ids: list = []        # 关联的 Grader
    grader_weights: dict = {}    # {grader_id: float}，空=等权
    repeat_count: int = 1        # 这个 trial 跑几次
    probe: str = ""              # 可选焦点
    order_index: int = 0
    
    # 形态特有（JSON）
    form_config: dict = {
        # assessment: { }  ← 只需 grader，最简单
        # review:     { reviewer_role: str, system_prompt: str }
        # experience: { persona_id?: str, persona_prompt: str }
        # scenario:   { persona_id?: str, persona_prompt: str,
        #               role_a_prompt: str, role_b_prompt: str,
        #               max_turns: int }
    }

class EvalTrialResult:
    """Trial 的一次执行结果"""
    trial_config_id: str
    task_id: str                 # 冗余，方便查询
    project_id: str              # 冗余
    repeat_index: int            # 0-based，同一 TrialConfig 的第几次重复
    
    # 评分结果（纯 Grader 分数，无 Simulator 自评）
    grader_results: list = []
    # [{ grader_id, grader_name, scores: {维度: 分数}, 
    #    comments: {维度: 评语}, feedback: str }]
    overall_score: float? = None
    
    # 过程数据
    process: list = []           # 对话节点（scenario）或探索结果（experience）
    llm_calls: list = []         # 所有 LLM 调用的完整 input/output
    # [{ role: "system"|"user"|"assistant", content: str, 
    #    model: str, tokens_in: int, tokens_out: int }]
    
    tokens_in: int = 0
    tokens_out: int = 0
    cost: float = 0.0
    status: str = "pending"
    error: str = ""
```

**影响范围**：第三节全部重写、第四节重写、第五节全部重写、第八节 API 重写、第九节计划重写

---

#### R8：统一评分模型

**用户原话**：所有 grader 统一分数模型（1-10 或百分制），grader 权重相等但可调。

**修改方案**：
- 所有 Grader 统一输出 **1-10 分**（当前已如此，保持）
- Trial 分 = Σ(grader_weight[i] × grader_avg[i]) / Σ(grader_weight[i])
  - `grader_avg[i]` = mean(该 Grader 各维度分数)
  - 默认所有 grader_weight = 1.0
  - 每个 TrialConfig 的 `grader_weights` 字段可覆盖
- Task 分 = mean(该 Task 所有 TrialResult 的 overall_score)
- **无 Simulator 自评分**——只有 Grader 分数
- **无转化结果分**——场景模拟也只看 Grader 分数

**影响范围**：第四节评分计算全部简化重写

---

#### R9：场景模拟移除转化成功/失败

**用户原话**：场景模拟不要转化成功/失败的计算！没有意义！

**修改方案**：
- 移除 `scenario_outcome`、`converted`、`blocking_reason`
- 移除 `conversion_rate` 指标
- 移除 4.2 中场景模拟的特殊评分公式
- 场景模拟和其他形态一样：纯 Grader 分数

**影响范围**：3.3 TrialResult 模型、4.2 评分计算

---

#### R10：配置页扁平化——不按形态分组

**用户原话**：不要在任务配置页按 form 分类，应该就是一溜的 task，task 里的 trial 的性质之一是 form。

**修改方案**：
```
配置 Tab
  Task "内容综合评估"      [▶ 执行]  [✏️]
  Task "消费者测试"        [▶ 执行]  [✏️]
  Task "销售就绪度"        [▶ 执行]  [✏️]
  [+ 添加任务]
```
点击 [✏️] → 弹窗，里面是 Task 信息 + Trial 列表 + [+ 添加 Trial]

**影响范围**：5.3 配置 Tab 完全重画

---

#### R11：报告页扁平化 + Trial 有 repeat_count + 诊断位置

**用户原话**：
- 不要把每个 task 的多次执行 nest 在一起
- Trial 可以配次数（A 跑 1 次、B 跑 4 次），重复也列出来
- 综合诊断不在列表级别，至少在 Task 级别
- 只有执行过的 Task 出现在报告
- 报告页不能执行（无 ▶）

**修改方案**：
```
报告 Tab
  Task "内容综合评估"  总分: 7.8
    内容质量检查 (assessment, ×1)         8.2
    编辑审查 (review, ×1)                 7.5
    张晨-定价 #1 (experience, repeat 1/3) 6.5
    张晨-定价 #2 (experience, repeat 2/3) 7.0
    张晨-定价 #3 (experience, repeat 3/3) 6.8
    [🔍 生成 Task 摘要]  ← R5 的可选诊断
  
  Task "销售就绪度"  总分: 6.5
    ...
```
点击某个 Trial → **弹窗**展示完整过程 + 所有 LLM 输入输出

**影响范围**：5.4 报告 Tab 完全重画、5.4a Agent 桥接调整

---

#### R12：Trial 详情必须展示所有 LLM 原始提示词

**用户原话**：trial 中的所有过程（输入输出）都应该能看到原始的、LLM 收到的提示词。

**修改方案**：
- `EvalTrialResult.llm_calls` 记录每次 LLM 调用的完整 system prompt + user message + response
- Trial 详情弹窗中有"原始提示词"区域
- 调试和迭代提示词时非常有价值

**影响范围**：R7 的 TrialResult 模型已包含、5.4 Trial 详情弹窗 UI

---

#### R13：添加 Task 流程——第一步是添加 Trial

**用户原话**：添加 task 也是错的。应该第一步是新建 trial。

**修改方案**：
- [+ 添加任务] → 弹窗：
  1. Task 名称（必填）
  2. 第一个 Trial 配置（选形态 → 配置组件）
- 创建后可继续在 Task 编辑弹窗内添加更多 Trial
- 也就是说：创建 Task 的同时至少创建一个 Trial

**影响范围**：5.5 添加 Task 流程完全重写

---

#### R14：后台设置术语修正 + 结构梳理

**用户原话**："字段模板"早就改成"内容块模板"了。"传统流程提示词"和"评分器""模拟器"之间的关系是什么？感觉特别鸡肋？

**代码中的实际 Tab 名称**（已从 `settings/page.tsx` 确认）：

| 文档中写的 | 实际代码中 | 需修正为 |
|-----------|----------|---------|
| 创作者画像 | 创作者特质 | 创作者特质 |
| 字段模板 | 内容块模板 | 内容块模板 |
| 阶段模板 | 流程模板 | 流程模板 |
| Agent | Agent设置 | Agent设置 |
| 日志 | 调试日志 | 调试日志 |

**关于三个"像提示词"的 Tab 的关系**：

| Tab | 职责 | 和 Eval 的关系 |
|-----|------|---------------|
| 传统流程提示词 | 非 Agent 模式下的内容生成提示词 | ❌ 无关 |
| 模拟器 | 定义可复用的角色+交互模式 | ✅ Eval 的 review 和 scenario 用到 |
| 评分器 | 定义可复用的评分标准 | ✅ Eval 的所有形态用到 |

三者不重叠，但用户觉得 Tab 太多且不清晰。**当前不合并**，但在文档中需要清晰说明各 Tab 的职责和关系，尤其是模拟器和评分器在新 Eval 体系中的角色。

**影响范围**：第七节设置页面描述全面修正

---

### 12.2 修改清单

| # | 改动项 | 影响章节 | 状态 |
|---|--------|---------|------|
| R1 | Probe 全部改为可选 | 二(总览表+2.4+2.5)、六(提示词) | ✅ 已完成 |
| R2 | 所有角色统一在画像 Tab | 5.2、6.2 | ✅ 已完成 |
| R3 | 消费体验 → 探索式（非对话） | 2.4、六(提示词)、4.2 | ✅ 已完成 |
| R4 | 场景模拟 = 对话 + 50字限制 | 2.5、六(提示词) | ✅ 已完成 |
| R5 | 诊断放在 Task 级、移除 Report 级 | 2.6、3.4、5.4 | ✅ 已完成 |
| R6 | UI 极简化——弹窗 | 5.2、5.3、5.4、5.5 | ✅ 已完成 |
| R7 | Task=容器, Trial=可配置单元 | 3.1-3.5、八(API)、九(计划) | ✅ 模型已重写 |
| R8 | 统一评分: Grader-only, 可调权重 | 四(全部) | ✅ 已完成 |
| R9 | 移除场景转化成功/失败 | 3.3、4.2 | ✅ 已完成 |
| R10 | 配置页不按形态分组 | 5.3 | ✅ 已完成 |
| R11 | 报告页扁平（无执行记录嵌套） | 5.4、5.4a | ✅ 已完成 |
| R12 | Trial 详情展示 LLM 原始提示词 | 3.3、5.4 | ✅ 模型已包含 |
| R13 | 添加 Task = 先建 Trial | 5.5 | ✅ 已完成 |
| R14 | 后台设置术语修正 | 七(全部) | ⬜ 待处理 |

---

### 12.3 第三轮反馈修正（2026-02-18）

> **触发**：用户对已修改的 5.2、5.4、5.5 提出精确修正

#### F1：画像 Tab 不要分类——扁平列表

**用户原话**：不要区分是预装的还是用户画像，就一溜用户画像排下来即可。

**修正**：
- 移除 5.2 中的"── 预装角色 ──"和"── 用户画像 ──"分隔线
- 所有画像一溜排下来，`source` 标签只是元信息，不影响显示分组
- 已完成 ✅

#### F2：报告页无嵌套——一次执行 = 一条记录

**用户原话**：没有执行记录这一层，只有task这一层，一次执行就是一次task记录，不需要nest在一个task下面！task直接打开就是看板，每个执行记录都列出来。

**修正**：
- 报告 Tab = 扁平列表，每条 = 一次 Task 执行（Task名 + 总分 + 时间）
- 同一 Task 多次执行 → 多条记录（不嵌套在一个 Task 下）
- 点击记录 → **弹窗**看板，展示该次执行的所有 Trial 详情
- 两层结构（扁平列表 → 弹窗看板），不是三层
- 报告页只读，不可执行
- 已完成 ✅

#### F3：分数显示用数字——不要进度条

**用户原话**：这里的显示有点太浮夸了，直接用数字即可，不需要这些bar。

**修正**：
- 看板中的维度得分用纯数字（`需求匹配度 6.5`）而非 `██████▌░░░ 6.5`
- 已完成 ✅

#### F4：每条 Grader 建议 → 独立的让Agent修改按钮

**用户原话**：我想的是每条建议直接对应一个让agent修改，有几条就对应几个，每个点击以后直接出editcard。

**修正**：
- Trial 详情中 Grader 的每条 `建议` 后面各自跟一个 `[🤖 让Agent修改]`
- 点击后直接生成 EditCard 发到 Agent 面板
- 跨 Trial 分析的逐条建议格式不变（已经是每条一个按钮）
- 已完成 ✅

#### F5：添加 Task 流程——第一步是配置 Trial

**用户原话**：第一步不是选择形态！是补充trial！

**修正**：
- 添加任务 Modal：先填 Task 名称，然后直接配置第一个 Trial（选形态 → 配组件）
- 不再有单独的"第一步选形态"卡片页
- 创建后在 Task 编辑弹窗中可继续 [+ 添加 Trial]
- 已完成 ✅

#### F6：配置 Tab 扁平化（顺带修复）

**依据**：R10 要求配置页不按形态分组，但主文档 5.3 此前未更新。

**修正**：
- 配置页改为扁平 Task 列表，每个 Task 显示 Trial 摘要
- 点 [✏️] → 弹窗编辑 Task 信息 + Trial 列表 + [+ 添加 Trial]
- 已完成 ✅

### 12.4 确认后剩余工作

| 项目 | 状态 |
|------|------|
| 第二节（形态定义） | ✅ 已按 R1-R5 更新 |
| 第三节（数据模型） | ✅ 已按 R7 重写 |
| 第四节（评分计算） | ✅ 已按 R8-R9 重写 |
| **第五节（前端面板）** | **✅ 全部更新**（5.2 扁平画像、5.3 扁平配置、5.4 两层报告、5.5 Trial优先添加） |
| 第六节（提示词） | ✅ 已按 R1-R4 更新 |
| 第七节（设置页） | ⬜ **R14 待处理**——术语修正 + Tab 关系说明 |
| 第八节（API） | ✅ 已按 R7 重写 |
| 第九节（实施计划） | ✅ Phase 3 已更新为新结构 |
| suggestion_card_design.md 一致性 | ✅ 已同步 |

**下一步**：处理 R14（第七节后台设置术语修正），然后可以进入实施。

### 12.5 实施进度日志（2026-02-18，第一轮落地）

本轮已完成后端基础落地，策略是“新旧并行，先不破坏旧链路”：

1. 新增 V2 数据模型（独立表）：
   - `EvalTaskV2`（Task 容器）
   - `EvalTrialConfigV2`（最小配置单元）
   - `EvalTrialResultV2`（执行结果）
   - `TaskAnalysisV2`（Task 级跨 Trial 分析）

2. 新增/补齐 V2 API（与旧 API 并存）：
   - `GET/POST /api/eval/tasks/{project_id}`
   - `PUT /api/eval/task/{task_id}/v2`
   - `POST /api/eval/task/{task_id}/execute`
   - `POST /api/eval/tasks/{project_id}/execute-all`
   - `GET /api/eval/task/{task_id}/trials`
   - `GET /api/eval/task/{task_id}/latest`
   - `GET /api/eval/tasks/{project_id}/report`
   - `POST /api/eval/task/{task_id}/diagnose`
   - `GET /api/eval/task/{task_id}/diagnosis`

3. 新增评分与聚合工具：
   - `compute_content_hash`
   - `compute_weighted_grader_score`
   - `aggregate_task_scores`
   - `is_task_stale`

4. 本轮测试与闭环：
   - 新增测试文件：
     - `backend/tests/test_eval_v2_service.py`
     - `backend/tests/test_eval_v2_flow.py`
   - 结果：`6 passed`
   - 覆盖点：
     - hash 稳定性（顺序无关）
     - grader 权重与 task 聚合统计
     - Task 容器 + TrialConfig 重复执行 + latest/report 查询
     - Task 级跨 Trial diagnose 输出结构

### 12.6 实施进度日志（2026-02-18，第二轮落地）

本轮完成 Phase 3 的首版前端主链路改造，并保持对现有架构兼容：

1. `eval-phase-panel.tsx` 重构为三 Tab：
   - 人物画像（接入 `eval_persona_setup`）
   - 任务配置（接入 V2 API）
   - 评估报告（接入 V2 API）

2. 任务配置 Tab（V2）：
   - 扁平 Task 列表
   - [运行]/[编辑]/[删除]
   - “添加任务”Modal（Task 名称 + 第一个 Trial）
   - 支持一个 Task 内增删多个 Trial（form_type / repeat / grader / probe / persona 关联）

3. 评估报告 Tab（V2）：
   - 扁平任务报告列表（展示最新分数）
   - 点击进入弹窗看板查看 Trial 详情
   - Trial 中 Grader 反馈拆分建议，每条可 [让Agent修改]
   - 看板底部支持“生成跨 Trial 分析”，每条建议可 [让Agent修改]

4. M3 桥接复用：
   - 所有“让Agent修改”按钮继续通过 `onSendToAgent` 透传到 Agent 面板
   - 无新增额外桥接协议，保持与现有 Suggestion Card 链路兼容

5. 本轮测试与闭环：
   - 前端：`npm run build` 通过（Next.js 编译 + TS 检查通过）
   - 后端回归：`pytest tests/test_eval_v2_service.py tests/test_eval_v2_flow.py tests/test_simulator_types.py -q`
   - 结果：`19 passed`

### 12.7 实施进度日志（2026-02-18，第三轮落地）

本轮重点推进两件事：Experience 三步探索落地 + 报告页执行记录扁平化。

1. Experience 形态升级为“三步分块探索”：
   - 新增 `backend/core/tools/eval_v2_executor.py`
   - 流程：探索规划 -> 逐块探索（每块一次 LLM）-> 总结评价
   - `api/eval.py` 中 Experience 分支已切换为该执行器
   - 当未配置 Grader 时，回退使用分块探索均值作为试验分

2. 报告 API 升级为执行记录扁平化：
   - 新增 `GET /api/eval/tasks/{project_id}/executions`
   - 新增 `GET /api/eval/task/{task_id}/batch/{batch_id}`
   - `diagnose/diagnosis` 支持 `batch_id` 参数，分析可绑定到指定执行批次

3. 前端报告页同步升级：
   - `evalV2API` 增加 execution/batch/diagnosis(batch) 接口
   - `eval-phase-panel.tsx` 报告 Tab 切换为“执行记录级扁平列表”
   - 点击记录打开 batch 详情看板，建议按钮保持 [让Agent修改] 桥接

4. 本轮测试与闭环：
   - 后端：`pytest tests/test_eval_v2_service.py tests/test_eval_v2_executor.py tests/test_eval_v2_flow.py -q` -> `9 passed`
   - 前端：`npm run build` 通过
   - 新增测试：
     - `test_eval_v2_executor.py`（三步流程 + 分块均值）
     - `test_eval_v2_flow.py` 增补执行记录扁平化与 batch 详情测试

### 12.8 实施进度日志（2026-02-18，第四轮落地）

本轮补齐“人物画像 AI 生成”链路，并与前面已落地的三 Tab/任务配置保持一致：

1. 后端新增 AI 生成人物画像接口：
   - `POST /api/eval/personas/generate`
   - 输入：`project_id` + `avoid_names`
   - 行为：结合项目意图与已有画像名，生成新的 `name + prompt`

2. 前端画像 Tab 接入按钮：
   - `EvalPersonaSetup` 增加 `[AI 生成画像]`
   - 生成后直接追加到当前画像列表，标记 `source: "ai"`，可立即编辑并保存

3. API 客户端补齐：
   - `frontend/lib/api.ts` 的 `evalAPI` 新增 `generatePersona(...)`

4. 本轮测试与闭环：
   - 后端：`pytest tests/test_eval_v2_flow.py tests/test_eval_v2_executor.py tests/test_eval_v2_service.py -q` -> `10 passed`
   - 前端：`npm run build` 通过
   - 新增用例：
     - `test_eval_v2_generate_persona_endpoint`（验证 avoid_names 与已有画像合并逻辑）

### 12.9 实施进度日志（2026-02-18，第五轮落地）

本轮完成两项与可追溯性/回归稳定性直接相关的增强：

1. 报告看板补充“原始提示词可见性”：
   - 在 Trial 详情中新增“查看原始提示词”按钮
   - 可展开查看每次 LLM 调用的 `system_prompt` / `user_message` / `output`
   - 满足“Trial 过程应可追踪”的要求

2. 强化回归测试：混合形态 + 多批次趋势
   - 新增用例 `test_eval_v2_mixed_form_multi_batch_trend`
   - 覆盖：
     - 一个 Task 内混合 form（assessment + experience）
     - 多 batch 执行记录扁平列表
     - batch 详情查询正确性（Trial 数、form 分布）

3. 本轮测试与闭环：
   - 后端：`pytest tests/test_eval_v2_flow.py tests/test_eval_v2_executor.py tests/test_eval_v2_service.py -q` -> `11 passed`
   - 前端：`npm run build` 通过

### 12.10 实施进度日志（2026-02-19，第六轮落地）

本轮继续收敛未完成里程碑，重点补齐 Persona 独立 CRUD API 并写接口级闭环测试：

1. 后端新增 Persona CRUD 接口（基于 `eval_persona_setup` ContentBlock 持久化）：
   - `POST /api/eval/personas/{project_id}`：新增画像
   - `PUT /api/eval/persona/{persona_id}`：更新画像
   - `DELETE /api/eval/persona/{persona_id}`：删除画像
   - 保持与既有 `GET /api/eval/personas/{project_id}` 一致，不引入新存储模型，避免架构冲突

2. 新增 Persona 块读写工具函数：
   - 自动创建 `eval_persona_setup`（若不存在）
   - 统一读写 JSON 结构（`{ personas: [...] }`）
   - 对历史 `background` 字段做兼容归一（写入时归一到 `prompt`）

3. 新增高价值测试：
   - `test_eval_v2_persona_crud_api`
   - 覆盖完整链路：创建 -> 查询 -> 更新 -> 再查询 -> 删除 -> 再查询
   - 断言接口行为与持久化结果一致（非走过场测试）

4. 本轮测试与闭环：
   - 后端：`pytest tests/test_eval_v2_flow.py tests/test_eval_v2_executor.py tests/test_eval_v2_service.py -q` -> `12 passed`

### 12.11 实施进度日志（2026-02-19，第七轮落地）

本轮继续完成 Phase 4 未完成项，补齐“统一 AI 生成提示词 API”并打通前端客户端：

1. 后端新增统一提示词生成接口：
   - `POST /api/eval/prompts/generate`
   - 输入：`prompt_type + context`
   - 输出：`generated_prompt`
   - 仅提供“生成”，不引入“优化/建议”分支，保持与设计原则一致

2. 生成逻辑细节：
   - 新增 `_generate_prompt_with_llm(...)`
   - 按 `prompt_type` 注入必需占位符要求（如 `{content}`、`{persona}`、`{probe_section}`）
   - LLM 异常时返回安全兜底模板，避免接口不可用

3. 前端 API 客户端补齐：
   - `frontend/lib/api.ts` 中 `evalAPI.generatePrompt(promptType, context)`
   - 为后续“提示词编辑器上的 AI 生成按钮”提供统一调用入口

4. 新增测试与回归：
   - 新增 `test_eval_v2_generate_prompt_endpoint`（接口层验证 prompt_type/context 透传）
   - 后端：`pytest tests/test_eval_v2_flow.py tests/test_eval_v2_executor.py tests/test_eval_v2_service.py -q` -> `13 passed`
   - 前端：`npm run build` 通过

### 12.12 实施进度日志（2026-02-19，第八轮落地）

本轮完成 Phase 4 的“评估提示词设置”主链路，覆盖后端接口、设置页入口与编辑器：

1. 后端设置接口新增（`/api/settings`）：
   - `GET /api/settings/eval-prompts`
   - `PUT /api/settings/eval-prompts/{prompt_id}`
   - 首次访问自动初始化评估预置模板（experience 三步、scenario A/B、cross-trial）

2. 前端设置页新增“🧪 评估提示词”Tab：
   - `settings/page.tsx` 新增 Tab 与数据加载分支
   - 新增 `EvalPromptsSection` 编辑器组件
   - 支持占位符检查（按模板 phase 校验）
   - 每个模板编辑态支持 `[🤖 AI 生成]`（调用 `/api/eval/prompts/generate`）

3. 新增高价值测试：
   - `test_settings_eval_prompts.py`
   - 覆盖：预置模板自动初始化 + 更新接口持久化

4. 本轮测试与闭环：
   - 后端：`pytest tests/test_settings_eval_prompts.py tests/test_eval_v2_flow.py tests/test_eval_v2_executor.py tests/test_eval_v2_service.py -q` -> `14 passed`
   - 前端：`npm run build` 通过

### 12.13 实施进度日志（2026-02-19，第九轮落地）

本轮补齐“全量预置 Grader/Simulator 模板与后台配置同步”：

1. 后端新增预置同步接口：
   - `POST /api/settings/eval-presets/sync`
   - 行为：幂等同步预置 Grader + 预置 Simulator（同名预置更新，不重复插入）
   - 返回：新增/更新数量统计

2. 设置页联动：
   - 在“评估提示词”Tab 新增按钮 `同步预置 Grader/Simulator`
   - 一键触发同步并显示统计结果，确保后台配置与 Eval 预置模板可持续一致

3. 新增高价值测试：
   - `test_eval_presets_sync_is_idempotent`
   - 覆盖：首次同步有新增、二次同步不重复插入、预置数量稳定

4. 本轮测试与闭环：
   - 后端：`pytest tests/test_settings_eval_prompts.py tests/test_eval_v2_flow.py tests/test_eval_v2_executor.py tests/test_eval_v2_service.py -q` -> `15 passed`
   - 前端：`npm run build` 通过

### 12.14 实施进度日志（2026-02-19，第十轮落地）

本轮继续完成未勾选项：补齐“四形态至少跑一次”的 API 级端到端测试。

1. 新增高价值 E2E 用例：
   - `test_eval_v2_e2e_all_forms_single_task`
   - 在同一个 Task 中配置 4 个 TrialConfig（assessment/review/experience/scenario）
   - 执行一次 Task，断言四形态都被实际调度并产出 TrialResult

2. 覆盖重点：
   - 单任务多形态编排正确性（核心架构：Task 容器 + Trial form_type 分发）
   - 执行结果完整性（4条 Trial、状态 completed、分数字段存在）
   - 与已有 Persona/Grader/Experience 执行链路兼容

3. 本轮测试与闭环：
   - 后端：`pytest tests/test_settings_eval_prompts.py tests/test_eval_v2_flow.py tests/test_eval_v2_executor.py tests/test_eval_v2_service.py -q` -> `16 passed`

### 12.15 实施进度日志（2026-02-19，第十一轮落地）

本轮完成“旧 Eval -> Eval V2”迁移脚本及幂等验证，覆盖剩余里程碑中的高风险项：

1. 新增迁移脚本：
   - `backend/scripts/migrate_legacy_eval_to_v2.py`
   - 支持 `--dry-run`（默认）与 `--execute`
   - 迁移范围：
     - `EvalTask` -> `EvalTaskV2 + EvalTrialConfigV2`
     - `EvalTrial` -> `EvalTrialResultV2`
   - 保留旧链路不删除，采用“并行迁移”策略，避免破坏现网兼容性

2. 幂等机制：
   - Task 级基于 `legacy_eval_task_id` 标记去重
   - Result 级基于 `(trial_config_id, batch_id, repeat_index)` 去重
   - 重复执行迁移不会重复写入

3. 新增高价值测试：
   - `test_migrate_legacy_eval_to_v2_idempotent`
   - 覆盖：首次迁移创建、二次迁移不重复、最新批次聚合正确

4. 本轮测试与闭环：
   - 后端：`pytest tests/test_migrate_legacy_eval_to_v2.py tests/test_settings_eval_prompts.py tests/test_eval_v2_flow.py tests/test_eval_v2_executor.py tests/test_eval_v2_service.py -q` -> `17 passed`

### 12.16 实施进度日志（2026-02-19，第十二轮落地）

本轮继续清理遗留并完成文档一致性收尾：

1. 旧前端入口清理：
   - 删除 `frontend/components/eval-panel.tsx`
   - 移除 `frontend/lib/api.ts` 中仅旧面板使用的 EvalRun/EvalTask 旧客户端方法
   - Eval 主入口保持为 `eval-phase-panel` 三 Tab 架构

2. 文档一致性更新：
   - 更新 `docs/eval_system_design.md`，补充“V2 主链路为 TaskV2/TrialConfigV2/TrialResultV2”
   - 更新 `docs/first_principles.md` 的评估架构描述，对齐 Eval V2 实际落地

3. 本轮测试与闭环：
   - 后端：`pytest tests/test_migrate_legacy_eval_to_v2.py tests/test_settings_eval_prompts.py tests/test_eval_v2_flow.py tests/test_eval_v2_executor.py tests/test_eval_v2_service.py -q` -> `17 passed`
   - 前端：`npm run build` 通过

