# Agent æ¶æ„è¿ç§»æ–¹æ¡ˆï¼šæ­£ç¡®ä½¿ç”¨ LangGraph

> åˆ›å»ºæ—¶é—´: 2026-02-11
> çŠ¶æ€: æ–¹æ¡ˆå·²ç¡®è®¤ï¼Œå¾…å®æ–½

---

## ä¸€ã€å…±è¯†å›é¡¾

ç»è¿‡è¯¦ç»†è®¨è®ºï¼Œæˆ‘ä»¬è¾¾æˆä»¥ä¸‹å…±è¯†ï¼š

1. **åˆ é™¤ `ai_client`**ï¼šè‡ªå®šä¹‰çš„ `AIClient` ä¸æ”¯æŒ LangChain æ¶ˆæ¯ç±»å‹ã€ä¸æ”¯æŒ Tool Callingã€æ— æ³•èå…¥ LangGraph ç”Ÿæ€ã€‚æ‰€æœ‰ LLM è°ƒç”¨ç»Ÿä¸€èµ° LangChain çš„ `BaseChatModel`ã€‚
2. **æ­£ç¡®ä½¿ç”¨ LangGraph**ï¼šå½“å‰ä»£ç å®šä¹‰äº† LangGraph å›¾ï¼Œä½†å‰ç«¯å”¯ä¸€ä½¿ç”¨çš„ `/stream` ç«¯ç‚¹å®Œå…¨ç»•å¼€äº†è¿™ä¸ªå›¾ï¼Œæ‰‹åŠ¨è°ƒç”¨ `route_intent()` å†ç”¨ `if/elif` åˆ†å‘ã€‚è¿™æ˜¯"ä¼ª LangGraph"ã€‚
3. **LLM é©±åŠ¨è·¯ç”±**ï¼šç”¨ OpenAI Tool Callingï¼ˆ`bind_tools`ï¼‰æ›¿ä»£æ‰‹å†™çš„æ„å›¾åˆ†ç±» prompt + JSON è§£æï¼Œè®© LLM åŸç”Ÿåœ°é€‰æ‹©è°ƒç”¨å“ªä¸ªå·¥å…·ã€‚
4. **å¤š API åˆ‡æ¢**ï¼šåˆ©ç”¨ LangChain çš„ `BaseChatModel` æŠ½è±¡å±‚ï¼Œåœ¨é…ç½®å±‚æ”¯æŒå¤šä¸ª LLM Providerï¼ˆOpenAI / Gemini / Qwen / DeepSeekï¼‰ï¼Œç”¨æˆ·å¯åœ¨åå°åˆ‡æ¢ã€‚
5. **Token çº§æµå¼è¾“å‡º**ï¼šç»Ÿä¸€ç”¨ `graph.astream_events(version="v2")` å®ç°æ‰€æœ‰è·¯ç”±çš„ token-by-token æµå¼è¾“å‡ºï¼Œè€Œéåªæœ‰ `chat` è·¯ç”±æ”¯æŒã€‚

---

## äºŒã€ç°æœ‰æ¶æ„é—®é¢˜è¯Šæ–­

### 2.1 åŒè½¨è·¯ç”±ï¼ˆæ ¸å¿ƒç¼ºé™·ï¼‰

```
å‰ç«¯ â†’ POST /api/agent/stream
         â”‚
         â”œâ”€â”€ agent.py: æ‰‹åŠ¨è°ƒç”¨ route_intent()
         â”œâ”€â”€ agent.py: if route_target == "chat": ... elif "generic_research": ...
         â”œâ”€â”€ agent.py: node_map[route_target] = tool_node  â† æ‰‹åŠ¨åˆ†å‘
         â”‚
         â””â”€â”€ LangGraph å›¾ï¼ˆå®šä¹‰äº† ~20 ä¸ªèŠ‚ç‚¹ + æ¡ä»¶è¾¹ï¼‰â† ä»æœªè¢« /stream è°ƒç”¨

å‰ç«¯ â†’ POST /api/agent/chat  â† å‰ç«¯ä¸ç”¨
         â”‚
         â””â”€â”€ content_agent.run() â†’ graph.ainvoke()  â† LangGraph å›¾è¢«è°ƒç”¨ï¼Œä½†æ— æµå¼
```

**é—®é¢˜**ï¼šLangGraph å›¾æ˜¯æ­»ä»£ç ã€‚æ‰€æœ‰å®é™…æµé‡èµ° `/stream`ï¼Œå®Œå…¨æ‰‹åŠ¨åˆ†å‘ã€‚è¿™æ„å‘³ç€ LangGraph çš„æ¡ä»¶è¾¹ã€å¤šæ„å›¾å¤„ç†ï¼ˆ`continue_pending`ï¼‰ã€çŠ¶æ€æ£€æŸ¥ç‚¹ç­‰èƒ½åŠ›å…¨éƒ¨æœªä½¿ç”¨ã€‚

### 2.2 æ„å›¾è·¯ç”±çš„ä¸¤é˜¶æ®µ LLM è°ƒç”¨

```
è¯·æ±‚ â†’ route_intent() [LLMè°ƒç”¨1: æ„å›¾åˆ†ç±» JSON] â†’ è§£æ intent/target/operation
     â†’ èŠ‚ç‚¹å‡½æ•° [LLMè°ƒç”¨2: æ‰§è¡Œä»»åŠ¡]
```

**é—®é¢˜**ï¼šæ¯æ¬¡è¯·æ±‚éƒ½éœ€è¦ 2 æ¬¡ LLM è°ƒç”¨ï¼ˆä¸€æ¬¡åˆ†ç±»ã€ä¸€æ¬¡æ‰§è¡Œï¼‰ã€‚æ­£ç¡®çš„åšæ³•æ˜¯ç”¨ Tool Callingï¼Œè®© LLM åœ¨ä¸€æ¬¡è°ƒç”¨ä¸­åŒæ—¶å†³å®šæ„å›¾å’Œå‚æ•°ã€‚

### 2.3 ai_client çš„å±€é™æ€§

| èƒ½åŠ› | ai_client | ChatOpenAI (LangChain) |
|------|-----------|----------------------|
| çº¯æ–‡æœ¬å¯¹è¯ | âœ… | âœ… |
| æµå¼è¾“å‡º | âœ… `stream_chat()` | âœ… `astream()` |
| Tool Calling | âŒ ä¸æ”¯æŒ | âœ… `bind_tools()` |
| ç»“æ„åŒ–è¾“å‡º | âš ï¸ æ‰‹åŠ¨ JSON schema æ³¨å…¥ | âœ… `with_structured_output()` |
| LangChain æ¶ˆæ¯ç±»å‹ | âŒ è‡ªå®šä¹‰ `ChatMessage` | âœ… `HumanMessage/AIMessage/ToolMessage` |
| LangGraph å…¼å®¹ | âŒ | âœ… åŸç”Ÿå…¼å®¹ |
| API åˆ‡æ¢ | âŒ ç¡¬ç¼–ç  OpenAI | âœ… `BaseChatModel` å¤š provider |

### 2.4 State è†¨èƒ€

å½“å‰ `ContentProductionState` æœ‰ **27 ä¸ªå­—æ®µ**ï¼Œå…¶ä¸­å¤§éƒ¨åˆ†æ˜¯ä¸ºäº†åœ¨æ‰‹åŠ¨åˆ†å‘ä¸­ä¼ é€’ä¿¡æ¯ã€‚æ­£ç¡®ä½¿ç”¨ LangGraph åï¼ŒçŠ¶æ€å¯ä»¥å¤§å¹…ç®€åŒ–ã€‚

### 2.5 æµå¼è¾“å‡ºä¸ä¸€è‡´

- `chat` è·¯ç”±ï¼šé€šè¿‡ `ai_client.stream_chat()` å®ç° token çº§æµå¼
- å…¶ä»–æ‰€æœ‰è·¯ç”±ï¼šç­‰å¾…èŠ‚ç‚¹å‡½æ•°æ‰§è¡Œå®Œæ¯•ï¼Œä¸€æ¬¡æ€§å‘é€ `content` äº‹ä»¶

---

## ä¸‰ã€ç›®æ ‡æ¶æ„

### 3.1 æ¶æ„æ€»è§ˆ

```
å‰ç«¯ â†’ POST /api/agent/stream
         â”‚
         â””â”€â”€ agent.py:
               1. æ„å»º AgentStateï¼ˆmessages + metadataï¼‰
               2. graph.astream_events(input, version="v2")
               3. éå†äº‹ä»¶æµï¼š
                  - on_chat_model_stream â†’ yield SSE token
                  - on_tool_start â†’ yield SSE status
                  - on_tool_end â†’ yield SSE result
                  - å›¾ç»“æŸ â†’ yield SSE done + ä¿å­˜DB
```

### 3.2 LLM å±‚ï¼šç»Ÿä¸€ ChatModel + å¤š Provider åˆ‡æ¢

**æ–°æ–‡ä»¶ `backend/core/llm.py`**ï¼š

```python
"""
ç»Ÿä¸€çš„ LLM å®ä¾‹ç®¡ç†
æ”¯æŒå¤š Provider åˆ‡æ¢ï¼ˆOpenAI / Gemini / Qwen / DeepSeekï¼‰
"""
from langchain_openai import ChatOpenAI
from core.config import settings

# Provider é…ç½®æ˜ å°„
# æ¯ä¸ª provider å¯¹åº”ä¸€ç»„ (ChatModelç±», é»˜è®¤æ¨¡å‹, é¢å¤–å‚æ•°)
PROVIDER_REGISTRY = {
    "openai": {
        "class": "langchain_openai.ChatOpenAI",
        "default_model": "gpt-5.1",
        "env_key": "OPENAI_API_KEY",
    },
    # æœªæ¥æ‰©å±•ï¼š
    # "gemini": {
    #     "class": "langchain_google_genai.ChatGoogleGenerativeAI",
    #     "default_model": "gemini-2.0-flash",
    #     "env_key": "GOOGLE_API_KEY",
    # },
    # "qwen": {
    #     "class": "langchain_community.chat_models.ChatTongyi",
    #     "default_model": "qwen-max",
    #     "env_key": "DASHSCOPE_API_KEY",
    # },
    # "deepseek": {
    #     "class": "langchain_openai.ChatOpenAI",  # DeepSeek å…¼å®¹ OpenAI API
    #     "default_model": "deepseek-chat",
    #     "env_key": "DEEPSEEK_API_KEY",
    #     "base_url": "https://api.deepseek.com/v1",
    # },
}

def get_chat_model(
    provider: str = "openai",
    model: str | None = None,
    temperature: float = 0.7,
    streaming: bool = True,
    **kwargs,
) -> ChatOpenAI:
    """
    è·å– ChatModel å®ä¾‹ã€‚

    Args:
        provider: LLM æä¾›å•†åç§°ï¼ˆopenai/gemini/qwen/deepseekï¼‰
        model: æ¨¡å‹åç§°ï¼ŒNone åˆ™ä½¿ç”¨ provider é»˜è®¤æ¨¡å‹
        temperature: æ¸©åº¦
        streaming: æ˜¯å¦å¯ç”¨æµå¼ï¼ˆLangGraph astream_events éœ€è¦ï¼‰
        **kwargs: ä¼ é€’ç»™ ChatModel æ„é€ å‡½æ•°çš„é¢å¤–å‚æ•°

    Returns:
        BaseChatModel å®ä¾‹
    """
    # å½“å‰é˜¶æ®µåªå®ç° OpenAIï¼Œå…¶ä»– provider åç»­æŒ‰éœ€æ·»åŠ 
    model = model or settings.openai_model or "gpt-5.1"
    return ChatOpenAI(
        model=model,
        temperature=temperature,
        streaming=streaming,
        api_key=settings.openai_api_key,
        organization=settings.openai_org_id or None,
        base_url=settings.openai_api_base or None,
        timeout=120,
        **kwargs,
    )

# é»˜è®¤å®ä¾‹ï¼ˆAgent ä¸»æ¨¡å‹ï¼‰
llm = get_chat_model()

# å°æ¨¡å‹å®ä¾‹ï¼ˆæ‘˜è¦ç”Ÿæˆç­‰è½»é‡ä»»åŠ¡ï¼‰
llm_mini = get_chat_model(model="gpt-4o-mini", temperature=0)
```

**Settings æ‰©å±•ï¼ˆ`backend/core/config.py`ï¼‰**ï¼š

```python
class Settings(BaseSettings):
    # å½“å‰ä½¿ç”¨çš„ LLM Provider
    llm_provider: str = "openai"

    # OpenAI
    openai_api_key: str = ""
    openai_org_id: str = ""
    openai_model: str = "gpt-5.1"
    openai_api_base: str = ""

    # æœªæ¥æ‰©å±•ï¼ˆæš‚ä¸å®ç°ï¼‰
    # google_api_key: str = ""
    # dashscope_api_key: str = ""
    # deepseek_api_key: str = ""

    # ... å…¶ä»–é…ç½®ä¸å˜
```

### 3.3 å·¥å…·å±‚ï¼šLangChain `@tool`

**æ–°æ–‡ä»¶ `backend/core/agent_tools.py`**ï¼š

æ‰€æœ‰ç°æœ‰çš„"èŠ‚ç‚¹å‡½æ•°"ä¸­æ¶‰åŠå®é™…æ“ä½œçš„éƒ¨åˆ†ï¼ˆä¿®æ”¹å­—æ®µã€ç”Ÿæˆå†…å®¹ã€è°ƒç ”ã€ç®¡ç†æ¶æ„ç­‰ï¼‰ï¼Œè½¬åŒ–ä¸º LangChain `@tool` å‡½æ•°ã€‚

```python
"""
Agent å·¥å…·å®šä¹‰
ä½¿ç”¨ LangChain @tool è£…é¥°å™¨ï¼Œè®© LLM é€šè¿‡ Tool Calling è‡ªåŠ¨é€‰æ‹©
"""
from langchain_core.tools import tool
from typing import Optional

@tool
def modify_field(field_name: str, instruction: str, reference_fields: list[str] = []) -> str:
    """ä¿®æ”¹æŒ‡å®šå­—æ®µçš„å†…å®¹ã€‚å½“ç”¨æˆ·è¦æ±‚ä¿®æ”¹ã€è°ƒæ•´ã€é‡å†™æŸä¸ªå­—æ®µæ—¶ä½¿ç”¨ã€‚

    Args:
        field_name: è¦ä¿®æ”¹çš„ç›®æ ‡å­—æ®µåç§°
        instruction: ç”¨æˆ·çš„ä¿®æ”¹æŒ‡ä»¤ï¼ˆå¦‚"æŠŠ5ä¸ªæ¨¡å—æ”¹æˆ7ä¸ª"ï¼‰
        reference_fields: éœ€è¦å‚è€ƒçš„å…¶ä»–å­—æ®µåç§°åˆ—è¡¨
    """
    # å®ç°ï¼šè¯»å–å­—æ®µå†…å®¹ â†’ æ„å»º prompt â†’ LLM ç”Ÿæˆ edits â†’ apply_edits
    ...

@tool
def generate_field_content(field_name: str, instruction: str = "") -> str:
    """ç”ŸæˆæŒ‡å®šå­—æ®µçš„å†…å®¹ã€‚å½“ç”¨æˆ·è¦æ±‚ç”Ÿæˆã€åˆ›å»ºæŸä¸ªå­—æ®µçš„å†…å®¹æ—¶ä½¿ç”¨ã€‚

    Args:
        field_name: è¦ç”Ÿæˆå†…å®¹çš„å­—æ®µåç§°
        instruction: é¢å¤–çš„ç”ŸæˆæŒ‡ä»¤ï¼ˆå¯é€‰ï¼‰
    """
    ...

@tool
def query_field(field_name: str, question: str) -> str:
    """æŸ¥è¯¢å­—æ®µå†…å®¹å¹¶å›ç­”é—®é¢˜ã€‚å½“ç”¨æˆ·è¯¢é—®æŸä¸ªå­—æ®µçš„å†…å®¹æˆ–æƒ³äº†è§£ç›¸å…³ä¿¡æ¯æ—¶ä½¿ç”¨ã€‚

    Args:
        field_name: è¦æŸ¥è¯¢çš„å­—æ®µåç§°
        question: ç”¨æˆ·çš„é—®é¢˜
    """
    ...

@tool
def manage_architecture(operation: str, target: str, details: str = "") -> str:
    """ç®¡ç†é¡¹ç›®æ¶æ„ï¼ˆæ·»åŠ /åˆ é™¤/ç§»åŠ¨å­—æ®µæˆ–é˜¶æ®µï¼‰ã€‚

    Args:
        operation: æ“ä½œç±»å‹ï¼ˆadd_field/remove_field/add_phase/remove_phase/move_fieldï¼‰
        target: æ“ä½œç›®æ ‡ï¼ˆå­—æ®µåæˆ–é˜¶æ®µåï¼‰
        details: æ“ä½œè¯¦æƒ…ï¼ˆå¦‚æ–°å­—æ®µçš„æè¿°ã€ç›®æ ‡ä½ç½®ç­‰ï¼‰
    """
    ...

@tool
def advance_to_phase(target_phase: str = "") -> str:
    """æ¨è¿›é¡¹ç›®åˆ°ä¸‹ä¸€é˜¶æ®µæˆ–æŒ‡å®šé˜¶æ®µã€‚

    Args:
        target_phase: ç›®æ ‡é˜¶æ®µåç§°ï¼ˆç©ºå­—ç¬¦ä¸²è¡¨ç¤ºä¸‹ä¸€é˜¶æ®µï¼‰
    """
    ...

@tool
def run_research(query: str, research_type: str = "consumer") -> str:
    """æ‰§è¡Œè°ƒç ”ã€‚consumer=æ¶ˆè´¹è€…è°ƒç ”ï¼Œgeneric=é€šç”¨æ·±åº¦è°ƒç ”ã€‚

    Args:
        query: è°ƒç ”ä¸»é¢˜æˆ–æŸ¥è¯¢
        research_type: è°ƒç ”ç±»å‹ï¼ˆconsumer/genericï¼‰
    """
    ...

@tool
def manage_persona(operation: str, persona_data: str = "") -> str:
    """ç®¡ç†ç”¨æˆ·ç”»åƒ/è§’è‰²ã€‚

    Args:
        operation: æ“ä½œç±»å‹ï¼ˆlist/create/update/delete/generateï¼‰
        persona_data: è§’è‰²æ•°æ®ï¼ˆJSON æ ¼å¼ï¼Œåˆ›å»º/æ›´æ–°æ—¶éœ€è¦ï¼‰
    """
    ...

@tool
def run_evaluation() -> str:
    """å¯¹é¡¹ç›®å†…å®¹æ‰§è¡Œå…¨é¢è¯„ä¼°ï¼Œç”Ÿæˆè¯„ä¼°æŠ¥å‘Šã€‚"""
    ...

@tool
def read_field(field_name: str) -> str:
    """è¯»å–æŒ‡å®šå­—æ®µçš„å®Œæ•´å†…å®¹ã€‚å½“ç”¨æˆ·æƒ³æŸ¥çœ‹æŸä¸ªå­—æ®µçš„å†…å®¹æ—¶ä½¿ç”¨ã€‚

    Args:
        field_name: è¦è¯»å–çš„å­—æ®µåç§°
    """
    ...

@tool
def update_field(field_name: str, content: str) -> str:
    """ç›´æ¥ç”¨ç»™å®šå†…å®¹è¦†å†™æŒ‡å®šå­—æ®µã€‚å½“ç”¨æˆ·æä¾›äº†å®Œæ•´å†…å®¹è¦æ±‚ç›´æ¥æ›¿æ¢æ—¶ä½¿ç”¨ã€‚

    Args:
        field_name: è¦æ›´æ–°çš„å­—æ®µåç§°
        content: æ–°å†…å®¹ï¼ˆå®Œæ•´æ›¿æ¢ï¼‰
    """
    ...

@tool
def generate_outline(topic: str = "") -> str:
    """ç”Ÿæˆå†…å®¹å¤§çº²/è§„åˆ’ã€‚

    Args:
        topic: å¤§çº²ä¸»é¢˜ï¼ˆä¸ºç©ºåˆ™åŸºäºé¡¹ç›®æ„å›¾è‡ªåŠ¨ç”Ÿæˆï¼‰
    """
    ...
```

**å…³é”®è®¾è®¡è¦ç‚¹**ï¼š

1. æ¯ä¸ª `@tool` çš„ docstring å°±æ˜¯ LLM çœ‹åˆ°çš„å·¥å…·æè¿°ï¼Œå¿…é¡»å†™æ¸…æ¥š"ä»€ä¹ˆæ—¶å€™ç”¨"
2. å‚æ•°é€šè¿‡ `Args` æè¿°ï¼ŒLLM ä¼šè‡ªåŠ¨æå–
3. å·¥å…·å‡½æ•°å†…éƒ¨å¯ä»¥è®¿é—® DBï¼ˆé€šè¿‡é—­åŒ…æˆ–å…¨å±€ sessionï¼‰ï¼Œä¸éœ€è¦ä» State ä¼ é€’
4. å·¥å…·å‡½æ•°è¿”å›å­—ç¬¦ä¸²ï¼ˆLLM ä¼šçœ‹åˆ°è¿”å›å€¼å¹¶å†³å®šä¸‹ä¸€æ­¥ï¼‰

### 3.4 Graph å®šä¹‰

**`backend/core/orchestrator.py`ï¼ˆé‡å†™ï¼‰**ï¼š

```python
"""
LangGraph Agent æ ¸å¿ƒç¼–æ’å™¨ï¼ˆé‡å†™ç‰ˆï¼‰

æ¶æ„ï¼šCustom StateGraph + Tool Calling
- å…¥å£èŠ‚ç‚¹: agent_nodeï¼ˆLLM å†³ç­– + Tool Callingï¼‰
- å·¥å…·èŠ‚ç‚¹: tool_nodeï¼ˆæ‰§è¡Œè¢«é€‰ä¸­çš„å·¥å…·ï¼‰
- æ¡ä»¶è¾¹: should_continueï¼ˆæ£€æŸ¥æ˜¯å¦æœ‰ tool_callsï¼‰
"""
from typing import TypedDict, Annotated, Optional
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage, ToolMessage
import operator

from core.llm import llm
from core.agent_tools import AGENT_TOOLS  # æ‰€æœ‰ @tool çš„åˆ—è¡¨


# ============== State å®šä¹‰ ==============

class AgentState(TypedDict):
    """
    Agent çŠ¶æ€ï¼ˆç²¾ç®€ç‰ˆï¼‰
    åªä¿ç•™ LangGraph è¿è½¬å¿…éœ€çš„å­—æ®µã€‚

    è®¾è®¡åŸåˆ™ï¼š
    - DB æ“ä½œåœ¨ @tool å‡½æ•°å†…å®Œæˆï¼Œä¸é€šè¿‡ State ä¼ é€’
    - field_updated / is_producing ç­‰ä¿¡æ¯é€šè¿‡ tool_end äº‹ä»¶çš„å·¥å…·åæ¨æ–­ï¼Œä¸æ”¾ State
    """
    # æ¶ˆæ¯å†å²ï¼ˆLangGraph æ ¸å¿ƒï¼šåŒ…å« HumanMessage, AIMessage, ToolMessageï¼‰
    messages: Annotated[list[BaseMessage], operator.add]

    # é¡¹ç›®ä¸Šä¸‹æ–‡ï¼ˆæ³¨å…¥åˆ° system promptï¼Œä¸å‚ä¸å›¾è·¯ç”±ï¼‰
    project_id: str
    current_phase: str
    creator_profile: str


# ============== èŠ‚ç‚¹å‡½æ•° ==============

def build_system_prompt(state: AgentState) -> str:
    """
    æ„å»º system promptã€‚
    åŒ…å«ï¼šè§’è‰²å®šä¹‰ + åˆ›ä½œè€…ç‰¹è´¨ + å­—æ®µç´¢å¼• + é˜¶æ®µä¸Šä¸‹æ–‡
    """
    creator_profile = state.get("creator_profile", "")
    current_phase = state.get("current_phase", "intent")
    project_id = state.get("project_id", "")

    # å­—æ®µç´¢å¼•ï¼ˆå¹³å°è®°å¿† â€” éœ€è¦ implementation_plan_v3 ä¸­çš„ digest_service æ¨¡å—ï¼‰
    # æ³¨æ„ï¼šdigest_service æ˜¯æ–°å»ºæ¨¡å—ï¼Œåˆå§‹è¿ç§»æ—¶å¯è·³è¿‡æ­¤æ®µ
    field_index_section = ""
    if project_id:
        try:
            from core.digest_service import build_field_index
        except ImportError:
            build_field_index = None  # digest_service å°šæœªå®ç°æ—¶çš„é™çº§
        fi = build_field_index(project_id) if build_field_index else None
        if fi:
            field_index_section = f"""

## é¡¹ç›®å­—æ®µç´¢å¼•
ä»¥ä¸‹æ˜¯æœ¬é¡¹ç›®æ‰€æœ‰å­—æ®µåŠå…¶æ‘˜è¦ã€‚
ç”¨é€”ï¼šå¸®ä½ å®šä½ä¸ç”¨æˆ·æŒ‡ä»¤ç›¸å…³çš„å­—æ®µã€‚
æ³¨æ„ï¼šæ‘˜è¦åªæ˜¯ç´¢å¼•ï¼Œä¸æ˜¯å®Œæ•´å†…å®¹ã€‚ä¸è¦åŸºäºæ‘˜è¦çŒœæµ‹æˆ–ç¼–é€ å†…å®¹ã€‚

{fi}
"""

    return f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½çš„å†…å®¹ç”Ÿäº§ Agentã€‚

## ä½ çš„èƒ½åŠ›
ä½ å¯ä»¥é€šè¿‡å·¥å…·æ¥æ‰§è¡Œå„ç§æ“ä½œã€‚LLM ä¼šæ ¹æ®ç”¨æˆ·æŒ‡ä»¤è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„å·¥å…·ã€‚
å¦‚æœç”¨æˆ·åªæ˜¯èŠå¤©æˆ–æé—®ï¼ˆä¸éœ€è¦æ‰§è¡Œæ“ä½œï¼‰ï¼Œç›´æ¥å›å¤å³å¯ï¼Œä¸è¦è°ƒç”¨å·¥å…·ã€‚

## åˆ›ä½œè€…ä¿¡æ¯
{creator_profile or 'ï¼ˆæš‚æ— åˆ›ä½œè€…ä¿¡æ¯ï¼‰'}

## å½“å‰é˜¶æ®µ
{current_phase}
{field_index_section}

## äº¤äº’è§„åˆ™
1. ç”¨æˆ·è¦æ±‚"åš"æŸäº‹ â†’ è°ƒç”¨å¯¹åº”å·¥å…·
2. ç”¨æˆ·åœ¨é—®é—®é¢˜æˆ–é—²èŠ â†’ ç›´æ¥å›å¤
3. ä¸€æ¬¡å¯ä»¥è°ƒç”¨å¤šä¸ªå·¥å…·ï¼ˆå¦‚æœç”¨æˆ·æœ‰å¤šä¸ªè¦æ±‚ï¼‰
4. å·¥å…·è¿”å›ç»“æœåï¼Œç”¨ç®€æ´çš„è¯­è¨€å‘Šè¯‰ç”¨æˆ·ç»“æœ
"""


async def agent_node(state: AgentState) -> dict:
    """
    Agent å†³ç­–èŠ‚ç‚¹ã€‚
    ç”¨ bind_tools çš„ LLM å†³å®šï¼šç›´æ¥å›å¤ or è°ƒç”¨å·¥å…·ã€‚
    """
    system_prompt = build_system_prompt(state)

    # å°† system prompt ä½œä¸ºç¬¬ä¸€æ¡æ¶ˆæ¯æ³¨å…¥
    # æ³¨æ„ï¼šæ¯æ¬¡è°ƒç”¨éƒ½é‡æ–°ç”Ÿæˆ system promptï¼ˆå› ä¸ºå­—æ®µç´¢å¼•å¯èƒ½å˜åŒ–ï¼‰
    messages_with_system = [SystemMessage(content=system_prompt)] + state["messages"]

    # LLM è°ƒç”¨ï¼ˆå·² bind_toolsï¼ŒLLM ä¼šè‡ªåŠ¨å†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·ï¼‰
    llm_with_tools = llm.bind_tools(AGENT_TOOLS)
    response = await llm_with_tools.ainvoke(messages_with_system)

    return {"messages": [response]}


def should_continue(state: AgentState) -> str:
    """
    æ¡ä»¶è¾¹ï¼šæ£€æŸ¥æœ€åä¸€æ¡æ¶ˆæ¯æ˜¯å¦åŒ…å« tool_callsã€‚
    - æœ‰ tool_calls â†’ å» tools èŠ‚ç‚¹æ‰§è¡Œ
    - æ—  tool_calls â†’ ç»“æŸï¼ˆLLM ç›´æ¥å›å¤äº†ç”¨æˆ·ï¼‰
    """
    last_message = state["messages"][-1]
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return "tools"
    return END


# ============== å›¾æ„å»º ==============

def create_agent_graph():
    """
    åˆ›å»º Agent å›¾ã€‚

    ç»“æ„ï¼š
        agent_node â”€â”€(æœ‰tool_calls)â”€â”€â†’ tool_node â”€â”€â†’ agent_nodeï¼ˆå¾ªç¯ï¼‰
            â”‚
            â””â”€â”€(æ— tool_calls)â”€â”€â†’ END
    """
    graph = StateGraph(AgentState)

    # èŠ‚ç‚¹
    graph.add_node("agent", agent_node)
    graph.add_node("tools", ToolNode(AGENT_TOOLS))

    # å…¥å£
    graph.set_entry_point("agent")

    # æ¡ä»¶è¾¹ï¼šagent â†’ tools æˆ– END
    graph.add_conditional_edges("agent", should_continue, {
        "tools": "tools",
        END: END,
    })

    # tools æ‰§è¡Œå®Œåå›åˆ° agentï¼ˆè®© LLM çœ‹åˆ°å·¥å…·ç»“æœï¼Œå†³å®šä¸‹ä¸€æ­¥ï¼‰
    graph.add_edge("tools", "agent")

    return graph.compile()


# å…¨å±€å®ä¾‹
agent_graph = create_agent_graph()
```

### 3.5 API å±‚ï¼šç»Ÿä¸€æµå¼è¾“å‡º

**`backend/api/agent.py`ï¼ˆé‡å†™ stream endpointï¼‰**ï¼š

```python
@router.post("/stream")
async def stream_chat(request: ChatRequest, db: Session = Depends(get_db)):
    """
    ä¸ Agent å¯¹è¯ï¼ˆSSE æµå¼è¾“å‡ºï¼‰

    æ¶æ„ï¼š
    1. æ„å»º AgentState
    2. graph.astream_events(version="v2") éå†äº‹ä»¶æµ
    3. æ ¹æ®äº‹ä»¶ç±»å‹ yield SSE äº‹ä»¶
    """
    project = db.query(Project).filter(Project.id == request.project_id).first()
    if not project:
        raise HTTPException(status_code=404, detail="Project not found")

    current_phase = request.current_phase or project.current_phase

    # ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
    user_msg = ChatMessage(...)
    db.add(user_msg)
    db.commit()

    # åŠ è½½å¯¹è¯å†å² â†’ LangChain Message åˆ—è¡¨
    chat_history = _load_chat_history(db, request.project_id, current_phase)

    # æ„å»º AgentStateï¼ˆåªæœ‰ 4 ä¸ªå­—æ®µï¼‰
    input_state = {
        "messages": chat_history + [HumanMessage(content=request.message)],
        "project_id": request.project_id,
        "current_phase": current_phase,
        "creator_profile": project.creator_profile.to_prompt_context() if project.creator_profile else "",
    }

    # äº§å‡ºç±»å·¥å…·åˆ—è¡¨ï¼ˆè¿™äº›å·¥å…·æ‰§è¡Œåå‰ç«¯éœ€åˆ·æ–°å·¦ä¾§é¢æ¿ï¼‰
    PRODUCE_TOOLS = {"modify_field", "generate_field_content", "manage_architecture", "advance_to_phase"}

    async def event_generator():
        yield sse_event({"type": "user_saved", "message_id": user_msg.id})

        full_content = ""
        current_tool = None
        is_producing = False   # æ˜¯å¦æœ‰å­—æ®µäº§å‡ºï¼ˆä»å·¥å…·åæ¨æ–­ï¼‰

        # config ä¼ é€’ project_id ç­‰ä¿¡æ¯ç»™ @tool å‡½æ•°
        config = {"configurable": {"project_id": request.project_id}}

        async for event in agent_graph.astream_events(input_state, config=config, version="v2"):
            kind = event["event"]

            # Token çº§æµå¼ï¼ˆæ‰€æœ‰è·¯ç”±ç»Ÿä¸€ï¼‰
            # æ³¨æ„ï¼šåªè½¬å‘ agent èŠ‚ç‚¹çš„ LLM streamï¼Œå·¥å…·å†…éƒ¨ LLM è°ƒç”¨ä¸è½¬å‘
            if kind == "on_chat_model_stream":
                # é€šè¿‡ tags æˆ– name åˆ¤æ–­äº‹ä»¶æ¥æºï¼Œåªè½¬å‘ agent èŠ‚ç‚¹
                tags = event.get("tags", [])
                if "agent" in tags or event.get("name") == "agent":
                    chunk = event["data"]["chunk"]
                    if chunk.content:
                        full_content += chunk.content
                        yield sse_event({"type": "token", "content": chunk.content})

            # å·¥å…·å¼€å§‹
            elif kind == "on_tool_start":
                tool_name = event["name"]
                current_tool = tool_name
                yield sse_event({"type": "tool_start", "tool": tool_name})

            # å·¥å…·ç»“æŸ
            elif kind == "on_tool_end":
                tool_output = event["data"].get("output", "")
                field_updated = current_tool in PRODUCE_TOOLS
                if field_updated:
                    is_producing = True
                yield sse_event({
                    "type": "tool_end",
                    "tool": current_tool,
                    "output": tool_output[:500],
                    "field_updated": field_updated,
                })
                current_tool = None

        # å›¾æ‰§è¡Œå®Œæ¯• â†’ ä¿å­˜å“åº” + å‘é€ done
        agent_msg = ChatMessage(
            id=generate_uuid(),
            project_id=request.project_id,
            role="assistant",
            content=full_content,
            message_metadata={"phase": current_phase},
        )
        db.add(agent_msg)
        db.commit()

        yield sse_event({
            "type": "done",
            "message_id": agent_msg.id,
            "is_producing": is_producing,
        })

    return StreamingResponse(event_generator(), media_type="text/event-stream", ...)
```

### 3.6 SSE äº‹ä»¶ç±»å‹æ˜ å°„

| ç°æœ‰äº‹ä»¶ | æ–°æ¶æ„äº‹ä»¶ | è¯´æ˜ |
|----------|-----------|------|
| `user_saved` | `user_saved` | ä¸å˜ |
| `route` | `tool_start` | è·¯ç”±ä¿¡æ¯æ”¹ä¸ºå·¥å…·å¼€å§‹äº‹ä»¶ |
| `token` | `token` | ä¸å˜ï¼Œä½†ç°åœ¨æ‰€æœ‰è·¯ç”±éƒ½æ”¯æŒ |
| `content` | _(åˆ é™¤)_ | ä¸å†éœ€è¦ï¼Œæ‰€æœ‰å†…å®¹é€šè¿‡ token æµå¼ |
| `done` | `done` | ä¸å˜ |
| `error` | `error` | ä¸å˜ |
| _(æ–°å¢)_ | `tool_start` | å·¥å…·å¼€å§‹æ‰§è¡Œ |
| _(æ–°å¢)_ | `tool_end` | å·¥å…·æ‰§è¡Œå®Œæ¯• |

**å‰ç«¯å…¼å®¹ç­–ç•¥**ï¼š

å‰ç«¯çš„ `agent-panel.tsx` éœ€è¦é€‚é…æ–°äº‹ä»¶ç±»å‹ï¼Œä½†æ”¹åŠ¨ä¸å¤§ï¼š
- `route` äº‹ä»¶ â†’ æ›¿æ¢ä¸º `tool_start`ï¼ˆæ˜¾ç¤º"æ­£åœ¨æ‰§è¡ŒXX..."ï¼‰
- `content` äº‹ä»¶ â†’ åˆ é™¤ï¼ˆä¸å†éœ€è¦ï¼Œæ‰€æœ‰å†…å®¹é€šè¿‡ `token`ï¼‰
- æ–°å¢ `tool_start` / `tool_end` å¤„ç†
- `token` / `done` / `error` ä¿æŒä¸å˜

---

## å››ã€è¯¦ç»†è¿ç§»è®¡åˆ’

### Phase 1: åŸºç¡€è®¾æ–½å±‚

#### Step 1.1 â€” æ–°å»º `backend/core/llm.py`

**ç±»å‹**: æ–°å»ºæ–‡ä»¶
**å†…å®¹**: ç»Ÿä¸€çš„ ChatModel å·¥å‚ + é»˜è®¤å®ä¾‹
**å…³é”®å‡½æ•°**: `get_chat_model(provider, model, temperature, streaming, **kwargs)`
**å…¨å±€å®ä¾‹**: `llm`ï¼ˆä¸»æ¨¡å‹ï¼‰, `llm_mini`ï¼ˆå°æ¨¡å‹ï¼Œæ‘˜è¦ç­‰è½»é‡ä»»åŠ¡ï¼‰

**éªŒè¯**: `cd backend && python -c "from core.llm import llm, llm_mini; print(type(llm))"`

#### Step 1.2 â€” æ‰©å±• `backend/core/config.py`

**ç±»å‹**: ä¿®æ”¹æ–‡ä»¶
**æ”¹åŠ¨**: `Settings` æ–°å¢ `llm_provider: str = "openai"`
**å½±å“èŒƒå›´**: ä»…é…ç½®å±‚ï¼Œæ— ä¸šåŠ¡é€»è¾‘å˜åŒ–

#### Step 1.3 â€” æ–°å»º `backend/core/agent_tools.py`

**ç±»å‹**: æ–°å»ºæ–‡ä»¶
**å†…å®¹**: æ‰€æœ‰ `@tool` å®šä¹‰ + `AGENT_TOOLS` åˆ—è¡¨
**å·¥å…·æ¸…å•**ï¼ˆåˆå§‹ç‰ˆæœ¬ï¼‰:

| å·¥å…·å | å¯¹åº”ç°æœ‰åŠŸèƒ½ | ä¼˜å…ˆçº§ |
|--------|-------------|--------|
| `modify_field` | `modify_node` | P0 |
| `generate_field_content` | `generate_field_node` | P0 |
| `query_field` | `query_node` | P0 |
| `manage_architecture` | `tool_node` (architecture) | P0 |
| `advance_to_phase` | `_do_advance_phase` | P0 |
| `run_research` | `research_node` + `_do_generic_research` | P0 |
| `run_evaluation` | `evaluate_node` | P1 |
| `manage_persona` | `tool_node` (persona) | P1 |
| `generate_outline` | `tool_node` (outline) | P1 |
| `read_field` | `read_field_node` | P1 |
| `update_field` | `update_field_node` | P1 |
| `manage_skill` | `tool_node` (skill) | P2 |

**éªŒè¯**: `cd backend && python -c "from core.agent_tools import AGENT_TOOLS; print(f'{len(AGENT_TOOLS)} tools loaded')"`

#### Step 1.4 â€” æ›´æ–° `backend/requirements.txt`

**æ”¹åŠ¨**:
```diff
 # LangChain / LangGraph
 langchain>=1.2.0
+langchain-openai>=0.3.0
 langgraph>=1.0.0
```

**è¯´æ˜**: `langchain-openai` æä¾› `ChatOpenAI`ï¼Œç›®å‰é€šè¿‡ `langchain` é—´æ¥ä¾èµ–ï¼Œä½†æ˜¾å¼å£°æ˜æ›´å®‰å…¨ã€‚

### Phase 2: Agent Graph é‡å†™

#### Step 2.1 â€” é‡å†™ `backend/core/orchestrator.py`

**ç±»å‹**: å¤§å¹…é‡å†™ï¼ˆä¿ç•™è¾…åŠ©å‡½æ•°ï¼Œåˆ é™¤æ—§å›¾ï¼‰
**ä¿ç•™**:
- `normalize_intent()`, `normalize_consumer_personas()` â€” è¾…åŠ©å‡½æ•°
- `_detect_modify_target()` â€” å¯ç§»å…¥å·¥å…·å‡½æ•°å†…éƒ¨

**åˆ é™¤**:
- `ContentProductionState`ï¼ˆ27 å­—æ®µï¼‰â†’ æ›¿æ¢ä¸º `AgentState`ï¼ˆ4 å­—æ®µï¼‰
- `route_intent()` â€” ä¸å†éœ€è¦ï¼ŒLLM é€šè¿‡ Tool Calling è‡ªåŠ¨è·¯ç”±
- `create_content_production_graph()` â€” æ›¿æ¢ä¸º `create_agent_graph()`
- `ContentProductionAgent` ç±» â€” æ›¿æ¢ä¸ºç®€å•çš„ `agent_graph` æ¨¡å—çº§å®ä¾‹
- `continue_pending_node()`, `route_after_phase()`, `route_after_tool()`, `route_by_intent()` â€” ä¸å†éœ€è¦

**æ”¹é€ çš„èŠ‚ç‚¹å‡½æ•°** â†’ ç§»å…¥ `agent_tools.py` ä½œä¸º `@tool`ï¼š
- `intent_analysis_node` â†’ ç‰¹æ®Šå¤„ç†ï¼ˆæ„å›¾åˆ†ææ˜¯å¯¹è¯å¼å¤šè½®ï¼Œä¸é€‚åˆåšå·¥å…·ï¼Œä¿ç•™ä¸ºç‰¹æ®ŠèŠ‚ç‚¹æˆ–ç”¨ interrupt æœºåˆ¶ï¼‰
- `research_node` â†’ `run_research` @tool
- `design_inner_node` / `produce_inner_node` / `design_outer_node` / `produce_outer_node` â†’ åˆå¹¶ä¸º `generate_field_content` @toolï¼ˆæ ¹æ®å­—æ®µåå’Œé˜¶æ®µè‡ªåŠ¨é€‰æ‹© promptï¼‰
- `evaluate_node` â†’ `run_evaluation` @tool
- `modify_node` â†’ `modify_field` @tool
- `query_node` â†’ `query_field` @tool
- `tool_node` â†’ æ‹†åˆ†ä¸ºå¤šä¸ª @toolï¼ˆmanage_architecture, manage_persona, generate_outline, manage_skillï¼‰
- `read_field_node` â†’ `read_field` @tool
- `update_field_node` â†’ `update_field` @tool
- `chat_node` â†’ åˆ é™¤ï¼ˆLLM ä¸è°ƒç”¨å·¥å…·æ—¶ç›´æ¥å›å¤å°±æ˜¯ chatï¼‰
- `continue_pending_node` â†’ åˆ é™¤ï¼ˆLLM å•æ¬¡è°ƒç”¨å¯è¿”å›å¤šä¸ª tool_callsï¼Œä¸å†éœ€è¦æ‰‹åŠ¨é˜Ÿåˆ—ï¼‰

**æ„å›¾åˆ†æï¼ˆintent phaseï¼‰çš„ç‰¹æ®Šå¤„ç†**ï¼š

æ„å›¾åˆ†ææ˜¯ä¸€ä¸ª 3 è½®é—®ç­”æµç¨‹ï¼Œä¸é€‚åˆç”¨å•æ¬¡ Tool Callingã€‚è§£å†³æ–¹æ¡ˆï¼š

1. æ„å›¾åˆ†æé˜¶æ®µçš„é€»è¾‘æ”¾åœ¨ `build_system_prompt()` ä¸­ï¼šå½“ `current_phase == "intent"` ä¸”æœªå®Œæˆæ—¶ï¼Œsystem prompt æŒ‡å¯¼ LLM æ‰§è¡Œé—®ç­”æµç¨‹ã€‚
2. ä¸éœ€è¦å•ç‹¬çš„èŠ‚ç‚¹æˆ–å·¥å…·ï¼ŒLLM åœ¨ system prompt å¼•å¯¼ä¸‹è‡ªç„¶åœ°è¿›è¡Œå¤šè½®å¯¹è¯ã€‚
3. å½“ LLM è®¤ä¸ºæ”¶é›†å¤Ÿä¿¡æ¯æ—¶ï¼Œè°ƒç”¨ `generate_field_content(field_name="æ„å›¾åˆ†æ")` å·¥å…·æ¥ç”Ÿæˆå’Œä¿å­˜ç»“æœã€‚

#### Step 2.2 â€” æ›´æ–° AgentState å®šä¹‰

```python
class AgentState(TypedDict):
    messages: Annotated[list[BaseMessage], operator.add]
    project_id: str
    current_phase: str
    creator_profile: str
```

**åªæœ‰ 4 ä¸ªå­—æ®µ**ï¼ˆæ¯”åŸæ¥ 27 ä¸ªå­—æ®µå‡å°‘ 85%ï¼‰ã€‚

**å¯¹æ¯”**:

| ç°æœ‰å­—æ®µ | æ–°çŠ¶æ€ä¸­ | å»å‘ |
|----------|---------|------|
| `project_id` | âœ… ä¿ç•™ | â€” |
| `current_phase` | âœ… ä¿ç•™ | â€” |
| `creator_profile` | âœ… ä¿ç•™ | â€” |
| `messages` | âœ… ä¿ç•™ | â€” |
| `phase_order` | âŒ åˆ é™¤ | å·¥å…·å‡½æ•°å†…ä» DB è¯»å– |
| `phase_status` | âŒ åˆ é™¤ | å·¥å…·å‡½æ•°å†…ä» DB è¯»å– |
| `autonomy_settings` | âŒ åˆ é™¤ | æš‚ä¸ä½¿ç”¨ |
| `fields` | âŒ åˆ é™¤ | å·¥å…·å‡½æ•°å†…ä» DB è¯»å– |
| `user_input` | âŒ åˆ é™¤ | å·²åœ¨ messages ä¸­ |
| `agent_output` | âŒ åˆ é™¤ | å·²åœ¨ messages ä¸­ |
| `waiting_for_human` | âŒ åˆ é™¤ | ç”¨ LangGraph interrupt æœºåˆ¶ |
| `route_target` | âŒ åˆ é™¤ | LLM é€šè¿‡ Tool Calling è‡ªåŠ¨è·¯ç”± |
| `is_producing` | âŒ åˆ é™¤ | ä» tool_end äº‹ä»¶çš„å·¥å…·åæ¨æ–­ï¼ˆ`PRODUCE_TOOLS`ï¼‰ |
| `use_deep_research` | âŒ åˆ é™¤ | å·¥å…·å‡½æ•°å‚æ•° |
| `error` | âŒ åˆ é™¤ | å¼‚å¸¸å¤„ç†åœ¨å·¥å…·å‡½æ•°å†… |
| `tokens_in/out/duration_ms/cost` | âŒ åˆ é™¤ | LangSmith æˆ– callback è¿½è¸ª |
| `full_prompt` | âŒ åˆ é™¤ | LangSmith è‡ªåŠ¨è®°å½• |
| `references` | âŒ åˆ é™¤ | å‰ç«¯è§£æ @ å¼•ç”¨åä½œä¸ºå·¥å…·å‚æ•°ä¼ é€’ |
| `referenced_contents` | âŒ åˆ é™¤ | å·¥å…·å‡½æ•°å†…æŒ‰éœ€è¯»å– DB |
| `parsed_intent_type` | âŒ åˆ é™¤ | Tool Calling è‡ªåŠ¨è·¯ç”± |
| `parsed_target_field` | âŒ åˆ é™¤ | å·¥å…·å‚æ•° |
| `parsed_operation` | âŒ åˆ é™¤ | å·¥å…·å‚æ•° |
| `modify_target_field` | âŒ åˆ é™¤ | å·¥å…·è¿”å›å€¼ |
| `pending_intents` | âŒ åˆ é™¤ | LLM å•æ¬¡è°ƒç”¨å¯è¿”å›å¤šä¸ª tool_calls |

### Phase 3: API å±‚é‡å†™

#### Step 3.1 â€” é‡å†™ `/stream` endpoint

**æ ¸å¿ƒå˜åŒ–**:
1. åˆ é™¤ `route_intent()` è°ƒç”¨
2. åˆ é™¤ `if/elif` åˆ†å‘é“¾ï¼ˆ~200 è¡Œï¼‰
3. æ›¿æ¢ä¸º `graph.astream_events(version="v2")` å¾ªç¯ï¼ˆ~50 è¡Œï¼‰
4. æ‰€æœ‰è·¯ç”±ç»Ÿä¸€ token çº§æµå¼

**ä¿ç•™**:
- `_resolve_references()` â€” ä»éœ€è§£æ @ å¼•ç”¨ï¼Œä½†ç»“æœæ³¨å…¥åˆ° messages è€Œé state
- `_save_result_to_field()` â€” ç§»å…¥å·¥å…·å‡½æ•°å†…éƒ¨
- `_build_chat_display()` â€” ç®€åŒ–æˆ–åˆ é™¤ï¼ˆLLM ç›´æ¥ç”Ÿæˆç”¨æˆ·çœ‹åˆ°çš„æ–‡æœ¬ï¼‰

#### Step 3.2 â€” é‡å†™ `/chat` endpoint

**æ”¹åŠ¨**: ä½¿ç”¨ `graph.ainvoke()` æ›¿ä»£ `content_agent.run()`
**ç®€åŒ–**: ä¸å†éœ€è¦æ‰‹åŠ¨æ„å»º 27 å­—æ®µçš„ initial_stateï¼Œåªéœ€ 4 å­—æ®µ

#### Step 3.3 â€” åˆ é™¤æˆ–ç²¾ç®€çš„ endpoint

| Endpoint | å¤„ç†æ–¹å¼ |
|----------|---------|
| `POST /chat` | ç®€åŒ–ï¼Œç”¨ graph.ainvoke() |
| `POST /stream` | é‡å†™ï¼Œç”¨ graph.astream_events() |
| `POST /tool` | åˆ é™¤ï¼Œå·¥å…·è°ƒç”¨å·²å†…åŒ–åˆ° Agent Loop |
| `POST /advance` | ä¿ç•™ï¼Œä½†æ”¹ä¸ºè°ƒç”¨ `advance_to_phase` @tool |
| `POST /retry` | ç®€åŒ– |
| `GET /history` | ä¸å˜ |
| `PUT /message` | ä¸å˜ |
| `DELETE /message` | ä¸å˜ |

### Phase 4: åˆ é™¤æ—§ä»£ç 

#### Step 4.1 â€” åˆ é™¤ `backend/core/ai_client.py`

æ•´ä¸ªæ–‡ä»¶åˆ é™¤ï¼ˆ329 è¡Œï¼‰ã€‚æ‰€æœ‰å¯¹ `ai_client` çš„å¼•ç”¨æ›¿æ¢ä¸º `llm` æˆ– `llm_mini`ã€‚

**å½±å“çš„æ–‡ä»¶**:

| æ–‡ä»¶ | å¼•ç”¨æ–¹å¼ | æ›¿æ¢ä¸º |
|------|---------|--------|
| `orchestrator.py` | `from core.ai_client import ai_client, ChatMessage` | `from core.llm import llm` + `from langchain_core.messages import HumanMessage, SystemMessage` |
| `api/agent.py` | `from core.ai_client import ai_client, ChatMessage as AIChatMessage` | `from core.llm import llm` |
| `tools/deep_research.py` | `from core.ai_client import ai_client` | `from core.llm import llm` |
| `tools/eval_engine.py` | `from core.ai_client import ai_client, ChatMessage` | `from core.llm import llm` |
| `tools/simulator.py` | `from core.ai_client import ai_client, ChatMessage` | `from core.llm import llm` |
| `tools/field_generator.py` | `from core.ai_client import ai_client, ChatMessage` | `from core.llm import llm` |
| `tools/evaluator.py` | `from core.ai_client import ai_client, ChatMessage` | `from core.llm import llm` |
| `tools/outline_generator.py` | `from core.ai_client import ai_client, ChatMessage` | `from core.llm import llm` |
| `tools/persona_manager.py` | `from core.ai_client import ai_client, ChatMessage` | `from core.llm import llm` |
| `tools/skill_manager.py` | `from core.ai_client import ai_client, ChatMessage` | `from core.llm import llm` |
| `api/blocks.py` (3å¤„) | `from core.ai_client import AIClient, ChatMessage` / `ai_client` | `from core.llm import llm` æˆ– `get_chat_model()` |

**`blocks.py` ç‰¹æ®Šæƒ…å†µ**ï¼š
`blocks.py` ä¸­æœ‰äº›åœ°æ–¹ä½¿ç”¨ `AIClient()` æ„é€ æ–°å®ä¾‹ï¼ˆè€Œéå•ä¾‹ `ai_client`ï¼‰ï¼Œè¿™äº›åœ°æ–¹æ”¹ä¸º `get_chat_model()` è°ƒç”¨æˆ–ç›´æ¥ä½¿ç”¨ `llm`ã€‚

**ChatMessage æ›¿æ¢è§„åˆ™**:
```python
# æ—§
ChatMessage(role="system", content="...")
ChatMessage(role="user", content="...")
ChatMessage(role="assistant", content="...")

# æ–°
SystemMessage(content="...")
HumanMessage(content="...")
AIMessage(content="...")
```

**LLM è°ƒç”¨æ›¿æ¢è§„åˆ™**:
```python
# æ—§
response = await ai_client.async_chat(messages, temperature=0.7)
content = response.content
tokens_in = response.tokens_in

# æ–°
response = await llm.ainvoke(messages)  # messages æ˜¯ LangChain Message åˆ—è¡¨
content = response.content
# token è¿½è¸ªé€šè¿‡ LangSmith æˆ– callback å®ç°
```

**æµå¼è°ƒç”¨æ›¿æ¢è§„åˆ™**:
```python
# æ—§
async for token in ai_client.stream_chat(messages):
    yield token

# æ–°
async for chunk in llm.astream(messages):
    if chunk.content:
        yield chunk.content
```

#### Step 4.2 â€” æ¸…ç† orchestrator.py

åˆ é™¤çš„ä»£ç ï¼ˆçº¦ 2500 è¡Œï¼‰ï¼š
- æ—§çš„ `ContentProductionState`ï¼ˆ27 å­—æ®µï¼‰
- `route_intent()` å‡½æ•°ï¼ˆ~320 è¡Œï¼‰
- æ‰€æœ‰é˜¶æ®µèŠ‚ç‚¹å‡½æ•°ï¼ˆå„ ~80-150 è¡Œ Ã— 7 ä¸ª â‰ˆ ~800 è¡Œï¼‰
- `modify_node`, `query_node`, `chat_node` ç­‰ï¼ˆå„ ~50-200 è¡Œï¼‰
- `tool_node`, `generate_field_node`, `read_field_node`, `update_field_node`
- `continue_pending_node`, `route_after_phase`, `route_after_tool`, `route_by_intent`
- æ—§çš„ `create_content_production_graph()`ï¼ˆ~150 è¡Œï¼‰
- `ContentProductionAgent` ç±»ï¼ˆ~100 è¡Œï¼‰

æ–°å¢çš„ä»£ç ï¼ˆçº¦ 150 è¡Œï¼‰ï¼š
- `AgentState`ï¼ˆ4 å­—æ®µï¼‰
- `build_system_prompt()`
- `agent_node()`
- `should_continue()`
- `create_agent_graph()`

**å‡€å‡å°‘**: ~2000+ è¡Œ

### Phase 5: å·¥å…·å‡½æ•°ä¸­çš„ DB æ“ä½œæ¨¡å¼

å·¥å…·å‡½æ•°éœ€è¦è®¿é—®æ•°æ®åº“ï¼Œä½† LangChain `@tool` ä¸ç›´æ¥æ”¯æŒ DIã€‚è§£å†³æ–¹æ¡ˆï¼š

```python
# æ–¹æ¡ˆï¼šåœ¨å·¥å…·å‡½æ•°å†…ä½¿ç”¨ get_db()
from core.database import get_db

@tool
def modify_field(field_name: str, instruction: str, reference_fields: list[str] = []) -> str:
    """ä¿®æ”¹æŒ‡å®šå­—æ®µçš„å†…å®¹ã€‚"""
    db = next(get_db())
    try:
        # ... è¯»å–å­—æ®µã€è°ƒç”¨ LLMã€apply_editsã€ä¿å­˜ ...
        db.commit()
        return f"å·²ä¿®æ”¹å­—æ®µã€Œ{field_name}ã€"
    except Exception as e:
        db.rollback()
        return f"ä¿®æ”¹å¤±è´¥: {str(e)}"
    finally:
        db.close()
```

**project_id ä¼ é€’é—®é¢˜**ï¼š

`@tool` å‡½æ•°æ— æ³•ç›´æ¥è®¿é—® `AgentState`ã€‚è§£å†³æ–¹æ¡ˆï¼š

1. **æ–¹æ¡ˆ A: é—­åŒ…æ³¨å…¥**ï¼ˆæ¨èï¼‰â€” åœ¨ API å±‚åˆ›å»ºå·¥å…·æ—¶æ³¨å…¥ project_idï¼š
   ```python
   def create_tools_for_project(project_id: str) -> list:
       @tool
       def modify_field(field_name: str, instruction: str) -> str:
           """ä¿®æ”¹æŒ‡å®šå­—æ®µçš„å†…å®¹ã€‚"""
           # è¿™é‡Œå¯ä»¥ç›´æ¥ä½¿ç”¨å¤–å±‚çš„ project_id
           ...
       return [modify_field, ...]
   ```
   æ¯æ¬¡è¯·æ±‚åˆ›å»ºæ–°çš„å·¥å…·åˆ—è¡¨ï¼Œå›¾ä¹Ÿéœ€è¦é‡æ–°åˆ›å»ºï¼ˆæˆ–ä½¿ç”¨ `configurable`ï¼‰ã€‚

2. **æ–¹æ¡ˆ B: RunnableConfig ä¼ é€’**ï¼ˆLangGraph åŸç”Ÿæ–¹å¼ï¼‰ï¼š
   ```python
   from langchain_core.runnables import RunnableConfig

   @tool
   def modify_field(field_name: str, instruction: str, config: RunnableConfig) -> str:
       """ä¿®æ”¹æŒ‡å®šå­—æ®µçš„å†…å®¹ã€‚"""
       project_id = config["configurable"]["project_id"]
       ...
   ```
   è°ƒç”¨å›¾æ—¶é€šè¿‡ `config={"configurable": {"project_id": "..."}}` ä¼ å…¥ã€‚

**æ¨èæ–¹æ¡ˆ B**ï¼šä¸éœ€è¦æ¯æ¬¡é‡å»ºå›¾ï¼ŒLangGraph åŸç”Ÿæ”¯æŒï¼Œæ€§èƒ½æ›´å¥½ã€‚

---

## äº”ã€@ å¼•ç”¨æœºåˆ¶çš„å˜åŒ–

### ç°æœ‰æœºåˆ¶
å‰ç«¯è§£æ `@å­—æ®µå` â†’ ä¼ å…¥ `references: ["å­—æ®µå"]` â†’ åç«¯ `_resolve_references()` æŸ¥å†…å®¹ â†’ æ³¨å…¥åˆ° `referenced_contents` state å­—æ®µã€‚

### æ–°æœºåˆ¶
@ å¼•ç”¨ä»ç„¶ç”±å‰ç«¯è§£æï¼Œä½†æ³¨å…¥æ–¹å¼æ”¹å˜ï¼š

```python
# API å±‚ï¼šå°† @ å¼•ç”¨å†…å®¹ä½œä¸ºç”¨æˆ·æ¶ˆæ¯çš„ä¸€éƒ¨åˆ†
if references:
    ref_contents = _resolve_references(db, project_id, references)
    ref_text = "\n".join(f"ã€{name}ã€‘\n{content}" for name, content in ref_contents.items())
    # é™„åŠ åˆ°ç”¨æˆ·æ¶ˆæ¯ä¸­
    augmented_message = f"{request.message}\n\n---\nä»¥ä¸‹æ˜¯ç”¨æˆ·å¼•ç”¨çš„å­—æ®µå†…å®¹ï¼š\n{ref_text}"
    input_messages.append(HumanMessage(content=augmented_message))
else:
    input_messages.append(HumanMessage(content=request.message))
```

è¿™æ · LLM è‡ªç„¶åœ°çœ‹åˆ°å¼•ç”¨å†…å®¹ï¼Œæ— éœ€åœ¨ State ä¸­ä¼ é€’ã€‚

---

## å…­ã€é Agent åœºæ™¯çš„ LLM è°ƒç”¨

ä»¥ä¸‹åœºæ™¯ä¸èµ° Agent Graphï¼Œä½†ä¹Ÿè¦ç”¨ `llm` / `llm_mini` æ›¿ä»£ `ai_client`ï¼š

| åœºæ™¯ | æ–‡ä»¶ | è°ƒç”¨æ–¹å¼ |
|------|------|---------|
| å­—æ®µç‹¬ç«‹ç”Ÿæˆï¼ˆå†…å®¹å—ç”ŸæˆæŒ‰é’®ï¼‰ | `api/blocks.py`, `tools/field_generator.py` | `llm.ainvoke()` / `llm.astream()` |
| æ‘˜è¦ç”Ÿæˆ | `core/digest_service.py` | `llm_mini.ainvoke()` |
| DeepResearch ç»¼åˆåˆ†æ | `tools/deep_research.py` | `llm.ainvoke()` |
| Eval å¼•æ“ï¼ˆæ¨¡æ‹Ÿå™¨/è¯„å®¡ï¼‰ | `tools/eval_engine.py` | `llm.ainvoke()` |
| æ¨¡æ‹Ÿå™¨ï¼ˆæ¶ˆè´¹è€…æ¨¡æ‹Ÿï¼‰ | `tools/simulator.py` | `llm.ainvoke()` |

æ›¿æ¢æ¨¡å¼ç»Ÿä¸€ï¼š
```python
# æ—§
from core.ai_client import ai_client, ChatMessage
messages = [ChatMessage(role="system", content=sp), ChatMessage(role="user", content=up)]
response = await ai_client.async_chat(messages, temperature=0.7)
text = response.content

# æ–°
from core.llm import llm  # æˆ– llm_mini
from langchain_core.messages import SystemMessage, HumanMessage
messages = [SystemMessage(content=sp), HumanMessage(content=up)]
response = await llm.ainvoke(messages)
text = response.content
```

---

## ä¸ƒã€è®¾è®¡è€ƒé‡ï¼šå·¥å…·å†…éƒ¨çš„ LLM æµå¼

### é—®é¢˜

åœ¨ Agent Loop ä¸­ï¼Œåªæœ‰ `agent_node` çš„ LLM è°ƒç”¨ä¼šè¢«ç›´æ¥æµå¼è¾“å‡ºç»™ç”¨æˆ·ã€‚å½“å·¥å…·å‡½æ•°ï¼ˆå¦‚ `generate_field_content`ï¼‰å†…éƒ¨ä¹Ÿè°ƒç”¨ LLM æ—¶ï¼Œè¿™äº›å†…éƒ¨è°ƒç”¨çš„ token ä¸ä¼šè‡ªåŠ¨æµå¼è¾“å‡ºã€‚

`astream_events(version="v2")` å®é™…ä¸Š**ä¼š**æ•è·åµŒå¥— runnable çš„äº‹ä»¶ã€‚ä½†é—®é¢˜æ˜¯ï¼š
- å·¥å…·å†…éƒ¨ LLM ç”Ÿæˆçš„å†…å®¹æ˜¯"å·¥å…·ç»“æœ"ï¼ˆä¿å­˜åˆ° DBï¼‰ï¼Œä¸åº”ä½œä¸º"èŠå¤©å†…å®¹"æµå¼è¾“å‡º
- Agent åœ¨å·¥å…·è¿”å›åçš„æ€»ç»“æ‰æ˜¯ç”¨æˆ·åœ¨èŠå¤©åŒºçœ‹åˆ°çš„å†…å®¹

### ç­–ç•¥

1. **åªæµå¼è½¬å‘ agent èŠ‚ç‚¹çš„ LLM è¾“å‡º**ï¼ˆé€šè¿‡ event tags/name åŒºåˆ†æ¥æºï¼‰
2. **å·¥å…·æ‰§è¡ŒæœŸé—´**å‘é€ `tool_start` äº‹ä»¶è®©ç”¨æˆ·çŸ¥é“åœ¨å¤„ç†
3. **å·¥å…·å†…éƒ¨ LLM** çš„è¿›åº¦å¯ä»¥é€šè¿‡ SSE heartbeat æˆ– progress äº‹ä»¶ä½“ç°
4. **æœªæ¥ä¼˜åŒ–**ï¼šå¯¹äºé•¿æ—¶é—´å·¥å…·ï¼ˆ>10sï¼‰ï¼Œå¯åœ¨å·¥å…·å†…éƒ¨é€šè¿‡ callback å‘é€è¿›åº¦äº‹ä»¶

è¿™ä¸ Cursor çš„ Agent æ¨¡å¼ä¸€è‡´ï¼šå·¥å…·æ‰§è¡Œæ—¶ç”¨æˆ·çœ‹åˆ° "æ­£åœ¨æ‰§è¡Œ..." çš„çŠ¶æ€æŒ‡ç¤ºï¼Œå·¥å…·å®Œæˆå Agent ç”¨è‡ªç„¶è¯­è¨€æ€»ç»“ç»“æœã€‚

---

## å…«ã€è¿ç§»é£é™©ä¸ Fallback

| é£é™© | å½±å“ | Fallback |
|------|------|---------|
| LLM ä¸è°ƒç”¨å·¥å…·ï¼ˆè¯¥è°ƒç”¨æ—¶ç›´æ¥å›å¤ï¼‰ | ç”¨æˆ·è¯´"ä¿®æ”¹XX"ä½† LLM åªæ˜¯èŠå¤© | system prompt å¼ºåŒ–å·¥å…·ä½¿ç”¨å¼•å¯¼ï¼›å…œåº•æ£€æµ‹å…³é”®è¯ |
| LLM è°ƒç”¨é”™è¯¯çš„å·¥å…· | ç”¨æˆ·è¯´"çœ‹çœ‹XX"ä½†è°ƒç”¨äº† modify_field | å·¥å…· docstring ç²¾ç¡®æè¿°ä½¿ç”¨åœºæ™¯ï¼›æ¯ä¸ªå·¥å…·åŠ å…¥å®‰å…¨æ£€æŸ¥ |
| Tool Calling ä¸æ”¯æŒçš„æ¨¡å‹ | åˆ‡æ¢åˆ°ä¸æ”¯æŒ function calling çš„æ¨¡å‹ | `get_chat_model()` æ£€æŸ¥ provider æ˜¯å¦æ”¯æŒï¼Œä¸æ”¯æŒåˆ™é™çº§ä¸º prompt æ–¹å¼ |
| astream_events äº‹ä»¶æ ¼å¼å˜åŒ– | LangGraph ç‰ˆæœ¬å‡çº§å¯¼è‡´äº‹ä»¶åå˜åŒ– | é”å®š langgraph ç‰ˆæœ¬ï¼›äº‹ä»¶å¤„ç†åŠ  try/except |
| å·¥å…·æ‰§è¡Œè¶…æ—¶ | æ·±åº¦è°ƒç ”ã€å­—æ®µç”Ÿæˆå¯èƒ½è¶…è¿‡ 60s | å·¥å…·å†…éƒ¨åŠ  timeoutï¼›SSE å®šæœŸå‘é€ heartbeat |
| project_id è·å–å¤±è´¥ | RunnableConfig ä¼ é€’ä¸¢å¤± | å·¥å…·å‡½æ•°å†…æ£€æŸ¥ï¼Œç¼ºå¤±åˆ™è¿”å›é”™è¯¯ä¿¡æ¯ |

---

## ä¹ã€å®æ–½é¡ºåºä¸ä¾èµ–å…³ç³»

```
Phase 1: åŸºç¡€è®¾æ–½ï¼ˆå¯ç‹¬ç«‹æ‰§è¡Œï¼Œä¸å½±å“ç°æœ‰åŠŸèƒ½ï¼‰
  Step 1.1 æ–°å»º llm.py          â† æ— ä¾èµ–
  Step 1.2 æ‰©å±• config.py       â† æ— ä¾èµ–
  Step 1.3 æ–°å»º agent_tools.py  â† ä¾èµ– Step 1.1
  Step 1.4 æ›´æ–° requirements.txt â† æ— ä¾èµ–

Phase 2: Agent Graphï¼ˆæ ¸å¿ƒæ”¹é€ ï¼‰
  Step 2.1 é‡å†™ orchestrator.py  â† ä¾èµ– Phase 1
  Step 2.2 æ›´æ–° AgentState       â† åŒ…å«åœ¨ Step 2.1 ä¸­

Phase 3: API å±‚ï¼ˆå¯¹å¤–æ¥å£æ”¹é€ ï¼‰
  Step 3.1 é‡å†™ /stream          â† ä¾èµ– Phase 2
  Step 3.2 é‡å†™ /chat            â† ä¾èµ– Phase 2
  Step 3.3 æ¸…ç†æ—§ endpoint       â† ä¾èµ– Step 3.1, 3.2

Phase 4: æ¸…ç†æ—§ä»£ç 
  Step 4.1 åˆ é™¤ ai_client.py     â† ä¾èµ– Phase 3ï¼ˆæ‰€æœ‰å¼•ç”¨å·²æ›¿æ¢ï¼‰
  Step 4.2 æ¸…ç† orchestrator.py  â† åŒ…å«åœ¨ Phase 2 ä¸­

Phase 5: å·¥å…·å‡½æ•°å†…éƒ¨ ai_client â†’ llm æ›¿æ¢
  ï¼ˆå¯ä¸ Phase 2-4 å¹¶è¡Œï¼ŒæŒ‰æ–‡ä»¶é€ä¸ªæ›¿æ¢ï¼‰
```

**ä¼°ç®—**:
- Phase 1: æ–°å¢ ~350 è¡Œ
- Phase 2: åˆ é™¤ ~2500 è¡Œï¼ˆorchestrator.py 2733è¡Œä¸­ç»å¤§éƒ¨åˆ†ï¼‰ï¼Œæ–°å¢ ~200 è¡Œ
- Phase 3: åˆ é™¤ ~300 è¡Œï¼ˆagent.py ä¸­ stream_chat + æ—§è¾…åŠ©å‡½æ•°ï¼‰ï¼Œæ–°å¢ ~120 è¡Œ
- Phase 4: åˆ é™¤ ~330 è¡Œï¼ˆai_client.py æ•´æ–‡ä»¶ï¼‰
- Phase 5: æ¯ä¸ªæ–‡ä»¶æ”¹åŠ¨ ~10-30 è¡Œï¼ˆ11 ä¸ªæ–‡ä»¶ â‰ˆ ~250 è¡Œæ”¹åŠ¨ï¼‰

**æ€»è®¡**: åˆ é™¤ ~3100 è¡Œï¼Œæ–°å¢ ~670 è¡Œï¼Œå‡€å‡å°‘ ~2400 è¡Œ

---

## åã€å‰ç«¯é€‚é…

### 10.1 agent-panel.tsx SSE äº‹ä»¶å¤„ç†

```typescript
// ç°æœ‰äº‹ä»¶å¤„ç†ï¼ˆä¿ç•™ï¼‰
case "user_saved": ...  // ä¸å˜
case "token": ...       // ä¸å˜
case "done": ...        // ä¸å˜
case "error": ...       // ä¸å˜

// éœ€è¦ä¿®æ”¹
case "route": ...       // åˆ é™¤æˆ–æ›¿æ¢ä¸º tool_start

// æ–°å¢äº‹ä»¶å¤„ç†
case "tool_start":
  // æ˜¾ç¤º "ğŸ”§ æ­£åœ¨æ‰§è¡Œ {tool_name}..."
  setStatusMessage(`ğŸ”§ æ­£åœ¨${toolNameMap[data.tool]}...`);
  break;

case "tool_end":
  // æ›´æ–°çŠ¶æ€ï¼Œè§¦å‘å­—æ®µåˆ·æ–°
  if (data.field_updated) {
    onContentUpdate?.();
  }
  break;
```

### 10.2 å·¥å…·åç§°æ˜ å°„ï¼ˆå‰ç«¯æ˜¾ç¤ºå‹å¥½åç§°ï¼‰

```typescript
const toolNameMap: Record<string, string> = {
  "modify_field": "ä¿®æ”¹å­—æ®µå†…å®¹",
  "generate_field_content": "ç”Ÿæˆå­—æ®µå†…å®¹",
  "query_field": "æŸ¥è¯¢å­—æ®µä¿¡æ¯",
  "manage_architecture": "ç®¡ç†é¡¹ç›®æ¶æ„",
  "advance_to_phase": "æ¨è¿›é˜¶æ®µ",
  "run_research": "æ‰§è¡Œè°ƒç ”",
  "run_evaluation": "æ‰§è¡Œè¯„ä¼°",
  "manage_persona": "ç®¡ç†ç”¨æˆ·ç”»åƒ",
  "generate_outline": "ç”Ÿæˆå¤§çº²",
  "read_field": "è¯»å–å­—æ®µå†…å®¹",
  "update_field": "æ›´æ–°å­—æ®µå†…å®¹",
};
```

### 10.3 PRODUCE_ROUTES çš„å˜åŒ–

ç°æœ‰ `PRODUCE_ROUTES` é€šè¿‡ route_target åˆ¤æ–­æ˜¯å¦ä¸ºäº§å‡ºè·¯ç”±ã€‚æ–°æ¶æ„ä¸­ï¼Œè¿™ä¸ªåˆ¤æ–­æ”¹ä¸ºé€šè¿‡ `tool_end` äº‹ä»¶çš„ `field_updated` å­—æ®µï¼š

```typescript
// æ—§ï¼šé€šè¿‡ route äº‹ä»¶åˆ¤æ–­
if (PRODUCE_ROUTES.includes(currentRoute)) { ... }

// æ–°ï¼šé€šè¿‡ tool_end äº‹ä»¶åˆ¤æ–­
case "tool_end":
  if (data.field_updated) {
    // å­—æ®µå·²æ›´æ–°ï¼Œè§¦å‘å·¦ä¾§å·¥ä½œå°åˆ·æ–°
    onContentUpdate?.();
  }
  break;
```

---

## åä¸€ã€æ‰§è¡Œå‰æ£€æŸ¥æ¸…å•

| æ£€æŸ¥é¡¹ | è¯´æ˜ |
|--------|------|
| `langchain-openai` å·²å®‰è£… | `pip install langchain-openai` |
| `.env` ä¸­ `OPENAI_API_KEY` æœ‰æ•ˆ | LangChain çš„ ChatOpenAI ä¹Ÿéœ€è¦ |
| æ‰€æœ‰ `ai_client` å¼•ç”¨å·²æ›¿æ¢ | `grep -r "ai_client" backend/` ç»“æœä¸ºç©º |
| æ‰€æœ‰ `ChatMessage` (è‡ªå®šä¹‰) å·²æ›¿æ¢ | `grep -r "from core.ai_client import" backend/` ç»“æœä¸ºç©º |
| `AGENT_TOOLS` åˆ—è¡¨å®Œæ•´ | è¦†ç›–ç°æœ‰æ‰€æœ‰æ“ä½œç±»å‹ |
| å·¥å…· docstring æ¸…æ™° | LLM èƒ½æ ¹æ®æè¿°æ­£ç¡®é€‰æ‹©å·¥å…· |
| `RunnableConfig` ä¼ é€’ `project_id` | æ‰€æœ‰å·¥å…·å‡½æ•°èƒ½è·å– project_id |
| SSE äº‹ä»¶ç±»å‹å¯¹é½ | å‰ç«¯èƒ½å¤„ç†æ–°çš„ `tool_start` / `tool_end` äº‹ä»¶ |
| å‰ç«¯ `route` äº‹ä»¶å¤„ç†é™çº§ | è¿‡æ¸¡æœŸå…¼å®¹æ—§çš„ `route` äº‹ä»¶ |

---

## åäºŒã€æ‰§è¡ŒåéªŒè¯æ¸…å•

| é˜¶æ®µ | éªŒè¯æ–¹æ³• |
|------|----------|
| Phase 1 | 1. `from core.llm import llm, llm_mini` æˆåŠŸ 2. `from core.agent_tools import AGENT_TOOLS` æˆåŠŸ 3. `len(AGENT_TOOLS) >= 8` |
| Phase 2 | 1. `from core.orchestrator import agent_graph` æˆåŠŸ 2. `agent_graph.get_graph().nodes` åŒ…å« "agent" å’Œ "tools" |
| Phase 3 | 1. `/stream` è¯·æ±‚è¿”å› SSE æµ 2. çº¯èŠå¤©è¿”å› `token` äº‹ä»¶ 3. "ä¿®æ”¹@å­—æ®µ" è¿”å› `tool_start` â†’ `token` â†’ `tool_end` â†’ `done` 4. `content` äº‹ä»¶ä¸å†å‡ºç° |
| Phase 4 | 1. `grep -r "ai_client" backend/` æ— ç»“æœ 2. `backend/core/ai_client.py` ä¸å­˜åœ¨ |
| Phase 5 | 1. å­—æ®µç‹¬ç«‹ç”Ÿæˆä»æ­£å¸¸ 2. æ‘˜è¦ç”Ÿæˆæ­£å¸¸ 3. DeepResearch æ­£å¸¸ 4. Eval å¼•æ“æ­£å¸¸ |
