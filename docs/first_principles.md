# 内容生产系统：第一性原理

> 创建时间：2026-02-15
> 性质：从根本目的出发的项目理念、设计实践、差距分析、迭代复盘

---

## 一、这个项目在解决什么问题

### 1.1 第一性命题

**商业内容生产的核心矛盾是：创作者知道"做什么"，但不知道"做得好不好"。**

一个创作者要做一门课程、一份品牌方案、一套营销物料，他/她有主题（做什么），有受众（给谁看），但：

- **生产过程是黑箱**：从意图到成品之间缺少结构化的反馈回路。写完了才发现方向偏了，但已经沉没了时间成本。
- **质量评估靠直觉**：内容好不好，只有发布后才知道。发布前的判断高度依赖个人经验——而经验是最不可扩展的资源。
- **渠道适配是体力活**：同一套核心内容要适配不同平台（PPT、小红书、课程脚本），每次都是重复劳动。

这个系统要解决的根本问题是：**让 AI Agent 把内容生产从"凭感觉的手艺"变成"有反馈的工程"**。

### 1.2 解决方案的核心假设

1. **以终为始**（Backward Design）：先明确目标受众会做什么（期望行动），再倒推内容该怎么写。这不是"AI帮你写"，是"AI帮你想清楚再写"。
2. **结构化拆解**：把内容生产拆成可评估的原子单元（ContentBlock），每个单元有明确的输入（依赖）、输出（内容）、评估标准。
3. **闭环反馈**：通过消费者模拟（Simulator）和多维评估（Eval V2），在发布前获得"类真实"的反馈。
4. **Agent 引领流程**：不是用户驱动的工具，而是 Agent 主动推进的协作者。Agent 知道下一步该做什么。

### 1.3 这个假设和"让 AI 直接写内容"有什么本质区别？

ChatGPT 也能写课程大纲、写营销文案。区别在于：

| 维度 | 直接让 AI 写 | 本系统的方法 |
|------|-------------|------------|
| **上下文** | 每次对话从零开始 | 意图→调研→设计→生产→评估，全链路上下文累积 |
| **结构** | 一坨文本 | 树形内容块，每块可独立生成/评估/迭代 |
| **一致性** | 写 10 个字段，风格可能 10 种 | 创作者画像 + 记忆系统，保证全局一致 |
| **反馈** | 无 | 消费者模拟 + 评估体系，生产前闭环 |
| **迭代** | 重新写一遍 | 版本管理 + 依赖追踪，局部修改传导全局 |

---

## 二、架构分层与设计理念

### 2.1 架构全景

```
┌─────────────────── 用户 ───────────────────┐
│                                             │
│  三栏布局: [进度] [内容编辑] [Agent 对话]     │
│                                             │
├─────────── 前端 (Next.js 14) ──────────────┤
│  组件: ProgressPanel / ContentPanel /        │
│        AgentPanel / EvalPhasePanel / ...     │
│  状态: WorkspacePage 集中管理                  │
│  通信: SSE 流式 + REST CRUD                   │
│                                              │
├─────────── API (FastAPI) ──────────────────┤
│  /api/agent/stream  — SSE Agent 对话         │
│  /api/blocks/*      — ContentBlock CRUD      │
│  /api/eval/*        — Eval V2 体系           │
│  /api/modes/*       — Agent 模式管理          │
│  /api/memories/*    — 记忆 CRUD               │
│                                              │
├─────────── Agent 核心 ─────────────────────┤
│  orchestrator.py: StateGraph + ToolNode      │
│  agent_tools.py: 13 个 @tool（LLM 自动选择）  │
│  memory_service.py: 提炼/去重/合并/注入        │
│  build_system_prompt: 固定层+项目层+记忆层     │
│                                              │
├─────────── 工具层 ──────────────────────────┤
│  field_generator.py  — 字段生成               │
│  deep_research.py    — Tavily 深度调研         │
│  architecture_*.py   — 内容块结构读写          │
│  eval_engine.py      — 评估执行               │
│  simulator.py        — 消费者模拟             │
│  persona_manager.py  — 人物画像管理            │
│  edit_engine.py      — 精细编辑引擎            │
│                                              │
├─────────── 数据层 ──────────────────────────┤
│  Project → ContentBlock（树形，无限层级）       │
│  AgentMode / MemoryItem（模式与记忆）          │
│  EvalRun / EvalTask / EvalTrial / Grader     │
│  ChatMessage / ContentVersion / GenerationLog │
│  AsyncSqliteSaver（Agent Checkpointer）       │
└──────────────────────────────────────────────┘
```

### 2.2 六个核心设计决策

#### 决策 1：LLM Tool Calling 取代 if/else 路由

**原来的做法**：`route_intent()` 用 LLM 分类用户意图 → if/elif 派发到不同节点。
**现在的做法**：`llm.bind_tools(AGENT_TOOLS)` → LLM 自主决定是否调用工具、调用哪个。

**为什么这是对的**：意图分类本身就是 LLM 在做的事情。用一个 LLM 调用分类，再用另一个 LLM 调用执行，等于做了两次同样的推理。Tool Calling 把这两步合并成一步，减少延迟、减少错误、减少维护成本。

```python
# 旧：5000 字的分类 prompt + if/elif 路由
intent = await route_intent(message)
if intent == "modify": ...
elif intent == "generate": ...
elif intent == "research": ...

# 新：一个 system prompt + tool docstrings，LLM 自己选
llm_with_tools = llm.bind_tools(AGENT_TOOLS)
response = await llm_with_tools.ainvoke(messages)
# response 要么直接回复，要么带 tool_calls
```

#### 决策 2：ContentBlock 统一树形模型

**原来的做法**：`ProjectField`（固定阶段的字段列表）+ `ContentBlock`（灵活架构的树形结构），两套并行。
**现在的做法**：只有 `ContentBlock`，一切都是树节点。

**为什么这是对的**：内容生产的本质是树形结构——一门课程有章节，章节有知识点，知识点有逐字稿。固定的"字段列表"无法表达这种层级关系。`ContentBlock` 的 `parent_id` + `depends_on` 同时解决了"结构"和"依赖"两个问题。

```
Project
├── 意图分析 (phase, special_handler="intent")
├── 消费者调研 (phase, special_handler="research")
├── 内涵设计 (phase)
│   └── 方案一 (proposal)
│       ├── 核心论点 (field, depends_on=[意图分析])
│       ├── 案例故事 (field, depends_on=[核心论点, 消费者调研])
│       └── 场景库 (group)
│           ├── 场景1 (field)
│           └── 场景2 (field)
├── 内涵生产 (phase)
├── 外延设计 (phase)
├── 外延生产 (phase)
├── 消费者模拟 (phase, special_handler="simulate")
└── 项目评估 (phase, special_handler="evaluate")
```

#### 决策 3：记忆全量注入而非向量检索

**做法**：每次 Agent 调用前，从 DB 加载该项目的所有 `MemoryItem`，拼接为文本注入 system prompt。

**为什么这是对的**（来自 `memory_and_mode_system.md` 的推导）：

1. **量级**：单项目记忆约 50-150 条，约 1500-4500 tokens。对 128K 上下文窗口微不足道。
2. **向量检索的局限**：它找的是"话题相似"的记忆，但"偏好口语化"和"改场景库开头"话题完全不同——前者却直接约束后者。这种因果关系，cosine similarity 捕捉不到。
3. **LLM 自身就是最好的检索器**：给它全部记忆，它在推理过程中自动判断哪些和当前任务相关。

#### 决策 4：模式 = 视角，不是能力

5 种预置模式（助手/策略顾问/审稿人/目标读者/创意伙伴）共享**完全相同的工具集**。区别只在 system prompt 的身份段——换的是思考角度，不是操作权限。

**为什么这是对的**：对比"共创模式"的旧做法——绕过 Agent Graph，用裸 `llm.astream()`，没有工具、没有上下文、没有 Checkpointer。本质上不是"换视角"，是"换了一个残废的 Agent"。统一走 Agent Graph 意味着在任何模式下，Agent 都能读取内容块、修改字段、推进阶段。

#### 决策 5：对话线程按模式隔离，知识靠记忆传递

```
thread_id = "{project_id}:{mode}"
```

每种模式有独立的对话历史（包含完整的 ToolMessage 链），模式间不直接共享对话——因为 ToolMessage 是噪声。模式间的知识传递靠 Memory 层：对话结束后 LLM 提炼关键信息 → 入库 → 下次任何模式都能看到。

这解决了"未知的未知"问题：用户在助手模式说过"不要学术语气"，切到审稿人模式后，审稿人自动知道这件事（因为记忆已注入），而不需要用户重复说。

#### 决策 6：评估是结构化的（Eval V2: EvalRun → EvalTask → EvalTrial × Grader）

不是让 LLM 写一段"综合评价"，而是：
- **EvalRun**：一次完整评估
- **EvalTask**：评估的一个维度（如"内容逻辑性"）
- **EvalTrial**：一个维度下的一次执行
- **Grader**：评估者（可以是"内容专家"、"目标消费者"等不同视角）

每个 Trial 有量化评分和定性反馈。这让"内容好不好"从主观感受变成可追踪、可比较的数据。

---

## 三、迭代脉络（13 天，60+ 次提交）

### 3.1 时间线

```
02/03  ████░░░░░░░░░░░░░░░░░░░░  Day 1:  V1 首版——从零到可用
02/04  █████░░░░░░░░░░░░░░░░░░░  Day 2:  灵活架构引入（ContentBlock 诞生）
02/05  ████████░░░░░░░░░░░░░░░░  Day 3:  golden_context 废弃、流式API、树形视图
02/06  ███████░░░░░░░░░░░░░░░░░  Day 4:  Eval 系统 V1、pre_questions、自动触发链
02/07  ██████░░░░░░░░░░░░░░░░░░  Day 5:  Eval 大重构（multi-grader、状态持久化）
02/08  ██████░░░░░░░░░░░░░░░░░░  Day 6:  intent routing 重构、simulator 修复
02/09  ████░░░░░░░░░░░░░░░░░░░░  Day 7:  三栏布局增强、评估模板V2、bug fix 集中
02/10  ████████░░░░░░░░░░░░░░░░  Day 8:  导入导出、UI改进、Deep Research 修复
02/11  ██████████░░░░░░░░░░░░░░  Day 9:  🔥 LangGraph 迁移（最大重构）
02/12  ████████░░░░░░░░░░░░░░░░  Day 10: 迁移收尾、测试修复、审计
02/13  ██████████████░░░░░░░░░░  Day 11: 架构统一（ProjectField→ContentBlock）、SSE/Hook抽取
02/14  ████████████████░░░░░░░░  Day 12: Memory & Mode M1+M2、content-panel 瘦身、旧代码清除
02/15  ████████░░░░░░░░░░░░░░░░  Day 13: Memory M3、一致性审计、稳定化
```

### 3.2 五个关键迭代节点

#### 节点 1：V1 首版（02/03）
**状态**：基于 if/elif 路由的 Agent，固定的 ProjectField 结构，意图分析→调研→生产的线性流程。
**核心矛盾**：Agent 的路由逻辑和业务逻辑混杂。每加一个功能，都要在 orchestrator 里加一个 elif 分支。

#### 节点 2：灵活架构引入（02/04-05）
**变化**：引入 ContentBlock 树形模型。但 ProjectField 没有删——于是系统开始了长达 9 天的"双轨并行"。
**教训**：引入新架构时没有彻底替换旧架构，导致 `use_flexible_architecture` 布尔标志扩散到 14+ 个文件。**这是整个项目最昂贵的技术债**。

#### 节点 3：LangGraph 迁移（02/11-12）
**变化**：从手写的 if/elif 路由迁移到 LangGraph StateGraph + Tool Calling。
**规模**：这是最大的单次重构——重写了 orchestrator、新建了 agent_tools.py（13 个 @tool）、新建了 digest_service 和 edit_engine、修复了 176 个测试。
**意义**：迁移后，Agent 的行为完全由 system prompt + tool docstrings 定义，不再有硬编码的意图路由。这为后续的模式系统铺平了道路。

#### 节点 4：架构统一日（02/13-14）
**变化**：全面清除 ProjectField 双轨系统。
**规模**：修改了后端 10+ 个文件、前端全部组件。P0-1 任务列表有 15+ 个子项。
**教训**：如果在节点 2 就彻底迁移，后续 9 天不需要在每个新功能里写 `if use_flexible_architecture` 分支。

#### 节点 5：Memory & Mode 系统（02/14-15）
**变化**：统一 Agent 模式切换（删除共创分流），实现三层记忆系统。
**意义**：这是系统从"工具"走向"智能体"的关键一步——Agent 不再是无状态的，它有记忆，有视角。

### 3.3 迭代模式分析

回顾 60+ 次提交，可以看到两种交替出现的模式：

1. **功能爆发期**（Day 1-8）：密集添加功能，每天 3-8 次提交。commit 消息以 `feat:` 和 `fix:` 为主。特点是"先让它跑起来"。
2. **架构收敛期**（Day 9-13）：大规模重构和清理。commit 消息以 `refactor:`、`arch:`、`fix:` 为主。特点是"让它跑得对"。

这个节奏不是偶然的。功能爆发期积累的技术债，在架构收敛期集中偿还。**问题在于，有些债务本可以避免**——详见第四部分。

---

## 四、差距分析：当前实现 vs 理想状态

### 4.1 根本性缺陷（架构级，影响产品本质）

#### 缺陷 A：Agent 缺少"品味"——只有执行力，没有判断力

**现状**：Agent 是一个高效的执行者。用户说"帮我写场景库"，Agent 就写；用户说"把5个改成7个"，Agent 就改。但 Agent **从不主动说"我觉得这样不好"**。

**对标 Pi（Inflection AI）**：Pi 的设计核心是 *proactive empathy*——不等用户问就主动关心、主动引导。它不只回答问题，它在塑造对话。Pi 的"单一人格"（mono personality）意味着每次交互都是一致的，用户感知到的不是"AI 工具"，而是"一个有性格的助手"。

**对标 Cursor**：Cursor 的"智能感"来自 *anticipation*——Tab 键的预测不是在你要求后才给，而是在你还没按键时就出现。它预判你的下一步行动。这种"未卜先知"感是 Cursor 被感知为"聪明"的核心。

**本系统的差距**：

```
当前:  用户指令 → Agent 执行 → 用户评估 → 用户指令 → ...
理想:  用户指令 → Agent 执行 + 主动评估 + 建议改进 → 用户确认 → Agent 迭代 → ...
```

具体表现：
- Agent 生成内容后，不会自动检查是否与意图分析一致
- Agent 不会主动指出"这个字段和上一个字段的语气不统一"
- Agent 不会说"根据消费者画像，这段内容可能对目标受众太专业了"
- 即使在"审稿人"模式下，Agent 也需要用户主动发起审查请求

**修复方向**：在 `generate_field_content` 工具中，生成完成后自动做一次简短的自评（不需要完整 Eval，只需要一句话的 sanity check）。在 `build_system_prompt` 中，为所有模式添加"主动观察、主动建议"的行为原则。

#### 缺陷 B：评估→迭代的回路是断开的

**现状**：Eval V2 系统可以对内容做多维度评估（EvalRun → EvalTask → EvalTrial × Grader）。但评估结果**停留在评估报告里**。没有机制将评估发现自动转化为修改建议，也没有机制让 Agent 根据评估结果主动修改内容。

```
当前:  生产 → 评估 → 报告（止步于此，用户需手动发起修改）
理想:  生产 → 评估 → 报告 → Agent 生成修改计划 → 用户确认 → Agent 执行修改 → 重新评估
```

这意味着"闭环反馈"的最后一环——从反馈到行动——是缺失的。用户需要阅读评估报告，自己理解问题，自己用自然语言告诉 Agent 该改什么。这和"直接让 ChatGPT 改"没有本质区别。

**修复方向**：在评估完成后，自动调用 Agent（审稿人模式），让它阅读评估报告并生成具体的修改计划。用户可以逐条确认或拒绝。确认后 Agent 自动执行修改。

#### 缺陷 C：缺少"全局一致性守护"机制

**现状**：`build_system_prompt` 注入了创作者画像和记忆，但这只是"提供信息"。Agent 生成内容时，不会主动检查新内容是否与已有内容在语气、术语、核心信息上一致。

**cursorrule.md 的原始设计提到**：禁忌词检查 + 风格一致性检查 + 意图对齐检查。但这些在当前实现中**不存在**。`prompt_engine.py` 只做了创作者画像注入和字段依赖解析，没有一致性检查。

**为什么这是根本性的**：对于内容生产系统而言，"10 个字段风格统一"比"每个字段质量高"更重要。一个字段写得差可以重写，10 个字段风格不统一等于推倒重来。

### 4.2 "不够本质"的 Gap（重要但不致命）

#### Gap 1：前端状态管理（refreshKey 模式）

`WorkspacePage` 用 15+ 个 `useState` + `refreshKey` 计数器管理全局状态。AgentPanel 因为 `key={refreshKey}` 被完全销毁重建，丢失内部状态。

这是**工程质量问题**，不是产品问题。用 Zustand 或 Context 重构可以解决，但不会改变用户体验的本质。

#### Gap 2：Simulator 的真实度

消费者模拟基于 LLM 扮演消费者画像。但 LLM 本质上不是真实消费者——它不会"看不懂"，它只会假装"看不懂"。这是 LLM 模拟的固有局限。

这个 gap **不可通过工程手段完全弥合**——真正的消费者反馈需要真实用户。但可以通过更精细的模拟设计（多轮对话、任务驱动而非开放式）来提高仿真度。

#### Gap 3：版本管理是线性的

当前版本管理只有"修改前保存旧版本"（ContentVersion），没有分支、没有 diff 对比、没有选择性回滚。用户只能回滚到某个旧版本，不能"取旧版本的这段+新版本的那段"。

对于 MVP 来说够用。对于"内容生产平台"来说，需要类 Git 的分支能力。

#### Gap 4：无多用户协作

系统是单用户的——一个 SQLite 文件、一个本地进程。这和 Notion 的协作编辑、Cursor 的 Live Share 有本质差距。但对于"创作者个人工作台"的定位来说，这不是当前阶段的问题。

#### Gap 5：DeepResearch 可靠性

`deep_research.py` 依赖 Tavily Search API + LLM 综合。搜索质量取决于 Tavily 的覆盖率，综合质量取决于 LLM 对搜索结果的理解。没有 ground truth 验证机制——调研报告看起来像模像样，但引用的数据可能是错误的。

---

## 五、对标分析

### 5.1 Pi (Inflection AI) — 对齐 Agent 人格

Pi 的核心设计原则：**单一人格，始终如一，主动关怀**。

| Pi 的做法 | 本系统的做法 | 差距 |
|----------|------------|------|
| 单一连贯人格 | 5 种可切换模式 | 模式切换 ≠ 人格分裂。关键是每种模式的表达是否有一致的"底色"——都是为了帮创作者做得更好 |
| 主动发起话题 | 被动等待指令 | **核心差距**：Agent 应在观察到问题时主动开口 |
| 记住用户偏好 | Memory 系统（M2/M3） | ✅ 已基本对齐。全量注入 + 跨模式可见 |
| 情感温度管理 | system prompt 行为原则 | 原则写得好（尤其是审稿人的"批评对事不对人"），但执行一致性无法保证 |

**关键启示**：Pi 证明了"人格一致性"比"功能丰富度"更重要。用户和一个"一致的助手"建立信任的速度，远快于和一个"功能很多但感觉每次不一样"的工具。

本系统的 5 种模式设计方向是对的（认知姿态的不可约简分解），但需要一个**贯穿所有模式的人格底色**——例如"始终站在创作者的角度思考，始终为内容质量负责"。

### 5.2 Cursor — 对齐"智能感"

Cursor 被感知为"聪明"的三个核心机制：

1. **预测（Tab）**：在你还没输入时就预测你要写什么
2. **上下文感知（@codebase）**：理解整个代码库，不是只看当前文件
3. **即时反馈（Lint + Apply）**：修改立即可见，错误立即标记

| Cursor 的机制 | 本系统的对应 | 差距 |
|-------------|------------|------|
| Tab 预测 | 无 | Agent 可以在阶段推进时预测下一步并建议 |
| @codebase 全局理解 | `build_field_index()` 内容块索引 | ✅ 已有索引。但索引只是摘要，Agent 需要主动调用 `read_field` 才能看到全文 |
| 即时反馈 | SSE 流式输出 | ✅ 生成过程实时可见 |
| Apply 按钮 | 无 | Agent 建议修改后，需要一键应用的 UX |

**关键启示**：Cursor 的"智能感"来自**减少用户的认知步骤**——你不需要"想清楚怎么说"，Tab 一下就行。

本系统可以借鉴的是：在每个阶段完成后，Agent 不应该等用户说"继续"，而应该**主动呈现下一步的预览**——"接下来是内涵设计阶段，根据你的意图分析和消费者调研，我建议从这三个方向切入……你觉得哪个方向更好？"

### 5.3 Notion AI — 对齐"自然融入"

Notion AI 的设计哲学：**AI 不是一个单独的功能，它融入到每一个编辑动作中**。

| Notion 的做法 | 本系统的做法 | 差距 |
|-------------|------------|------|
| 在任何文本块中直接召唤 AI | Agent 在右栏，内容在中栏 | 空间上分离。用户需要在对话和内容之间来回切换 |
| AI 的修改直接出现在编辑器里 | 修改结果需要用户"在工作台查看" | 缺少 inline editing 的流畅感 |
| 选中文本 → AI 操作 | @引用机制 | @引用更强大（可引用整个字段），但选中即操作的即时感更好 |

**关键启示**：Notion AI 最成功的地方不是 AI 能力强——它用的 LLM 未必比本系统强。成功在于**AI 和编辑器无缝融合**，用户感知不到"切换到 AI 模式"这个动作。

本系统的三栏布局在功能上是完备的，但在体验上存在一个**模态切换的摩擦**：用户在中栏看到一段需要修改的内容，必须切到右栏告诉 Agent"帮我改"，然后再切回中栏看结果。理想状态是：在中栏的内容块上直接有一个 inline AI 操作入口。

---

## 六、经验教训

### 6.1 项目本身的教训

#### 教训 1：新架构引入时必须"断桥烧船"

引入 ContentBlock 时没有删除 ProjectField → 双轨并行 9 天 → `use_flexible_architecture` 标志污染 14+ 个文件 → 最终花了整整两天（Day 11-12）清理。

**如果重来**：Day 2 引入 ContentBlock 时，同时写一个数据迁移脚本，把所有旧项目的 ProjectField 迁移为 ContentBlock，然后当天删除 ProjectField。一天的迁移成本 vs 九天的双轨维护成本。

**根因**：怕迁移出错，所以选择了"兼容"。但"兼容"不是没有成本——它只是把成本分散到了之后的每一天，加上利息。

#### 教训 2：先迁移再扩展

LangGraph 迁移（Day 9-10）之所以是"最大的单次重构"，是因为它在 ProjectField 双轨系统上做的。如果先统一到 ContentBlock（Day 2 就做），LangGraph 迁移的改动量会减少 40%——因为不需要在每个 @tool 里写 `_find_block_or_field()`。

**普适原则**：每次引入新系统之前，先清理旧系统。否则新系统的每个组件都要兼容旧系统，复杂度是乘法不是加法。

#### 教训 3：评估系统也经历了同样的双轨问题

旧评估（`EvaluationTemplate` + `EvaluationReport`）和 Eval V2（`EvalRun` + `EvalTask` + `EvalTrial`）并行存在了好几天。直到 Day 12（P3-6）才彻底清除。

**模式识别**：这个项目至少在三个地方犯了同样的错误——ProjectField/ContentBlock、旧评估/Eval V2、共创模式/统一 Agent Graph。每次引入新系统时，旧系统都被保留为"向后兼容"，最终都需要专门花时间清理。

#### 教训 4：文档和代码的对齐很重要但很难维护

`docs/` 目录有 15+ 个设计文档，许多在实现后就过时了。`architecture.md` 甚至是空文件（0 字节）。`agent_design.md` 的 State 定义、工具列表、流程图全部和实际代码不匹配（直到 Day 12 才更新）。

**但也有反例**：`memory_and_mode_system.md` 写得很好——它先于代码写完，包含了设计决策的理由，实现时几乎照搬。**先写设计文档再写代码，文档的保质期显著更长**。

#### 教训 5：`build_system_prompt` 是系统的灵魂

整个 Agent 的行为质量，80% 取决于 `build_system_prompt` 这一个函数。它决定了 Agent 如何理解自己的角色、如何使用工具、如何和用户交互。

这个函数从最初的简单身份声明，到现在包含身份段 + 输出格式 + 能力声明 + 创作者信息 + 项目上下文 + 内容块索引 + 记忆 + 意图引导 + @引用约定 + 消歧规则 + 交互规则——一共约 5000 tokens 的精心设计的 prompt。

**教训**：system prompt 是 Agent 产品的核心资产。每一行都值得仔细斟酌，因为它会影响之后的每一次对话。

### 6.2 Vibe Coding 的教训

#### 教训 A：Vibe Coding 的"宿醉"效应

13 天 60+ 次提交的节奏，清楚地展示了 Vibe Coding 的典型模式：

**Day 1-8（功能爆发）**：AI 生成代码的速度极快，每天可以交付 3-8 个功能。但每个功能都在"最近的路径"上实现，不考虑全局一致性。结果：ProjectField/ContentBlock 双轨、旧评估/新评估双轨、共创分流……这些都是"先让它跑起来"的产物。

**Day 9-13（还债期）**：前 8 天积累的技术债需要集中偿还。LangGraph 迁移、架构统一、死代码清理——这些都不是新功能，而是让已有功能"跑得对"。

**比例关系**：功能开发 8 天，还债 5 天。**近 40% 的开发时间在还技术债**。

#### 教训 B：Context-Driven Development (CDD) 的重要性

这个项目的 `cursorrule.md` 起了关键作用——它告诉 AI 项目的设计理念、文档索引、数据流向。没有这个文件，AI 生成的代码会完全不顾已有架构。

但 `cursorrule.md` 也有局限：
- 它是静态的——当架构变化时（如 LangGraph 迁移后），cursorrule 需要手动更新
- 它不包含"当前状态"——AI 不知道 ProjectField 已经被废弃
- 它不包含"为什么这样做"——只有规则，没有推理

**理想状态（参考 CDD 方法论）**：
- **项目级上下文**：架构决策 + 当前状态 + 活跃技术债（类似本文档）
- **任务级上下文**：当前正在做什么 + 影响范围 + 验收标准
- **历史级上下文**：之前做过的决策 + 为什么那样做 + 哪些已被推翻

#### 教训 C：规格先行（SDD）vs 代码先行

这个项目的两个最成功的子系统——Memory/Mode 系统和 Eval V2——都是**先写了详细的设计文档再实现**（`memory_and_mode_system.md` 和 `eval_system_design.md`）。

而两个最痛苦的子系统——ProjectField/ContentBlock 双轨和旧评估体系——都是**直接开始写代码，边写边设计**。

这不是巧合。先写规格的好处：
1. 在实现之前发现设计缺陷（改文档比改代码便宜 100 倍）
2. 给 AI 提供完整上下文（`memory_and_mode_system.md` 可以直接作为 AI 的 spec）
3. 实现后文档仍然有效（因为代码是照着文档写的）

#### 教训 D：AI 生成的代码需要"审计点"

这个项目在 Day 11-13 做了多轮审计（P0-1 的二次审计、P3-6 的旧代码清除），发现了大量细节问题：
- 硬编码 `"gpt-5.1"` 应该用 `settings.openai_model`
- `getattr(llm, "model_name", "gpt-4o")` 里的默认值过时了
- docstring 里的 "ProjectField" 引用没有更新
- 5 个文件的 import 语句多余

这些问题每个都很小，但累积起来会导致"系统看起来能跑，但行为不可预测"。

**教训**：每次大的重构后，必须做一次 grep 审计——搜索旧名称、旧模式、硬编码值。AI 在重构时不会自动清理所有引用。

#### 教训 E：设计系统文档（design-system.md）的价值

872 行的 `design-system.md` 看似"过度设计"，但它解决了一个真实问题：当 AI 修改前端组件时，如果没有统一的图标/颜色/按钮规范，每个组件都会用不同的样式（emoji vs lucide icon、green vs emerald、Edit2 vs Pencil）。设计系统文档作为 AI 的约束条件，确保了 UI 一致性。

这印证了 SDD 的核心观点：**规格文档不是给人看的，是给 AI 看的**。AI 需要明确的规则才能保持一致性；没有规则，它每次都会做出不同的选择。

---

## 七、从第一性原理出发的下一步

### 7.1 优先级排序原则

> **改变用户感知的东西 > 改变代码质量的东西 > 改变开发体验的东西**

用户不关心你用的是 Zustand 还是 refreshKey，用户关心的是：
1. Agent 是否"聪明"（主动建议 > 被动执行）
2. 内容是否"好"（有评估闭环 > 无反馈黑箱）
3. 操作是否"流畅"（inline AI > 跨栏切换）

### 7.2 三件应该做的事（按优先级）

#### 1. Agent 主动品质守护（解决缺陷 A + C）

在 `generate_field_content` 和 `modify_field` 工具执行完毕后，自动做一次 LLM mini 调用：

```
"请检查以下新生成的内容是否：
1. 与创作者画像的语气一致
2. 与意图分析的目标受众匹配
3. 与已有内容块的术语和信息一致
如果发现问题，用一句话指出最严重的那个。如果没有问题，返回空字符串。"
```

这不需要改架构，只需要在工具函数里加 20 行代码。但它会让 Agent 从"无脑执行者"变成"有品味的协作者"。

#### 2. 评估→迭代自动闭环（解决缺陷 B）

评估完成后，自动在 Agent 对话中插入一条 SystemMessage：

```
"评估已完成。以下是发现的主要问题：
[评估报告摘要]
请根据这些发现，为创作者生成一份具体的修改计划。每条建议包含：
1. 需要修改的内容块名称
2. 具体的修改方向
3. 修改原因（引用评估发现）
请用 JSON 格式输出。"
```

然后前端展示修改计划，用户逐条确认/拒绝，确认后 Agent 自动执行 `modify_field`。

#### 3. 内容块 inline AI 操作（缩小 Notion 差距）

在中栏的 ContentBlockEditor 中，添加一个 floating toolbar：用户选中文本或点击内容块时，出现"AI 改写"、"AI 扩展"、"AI 精简"按钮。点击后直接在编辑器内完成修改，不需要切到右栏 Agent 对话。

这需要 `edit_engine.py` 的 inline 调用入口，但核心逻辑已经存在。

---

## 八、总结：这个项目的本质

这个项目试图回答一个问题：**AI 能否让内容生产从"凭感觉的手艺"变成"有反馈的工程"？**

13 天的迭代证明了：技术上可行，但还不够。当前系统实现了"结构化的生产"和"结构化的评估"，但缺少两个关键的最后一环：

1. **评估到迭代的闭环**——评估发现的问题能否自动转化为改进行动？
2. **Agent 的主动品质守护**——Agent 能否不等用户发现问题，就自己发现并指出？

解决了这两个问题，系统就从"一个帮你写内容的工具"升级为"一个和你一起对内容质量负责的伙伴"。

而 Vibe Coding 的教训更加普适：

- **先写规格再写代码**（design-system.md 和 memory_and_mode_system.md 的成功 vs ProjectField 双轨的教训）
- **新架构必须断桥烧船**（不留"兼容"选项，强制迁移）
- **40% 的时间会花在还债上**（这是规律，不是意外——提前计划好）
- **审计是必须的**（AI 生成的代码在重构时不会自动清理所有引用）
- **system prompt 是产品的灵魂**（值得花 10 倍于代码的时间打磨）

